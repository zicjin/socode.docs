<h1 class="devsite-page-title">tf.experimental.tensorrt.ConversionParams</h1>       <p>Parameters that are used for TF-TRT conversion.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.experimental.tensorrt.ConversionParams(
    rewriter_config_template=None,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32, minimum_segment_size=3,
    is_dynamic_op=True, maximum_cached_engines=1, use_calibration=True,
    max_batch_size=1, allow_build_at_runtime=True
)
</pre>  <h4 id="fields" data-text="Fields:">Fields:</h4> <ul> <li>
<b><code translate="no" dir="ltr">rewriter_config_template</code></b>: a template RewriterConfig proto used to create a TRT-enabled RewriterConfig. If None, it will use a default one.</li> <li>
<b><code translate="no" dir="ltr">max_workspace_size_bytes</code></b>: the maximum GPU temporary memory which the TRT engine can use at execution time. This corresponds to the 'workspaceSize' parameter of nvinfer1::IBuilder::setMaxWorkspaceSize().</li> <li>
<b><code translate="no" dir="ltr">precision_mode</code></b>: one the strings in TrtPrecisionMode.supported_precision_modes().</li> <li>
<b><code translate="no" dir="ltr">minimum_segment_size</code></b>: the minimum number of nodes required for a subgraph to be replaced by TRTEngineOp.</li> <li>
<b><code translate="no" dir="ltr">is_dynamic_op</code></b>: whether to generate dynamic TRT ops which will build the TRT network and engine at run time. i.e. Since TensorRT version &lt; 6.0 does not support dynamic dimensions other than the batch dimension, when the TensorFlow graph has a non-batch dimension of dynamic size, we would need to enable this option. This option should be set to True in TF 2.0.</li> <li>
<b><code translate="no" dir="ltr">maximum_cached_engines</code></b>: max number of cached TRT engines for dynamic TRT ops. Created TRT engines for a dynamic dimension are cached. This is the maximum number of engines that can be cached. If the number of cached engines is already at max but none of them supports the input shapes, the TRTEngineOp will fall back to run the original TF subgraph that corresponds to the TRTEngineOp.</li> <li>
<b><code translate="no" dir="ltr">use_calibration</code></b>: this argument is ignored if precision_mode is not INT8. If set to True, a calibration graph will be created to calibrate the missing ranges. The calibration graph must be converted to an inference graph by running calibration with calibrate(). If set to False, quantization nodes will be expected for every tensor in the graph (excluding those which will be fused). If a range is missing, an error will occur. Please note that accuracy may be negatively affected if there is a mismatch between which tensors TRT quantizes and which tensors were trained with fake quantization.</li> <li>
<b><code translate="no" dir="ltr">max_batch_size</code></b>: max size for the input batch. This parameter is only effective when use_implicit_batch is true.</li> <li>
<b><code translate="no" dir="ltr">allow_build_at_runtime</code></b>: whether to build TensorRT engines during runtime. If no TensorRT engine can be found in cache that can handle the given inputs during runtime, then a new TensorRT engine is built at runtime if allow_build_at_runtime=True, and otherwise native TF is used. This argument is only effective if is_dynamic_op=True.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">rewriter_config_template</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_workspace_size_bytes</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">precision_mode</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">minimum_segment_size</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">is_dynamic_op</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">maximum_cached_engines</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">use_calibration</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">max_batch_size</code> </td> <td> 
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">allow_build_at_runtime</code> </td> <td> 
</td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental/tensorrt/ConversionParams" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/experimental/tensorrt/ConversionParams</a>
  </p>
</div>
