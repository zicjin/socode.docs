<h1 class="devsite-page-title">tf.keras.mixed_precision.experimental.LossScaleOptimizer</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py#L940-L1110">  View source on GitHub </a> </td> </table> <p>An deprecated optimizer that applies loss scaling.</p> <p>Inherits From: <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">LossScaleOptimizer</code></a>, <a href="../../optimizers/optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer"><code translate="no" dir="ltr">tf.compat.v1.keras.mixed_precision.experimental.LossScaleOptimizer</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    optimizer, loss_scale
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> This class is deprecated and will be removed in TensorFlow 2.5. Please use the non-experimental class <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.LossScaleOptimizer</code></a> instead.</span></aside> <p>This class is identical to the non-experimental <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">keras.mixed_precision.LossScaleOptimizer</code></a> except its constructor takes different arguments. For this class (the experimental version), the constructor takes a <code translate="no" dir="ltr">loss_scale</code> argument. For the non-experimental class, the constructor encodes the loss scaling information in multiple arguments. Note that unlike this class, the non-experimental class does not accept a <a href="../../../mixed_precision/experimental/lossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.LossScale</code></a>, which is deprecated.</p> <p>If you currently use this class, you should switch to the non-experimental <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.LossScaleOptimizer</code></a> instead. We show several examples of converting the use of the experimental class to the equivalent non-experimental class.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# In all of the the examples below, `opt1` and `opt2` are identical
opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), loss_scale='dynamic')
opt2 = tf.keras.mixed_precision.LossScaleOptimizer(
    tf.keras.optimizers.SGD())
assert opt1.get_config() == opt2.get_config()
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), loss_scale=123)
# dynamic=False indicates to use fixed loss scaling. initial_scale=123
# refers to the initial loss scale, which is the single fixed loss scale
# when dynamic=False.
opt2 = tf.keras.mixed_precision.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), dynamic=False, initial_scale=123)
assert opt1.get_config() == opt2.get_config()
</pre> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
loss_scale = tf.compat.v1.mixed_precision.experimental.DynamicLossScale(
    initial_loss_scale=2048, increment_period=500)
opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), loss_scale=loss_scale)
opt2 = tf.keras.mixed_precision.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), initial_scale=2048,
    dynamic_growth_steps=500)
assert opt1.get_config() == opt2.get_config()
</pre> <p>Make sure to also switch from this class to the non-experimental class in isinstance checks, if you have any. If you do not do this, your model may run into hard-to-debug issues, as the experimental <code translate="no" dir="ltr">LossScaleOptimizer</code> subclasses the non-experimental <code translate="no" dir="ltr">LossScaleOptimizer</code>, but not vice versa. It is safe to switch isinstance checks to the non-experimental <code translate="no" dir="ltr">LossScaleOptimizer</code> even before using the non-experimental <code translate="no" dir="ltr">LossScaleOptimizer</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    tf.keras.optimizers.SGD(), loss_scale='dynamic')
# The experimental class subclasses the non-experimental class
isinstance(opt1, tf.keras.mixed_precision.LossScaleOptimizer)
True
opt2 = tf.keras.mixed_precision.LossScaleOptimizer(
    tf.keras.optimizers.SGD())
# The non-experimental class does NOT subclass the experimental class.
isinstance(opt2, tf.keras.mixed_precision.experimental.LossScaleOptimizer)
False
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">optimizer</code> </td> <td> The Optimizer instance to wrap. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale</code> </td> <td> The loss scale to scale the loss and gradients. This can either be an int/float to use a fixed loss scale, the string "dynamic" to use dynamic loss scaling, or an instance of a LossScale. The string "dynamic" equivalent to passing <code translate="no" dir="ltr">DynamicLossScale()</code>, and passing an int/float is equivalent to passing a FixedLossScale with the given loss scale. If a DynamicLossScale is passed, DynamicLossScale.multiplier must be 2 (the default). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> in case of any invalid argument. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dynamic</code> </td> <td> Bool indicating whether dynamic loss scaling is used. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic_counter</code> </td> <td> The number of steps since the loss scale was last increased or decreased. <p>This is None if <a href="../lossscaleoptimizer#dynamic"><code translate="no" dir="ltr">LossScaleOptimizer.dynamic</code></a> is False.</p> <p>The counter is incremented every step. Once it reaches <a href="../lossscaleoptimizer#dynamic_growth_steps"><code translate="no" dir="ltr">LossScaleOptimizer.dynamic_growth_steps</code></a>, the loss scale will be doubled and the counter will be reset back to zero. If nonfinite gradients are encountered, the loss scale will be halved and the counter will be reset back to zero. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">dynamic_growth_steps</code> </td> <td> The number of steps it takes to increase the loss scale. <p>This is None if <a href="../lossscaleoptimizer#dynamic"><code translate="no" dir="ltr">LossScaleOptimizer.dynamic</code></a> is False.</p> <p>Every <code translate="no" dir="ltr">dynamic_growth_steps</code> consecutive steps with finite gradients, the loss scale is increased. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_scale</code> </td> <td> The initial loss scale. <p>If <a href="../lossscaleoptimizer#dynamic"><code translate="no" dir="ltr">LossScaleOptimizer.dynamic</code></a> is False, this is the same number as <a href="../lossscaleoptimizer#loss_scale"><code translate="no" dir="ltr">LossScaleOptimizer.loss_scale</code></a>, as the loss scale never changes. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">inner_optimizer</code> </td> <td> The optimizer that this LossScaleOptimizer is wrapping. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale</code> </td> <td> The current loss scale as a float32 scalar tensor. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="get_scaled_loss" data-text="get_scaled_loss"><code translate="no" dir="ltr">get_scaled_loss</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py#L622-L648">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_scaled_loss(
    loss
)
</pre> <p>Scales the loss by the loss scale.</p> <p>This method is only needed if you compute gradients manually, e.g. with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. In that case, call this method to scale the loss before passing the loss to <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. If you use <a href="../../optimizers/optimizer#minimize"><code translate="no" dir="ltr">LossScaleOptimizer.minimize</code></a> or <a href="../lossscaleoptimizer#get_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_gradients</code></a>, loss scaling is automatically applied and this method is unneeded.</p> <p>If this method is called, <code translate="no" dir="ltr">get_unscaled_gradients</code> should also be called. See the <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.LossScaleOptimizer</code></a> doc for an example.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">loss</code> </td> <td> The loss, which will be multiplied by the loss scale. Can either be a tensor or a callable returning a tensor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> <code translate="no" dir="ltr">loss</code> multiplied by <a href="../lossscaleoptimizer#loss_scale"><code translate="no" dir="ltr">LossScaleOptimizer.loss_scale</code></a>. </td> </tr> 
</table> <h3 id="get_unscaled_gradients" data-text="get_unscaled_gradients"><code translate="no" dir="ltr">get_unscaled_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py#L650-L675">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_unscaled_gradients(
    grads
)
</pre> <p>Unscales the gradients by the loss scale.</p> <p>This method is only needed if you compute gradients manually, e.g. with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. In that case, call this method to unscale the gradients after computing them with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. If you use <a href="../../optimizers/optimizer#minimize"><code translate="no" dir="ltr">LossScaleOptimizer.minimize</code></a> or <a href="../lossscaleoptimizer#get_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_gradients</code></a>, loss scaling is automatically applied and this method is unneeded.</p> <p>If this method is called, <code translate="no" dir="ltr">get_scaled_loss</code> should also be called. See the <a href="../lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.LossScaleOptimizer</code></a> doc for an example.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads</code> </td> <td> A list of tensors, each which will be divided by the loss scale. Can have None values, which are ignored. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A new list the same size as <code translate="no" dir="ltr">grads</code>, where every non-None value in <code translate="no" dir="ltr">grads</code> is divided by <a href="../lossscaleoptimizer#loss_scale"><code translate="no" dir="ltr">LossScaleOptimizer.loss_scale</code></a>. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer</a>
  </p>
</div>
