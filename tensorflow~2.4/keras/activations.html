<h1 class="devsite-page-title">Module: tf.keras.activations</h1>       <p>Built-in activation functions.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="activations/deserialize"><code translate="no" dir="ltr">deserialize(...)</code></a>: Returns activation function given a string identifier.</p> <p><a href="activations/elu"><code translate="no" dir="ltr">elu(...)</code></a>: Exponential Linear Unit.</p> <p><a href="activations/exponential"><code translate="no" dir="ltr">exponential(...)</code></a>: Exponential activation function.</p> <p><a href="activations/gelu"><code translate="no" dir="ltr">gelu(...)</code></a>: Applies the Gaussian error linear unit (GELU) activation function.</p> <p><a href="activations/get"><code translate="no" dir="ltr">get(...)</code></a>: Returns function.</p> <p><a href="activations/hard_sigmoid"><code translate="no" dir="ltr">hard_sigmoid(...)</code></a>: Hard sigmoid activation function.</p> <p><a href="activations/linear"><code translate="no" dir="ltr">linear(...)</code></a>: Linear activation function (pass-through).</p> <p><a href="activations/relu"><code translate="no" dir="ltr">relu(...)</code></a>: Applies the rectified linear unit activation function.</p> <p><a href="activations/selu"><code translate="no" dir="ltr">selu(...)</code></a>: Scaled Exponential Linear Unit (SELU).</p> <p><a href="activations/serialize"><code translate="no" dir="ltr">serialize(...)</code></a>: Returns the string identifier of an activation function.</p> <p><a href="activations/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Sigmoid activation function, <code translate="no" dir="ltr">sigmoid(x) = 1 / (1 + exp(-x))</code>.</p> <p><a href="activations/softmax"><code translate="no" dir="ltr">softmax(...)</code></a>: Softmax converts a real vector to a vector of categorical probabilities.</p> <p><a href="activations/softplus"><code translate="no" dir="ltr">softplus(...)</code></a>: Softplus activation function, <code translate="no" dir="ltr">softplus(x) = log(exp(x) + 1)</code>.</p> <p><a href="activations/softsign"><code translate="no" dir="ltr">softsign(...)</code></a>: Softsign activation function, <code translate="no" dir="ltr">softsign(x) = x / (abs(x) + 1)</code>.</p> <p><a href="activations/swish"><code translate="no" dir="ltr">swish(...)</code></a>: Swish activation function, <code translate="no" dir="ltr">swish(x) = x * sigmoid(x)</code>.</p> <p><a href="activations/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Hyperbolic tangent activation function.</p>  <devsite-thumb-rating position="footer"> <template class="thumb-down-categories"> [{ "type": "thumb-down", "id": "missingTheInformationINeed", "label":"Missing the information I need" },{ "type": "thumb-down", "id": "tooComplicatedTooManySteps", "label":"Too complicated / too many steps" },{ "type": "thumb-down", "id": "outOfDate", "label":"Out of date" },{ "type": "thumb-down", "id": "samplesCodeIssue", "label":"Samples / code issue" },{ "type": "thumb-down", "id": "otherDown", "label":"Other" }] </template> <template class="thumb-up-categories"> [{ "type": "thumb-up", "id": "easyToUnderstand", "label":"Easy to understand" },{ "type": "thumb-up", "id": "solvedMyProblem", "label":"Solved my problem" },{ "type": "thumb-up", "id": "otherUp", "label":"Other" }] </template> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/activations" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/activations</a>
  </p>
</div>
