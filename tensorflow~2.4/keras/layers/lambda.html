<h1 class="devsite-page-title">tf.keras.layers.Lambda</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/layers/core.py#L757-L1077">  View source on GitHub </a> </td> </table> <p>Wraps arbitrary expressions as a <code translate="no" dir="ltr">Layer</code> object.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../module"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.Lambda</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)
</pre>  <p>The <code translate="no" dir="ltr">Lambda</code> layer exists so that arbitrary TensorFlow functions can be used when constructing <code translate="no" dir="ltr">Sequential</code> and Functional API models. <code translate="no" dir="ltr">Lambda</code> layers are best suited for simple operations or quick experimentation. For more advanced use cases, follow <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models">this guide</a> for subclassing <a href="layer"><code translate="no" dir="ltr">tf.keras.layers.Layer</code></a>.</p> <p>The main reason to subclass <a href="layer"><code translate="no" dir="ltr">tf.keras.layers.Layer</code></a> instead of using a <code translate="no" dir="ltr">Lambda</code> layer is saving and inspecting a Model. <code translate="no" dir="ltr">Lambda</code> layers are saved by serializing the Python bytecode, which is fundamentally non-portable. They should only be loaded in the same environment where they were saved. Subclassed layers can be saved in a more portable way by overriding their <code translate="no" dir="ltr">get_config</code> method. Models that rely on subclassed Layers are also often easier to visualize and reason about.</p> <h4 id="examples" data-text="Examples:">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># add a x -&gt; x^2 layer
model.add(Lambda(lambda x: x ** 2))
</pre>
<pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># add a layer that returns the concatenation
# of the positive part of the input and
# the opposite of the negative part

def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)

model.add(Lambda(antirectifier))
</pre> <h4 id="variables" data-text="Variables:">Variables:</h4> <p>While it is possible to use Variables with Lambda layers, this practice is discouraged as it can easily lead to bugs. For instance, consider the following layer:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">scale = tf.Variable(1.)
scale_layer = tf.keras.layers.Lambda(lambda x: x * scale)
</pre> <p>Because scale_layer does not directly track the <code translate="no" dir="ltr">scale</code> variable, it will not appear in <code translate="no" dir="ltr">scale_layer.trainable_weights</code> and will therefore not be trained if <code translate="no" dir="ltr">scale_layer</code> is used in a Model.</p> <p>A better pattern is to write a subclassed Layer:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class ScaleLayer(tf.keras.layers.Layer):
  def __init__(self):
    super(ScaleLayer, self).__init__()
    self.scale = tf.Variable(1.)

  def call(self, inputs):
    return inputs * self.scale
</pre> <p>In general, Lambda layers can be convenient for simple stateless computation, but anything more complex should use a subclass Layer instead.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">function</code> </td> <td> The function to be evaluated. Takes input tensor as first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_shape</code> </td> <td> Expected output shape from function. This argument can be inferred if not explicitly provided. Can be a tuple or function. If a tuple, it only specifies the first dimension onward; sample dimension is assumed either the same as the input: <code translate="no" dir="ltr">output_shape = (input_shape[0], ) + output_shape</code> or, the input is <code translate="no" dir="ltr">None</code> and the sample dimension is also <code translate="no" dir="ltr">None</code>: <code translate="no" dir="ltr">output_shape = (None, ) + output_shape</code> If a function, it specifies the entire shape as a function of the input shape: <code translate="no" dir="ltr">output_shape = f(input_shape)</code> </td> </tr>
<tr> <td> <code translate="no" dir="ltr">mask</code> </td> <td> Either None (indicating no masking) or a callable with the same signature as the <code translate="no" dir="ltr">compute_mask</code> layer method, or a tensor that will be returned as output mask regardless of what the input is. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">arguments</code> </td> <td> Optional dictionary of keyword arguments to be passed to the function. </td> </tr> </table> <h4 id="input_shape" data-text="Input shape:">Input shape:</h4> <p>Arbitrary. Use the keyword argument input_shape (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p> <h4 id="output_shape" data-text="Output shape:">Output shape:</h4> <p>Specified by <code translate="no" dir="ltr">output_shape</code> argument</p>  <devsite-thumb-rating position="footer"> <template class="thumb-down-categories"> [{ "type": "thumb-down", "id": "missingTheInformationINeed", "label":"Missing the information I need" },{ "type": "thumb-down", "id": "tooComplicatedTooManySteps", "label":"Too complicated / too many steps" },{ "type": "thumb-down", "id": "outOfDate", "label":"Out of date" },{ "type": "thumb-down", "id": "samplesCodeIssue", "label":"Samples / code issue" },{ "type": "thumb-down", "id": "otherDown", "label":"Other" }] </template> <template class="thumb-up-categories"> [{ "type": "thumb-up", "id": "easyToUnderstand", "label":"Easy to understand" },{ "type": "thumb-up", "id": "solvedMyProblem", "label":"Solved my problem" },{ "type": "thumb-up", "id": "otherUp", "label":"Other" }] </template> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Lambda" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/Lambda</a>
  </p>
</div>
