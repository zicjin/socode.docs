<h1 class="devsite-page-title">tf.tensor_scatter_nd_update</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/array_ops.py#L5228-L5512">  View source on GitHub </a> </td> </table> <p>"Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_nd_update</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update"><code translate="no" dir="ltr">tf.compat.v1.tensor_scatter_update</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
</pre>  <p>This operation creates a new tensor by applying sparse <code translate="no" dir="ltr">updates</code> to the input <code translate="no" dir="ltr">tensor</code>. This is similar to an index assignment.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp"># Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
</pre> <p>If an out of bound index is found on CPU, an error is returned.</p> <blockquote> <aside class="warning"><strong>Warning:</strong><span> There are some GPU specific semantics for this operation.</span></aside> <ul> <li>If an out of bound index is found, the index is ignored.</li> <li>The order in which updates are applied is nondeterministic, so the output will be nondeterministic if <code translate="no" dir="ltr">indices</code> contains duplicates.</li> </ul> </blockquote> <p>This operation is very similar to <a href="scatter_nd"><code translate="no" dir="ltr">tf.scatter_nd</code></a>, except that the updates are scattered onto an existing tensor (as opposed to a zero-tensor). If the memory for the existing tensor cannot be re-used, a copy is made and updated.</p> <h4 id="in_general" data-text="In general:">In general:</h4> <ul> <li>
<code translate="no" dir="ltr">indices</code> is an integer tensor - the indices to update in <code translate="no" dir="ltr">tensor</code>.</li> <li>
<code translate="no" dir="ltr">indices</code> has <strong>at least two</strong> axes, the last axis is the depth of the index vectors.</li> <li>For each index vector in <code translate="no" dir="ltr">indices</code> there is a corresponding entry in <code translate="no" dir="ltr">updates</code>.</li> <li>If the length of the index vectors matches the rank of the <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to scalars in <code translate="no" dir="ltr">tensor</code> and each update is a scalar.</li> <li>If the length of the index vectors is less than the rank of <code translate="no" dir="ltr">tensor</code>, then the index vectors each point to slices of <code translate="no" dir="ltr">tensor</code> and shape of the updates must match that slice.</li> </ul> <p>Overall this leads to the following shape constraints:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre> <p>Typical usage is often much simpler than this general form, and it can be better understood starting with simple examples:</p> <h3 id="scalar_updates" data-text="Scalar updates">Scalar updates</h3> <p>The simplest usage inserts scalar elements into a tensor by index. In this case, the <code translate="no" dir="ltr">index_depth</code> must equal the rank of the input <code translate="no" dir="ltr">tensor</code>, slice each column of <code translate="no" dir="ltr">indices</code> is an index into an axis of the input <code translate="no" dir="ltr">tensor</code>.</p> <p>In this simplest case the shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
assert updates.shape == [num_updates]
assert index_depth == tf.rank(tensor)`
</pre> <p>For example, to insert 4 scattered elements in a rank-1 tensor with 8 elements.</p> <div style="width:70%; margin:auto; margin-bottom:10px; margin-top:20px;"> <img style="width:100%" src="data:image/png;base64,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"> </div> <p>This scatter operation would look like this:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = [0, 0, 0, 0, 0, 0, 0, 0]    # tf.rank(tensor) == 1
indices = [[1], [3], [4], [7]]       # num_updates == 4, index_depth == 1
updates = [9, 10, 11, 12]            # num_updates == 4
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor([ 0 9  0 10  11  0  0 12], shape=(8,), dtype=int32)
</pre> <p>The length (first axis) of <code translate="no" dir="ltr">updates</code> must equal the length of the <code translate="no" dir="ltr">indices</code>: <code translate="no" dir="ltr">num_updates</code>. This is the the number of updates being inserted. Each scalar update is inserted into <code translate="no" dir="ltr">tensor</code> at the indexed location.</p> <p>For a higher rank input <code translate="no" dir="ltr">tensor</code> scalar updates can be inserted by using an <code translate="no" dir="ltr">index_depth</code> that matches <a href="rank"><code translate="no" dir="ltr">tf.rank(tensor)</code></a>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
updates = [5, 10]                    # num_updates == 2
print(tf.tensor_scatter_nd_update(tensor, indices, updates))
tf.Tensor(
    [[ 1  5]
     [ 1  1]
     [10  1]], shape=(3, 2), dtype=int32)
</pre> <h3 id="slice_updates" data-text="Slice updates">Slice updates</h3> <p>When the input <code translate="no" dir="ltr">tensor</code> has more than one axis scatter can be used to update entire slices.</p> <p>In this case it's helpful to think of the input <code translate="no" dir="ltr">tensor</code> as being a two level array-of-arrays. The shape of this two level array is split into the <code translate="no" dir="ltr">outer_shape</code> and the <code translate="no" dir="ltr">inner_shape</code>.</p> <p><code translate="no" dir="ltr">indices</code> indexes into the outer level of the input tensor (<code translate="no" dir="ltr">outer_shape</code>). and replaces the sub-array at that location with the coresponding item from the <code translate="no" dir="ltr">updates</code> list. The shape of each update is <code translate="no" dir="ltr">inner_shape</code>.</p> <p>When updating a list of slices the shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">num_updates, index_depth = indices.shape.as_list()
inner_shape = tensor.shape[:index_depth]
outer_shape = tensor.shape[index_depth:]
assert updates.shape == [num_updates, inner_shape]
</pre> <p>For example, to update rows of a <code translate="no" dir="ltr">(6, 3)</code> <code translate="no" dir="ltr">tensor</code>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = tf.zeros([6, 3], dtype=tf.int32)
</pre> <p>Use an index depth of one.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = tf.constant([[2], [4]])     # num_updates == 2, index_depth == 1
num_updates, index_depth = indices.shape.as_list()
</pre> <p>The <code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">6</code>, the inner shape is <code translate="no" dir="ltr">3</code>:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
</pre> <p>2 rows are being indexed so 2 <code translate="no" dir="ltr">updates</code> must be supplied. Each update must be shaped to match the <code translate="no" dir="ltr">inner_shape</code>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
# num_updates == 2, inner_shape==3
updates = tf.constant([[1, 2, 3],
                       [4, 5, 6]])
</pre> <p>Alltogether this gives:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[0, 0, 0],
       [0, 0, 0],
       [1, 2, 3],
       [0, 0, 0],
       [4, 5, 6],
       [0, 0, 0]], dtype=int32)
</pre> <h4 id="more_slice_update_examples" data-text="More slice update examples">More slice update examples</h4> <p>A tensor representing a batch of uniformly sized video clips naturally has 5 axes: <code translate="no" dir="ltr">[batch_size, time, width, height, channels]</code>.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
batch_size, time, width, height, channels = 13,11,7,5,3
video_batch = tf.zeros([batch_size, time, width, height, channels])
</pre> <p>To replace a selection of video clips:</p> <ul> <li>Use an <code translate="no" dir="ltr">index_depth</code> of 1 (indexing the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size]</code>)</li> <li>Provide updates each with a shape matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[time, width, height, channels]</code>.</li> </ul> <p>To relace the first two clips with ones:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = [[0],[1]]
new_clips = tf.ones([2, time, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_clips)
</pre> <p>To replace a selection of frames in the videos:</p> <ul> <li>
<code translate="no" dir="ltr">indices</code> must have an <code translate="no" dir="ltr">index_depth</code> of 2 for the <code translate="no" dir="ltr">outer_shape</code>: <code translate="no" dir="ltr">[batch_size, time]</code>.</li> <li>
<code translate="no" dir="ltr">updates</code> must be shaped like a list of images. Each update must have a shape, matching the <code translate="no" dir="ltr">inner_shape</code>: <code translate="no" dir="ltr">[width, height, channels]</code>.</li> </ul> <p>To replace the first frame of the first three video clips:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
indices = [[0, 0], [1, 0], [2, 0]] # num_updates=3, index_depth=2
new_images = tf.ones([
  # num_updates=3, inner_shape=(width, height, channels)
  3, width, height, channels])
tf.tensor_scatter_nd_update(video_batch, indices, new_images)
</pre> <h3 id="folded_indices" data-text="Folded indices">Folded indices</h3> <p>In simple cases it's convienient to think of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> as lists, but this is not a strict requirement. Instead of a flat <code translate="no" dir="ltr">num_updates</code>, the <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code> can be folded into a <code translate="no" dir="ltr">batch_shape</code>. This <code translate="no" dir="ltr">batch_shape</code> is all axes of the <code translate="no" dir="ltr">indices</code>, except for the innermost <code translate="no" dir="ltr">index_depth</code> axis.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The one exception is that the <code translate="no" dir="ltr">batch_shape</code> cannot be <code translate="no" dir="ltr">[]</code>. You can't update a single index by passing indices with shape <code translate="no" dir="ltr">[index_depth]</code>.</span>
</blockquote> <p><code translate="no" dir="ltr">updates</code> must have a matching <code translate="no" dir="ltr">batch_shape</code> (the axes before <code translate="no" dir="ltr">inner_shape</code>).</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert updates.shape == batch_shape + inner_shape
</pre>
<blockquote class="note">
<strong>Note:</strong><span> The result is equivalent to flattening the <code translate="no" dir="ltr">batch_shape</code> axes of <code translate="no" dir="ltr">indices</code> and <code translate="no" dir="ltr">updates</code>. This generalization just avoids the need for reshapes when it is more natural to construct "folded" indices and updates.</span>
</blockquote> <p>With this generalization the full shape constraints are:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">assert tf.rank(indices) &gt;= 2
index_depth = indices.shape[-1]
batch_shape = indices.shape[:-1]
assert index_depth &lt;= tf.rank(tensor)
outer_shape = tensor.shape[:index_depth]
inner_shape = tensor.shape[index_depth:]
assert updates.shape == batch_shape + inner_shape
</pre> <p>For example, to draw an <code translate="no" dir="ltr">X</code> on a <code translate="no" dir="ltr">(5,5)</code> matrix start with these indices:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tensor = tf.zeros([5,5])
indices = tf.constant([
 [[0,0],
  [1,1],
  [2,2],
  [3,3],
  [4,4]],
 [[0,4],
  [1,3],
  [2,2],
  [3,1],
  [4,0]],
])
indices.shape.as_list()  # batch_shape == [2, 5], index_depth == 2
[2, 5, 2]
</pre> <p>Here the <code translate="no" dir="ltr">indices</code> do not have a shape of <code translate="no" dir="ltr">[num_updates, index_depth]</code>, but a shape of <code translate="no" dir="ltr">batch_shape+[index_depth]</code>.</p> <p>Since the <code translate="no" dir="ltr">index_depth</code> is equal to the rank of <code translate="no" dir="ltr">tensor</code>:</p> <ul> <li>
<code translate="no" dir="ltr">outer_shape</code> is <code translate="no" dir="ltr">(5,5)</code>
</li> <li>
<code translate="no" dir="ltr">inner_shape</code> is <code translate="no" dir="ltr">()</code> - each update is scalar</li> <li>
<code translate="no" dir="ltr">updates.shape</code> is <code translate="no" dir="ltr">batch_shape + inner_shape == (5,2) + ()</code>
</li> </ul> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
updates = [
  [1,1,1,1,1],
  [1,1,1,1,1],
]
</pre> <p>Putting this together gives:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
array([[1., 0., 0., 0., 1.],
       [0., 1., 0., 1., 0.],
       [0., 0., 1., 0., 0.],
       [0., 1., 0., 1., 0.],
       [1., 0., 0., 0., 1.]], dtype=float32)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> Tensor to copy/update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">indices</code> </td> <td> Indices to update. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">updates</code> </td> <td> Updates to apply at the indices. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the operation. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A new tensor with the given shape and updates applied according to the indices. </td> </tr> 
</table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/tensor_scatter_nd_update" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/tensor_scatter_nd_update</a>
  </p>
</div>
