<h1 class="devsite-page-title">tf.data.experimental.map_and_batch</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/experimental/ops/batching.py#L207-L264">  View source on GitHub </a> </td> </table> <p>Fused implementation of <code translate="no" dir="ltr">map</code> and <code translate="no" dir="ltr">batch</code>. (deprecated)</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/map_and_batch"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.map_and_batch</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.map_and_batch(
    map_func, batch_size, num_parallel_batches=None, drop_remainder=False,
    num_parallel_calls=None
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use <a href="../dataset#map"><code translate="no" dir="ltr">tf.data.Dataset.map(map_func, num_parallel_calls)</code></a> followed by <a href="../dataset#batch"><code translate="no" dir="ltr">tf.data.Dataset.batch(batch_size, drop_remainder)</code></a>. Static tf.data optimizations will take care of using the fused implementation.</span></aside> <p>Maps <code translate="no" dir="ltr">map_func</code> across <code translate="no" dir="ltr">batch_size</code> consecutive elements of this dataset and then combines them into a batch. Functionally, it is equivalent to <code translate="no" dir="ltr">map</code> followed by <code translate="no" dir="ltr">batch</code>. This API is temporary and deprecated since input pipeline optimization now fuses consecutive <code translate="no" dir="ltr">map</code> and <code translate="no" dir="ltr">batch</code> operations automatically.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">map_func</code> </td> <td> A function mapping a nested structure of tensors to another nested structure of tensors. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">batch_size</code> </td> <td> A <a href="../../../tf#int64"><code translate="no" dir="ltr">tf.int64</code></a> scalar <a href="../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a>, representing the number of consecutive elements of this dataset to combine in a single batch. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_parallel_batches</code> </td> <td> (Optional.) A <a href="../../../tf#int64"><code translate="no" dir="ltr">tf.int64</code></a> scalar <a href="../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a>, representing the number of batches to create in parallel. On one hand, higher values can help mitigate the effect of stragglers. On the other hand, higher values can increase contention if CPU is scarce. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">drop_remainder</code> </td> <td> (Optional.) A <a href="../../../tf#bool"><code translate="no" dir="ltr">tf.bool</code></a> scalar <a href="../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a>, representing whether the last batch should be dropped in case its size is smaller than desired; the default behavior is not to drop the smaller batch. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_parallel_calls</code> </td> <td> (Optional.) A <a href="../../../tf#int32"><code translate="no" dir="ltr">tf.int32</code></a> scalar <a href="../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a>, representing the number of elements to process in parallel. If not specified, <code translate="no" dir="ltr">batch_size * num_parallel_batches</code> elements will be processed in parallel. If the value <a href="../../data#AUTOTUNE"><code translate="no" dir="ltr">tf.data.AUTOTUNE</code></a> is used, then the number of parallel calls is set dynamically based on available CPU. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Dataset</code> transformation function, which can be passed to <a href="../dataset#apply"><code translate="no" dir="ltr">tf.data.Dataset.apply</code></a>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If both <code translate="no" dir="ltr">num_parallel_batches</code> and <code translate="no" dir="ltr">num_parallel_calls</code> are specified. </td> </tr> </table>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/map_and_batch" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/map_and_batch</a>
  </p>
</div>
