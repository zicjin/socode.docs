<h1 class="devsite-page-title">Module: tf.compat.v1.train</h1>       <p>Support for training models.</p> <p>See the <a href="https://tensorflow.org/api_guides/python/train">Training</a> guide.</p> <h2 id="modules" data-text="Modules">Modules</h2> <p><a href="train/experimental"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.train.experimental namespace.</p> <p><a href="train/queue_runner"><code translate="no" dir="ltr">queue_runner</code></a> module: Public API for tf.train.queue_runner namespace.</p> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="train/adadeltaoptimizer"><code translate="no" dir="ltr">class AdadeltaOptimizer</code></a>: Optimizer that implements the Adadelta algorithm.</p> <p><a href="train/adagraddaoptimizer"><code translate="no" dir="ltr">class AdagradDAOptimizer</code></a>: Adagrad Dual Averaging algorithm for sparse linear models.</p> <p><a href="train/adagradoptimizer"><code translate="no" dir="ltr">class AdagradOptimizer</code></a>: Optimizer that implements the Adagrad algorithm.</p> <p><a href="train/adamoptimizer"><code translate="no" dir="ltr">class AdamOptimizer</code></a>: Optimizer that implements the Adam algorithm.</p> <p><a href="../../train/byteslist"><code translate="no" dir="ltr">class BytesList</code></a>: A ProtocolMessage</p> <p><a href="train/checkpoint"><code translate="no" dir="ltr">class Checkpoint</code></a>: Groups trackable objects, saving and restoring them.</p> <p><a href="../../train/checkpointmanager"><code translate="no" dir="ltr">class CheckpointManager</code></a>: Manages multiple checkpoints by keeping some and deleting unneeded ones.</p> <p><a href="../../train/checkpointoptions"><code translate="no" dir="ltr">class CheckpointOptions</code></a>: Options for constructing a Checkpoint.</p> <p><a href="../../estimator/checkpointsaverhook"><code translate="no" dir="ltr">class CheckpointSaverHook</code></a>: Saves checkpoints every N steps or seconds.</p> <p><a href="../../estimator/checkpointsaverlistener"><code translate="no" dir="ltr">class CheckpointSaverListener</code></a>: Interface for listeners that take action before or after checkpoint save.</p> <p><a href="train/chiefsessioncreator"><code translate="no" dir="ltr">class ChiefSessionCreator</code></a>: Creates a tf.compat.v1.Session for a chief.</p> <p><a href="../../train/clusterdef"><code translate="no" dir="ltr">class ClusterDef</code></a>: A ProtocolMessage</p> <p><a href="../../train/clusterspec"><code translate="no" dir="ltr">class ClusterSpec</code></a>: Represents a cluster as a set of "tasks", organized into "jobs".</p> <p><a href="../../train/coordinator"><code translate="no" dir="ltr">class Coordinator</code></a>: A coordinator for threads.</p> <p><a href="../../train/example"><code translate="no" dir="ltr">class Example</code></a>: A ProtocolMessage</p> <p><a href="../../train/exponentialmovingaverage"><code translate="no" dir="ltr">class ExponentialMovingAverage</code></a>: Maintains moving averages of variables by employing an exponential decay.</p> <p><a href="../../train/feature"><code translate="no" dir="ltr">class Feature</code></a>: A ProtocolMessage</p> <p><a href="../../train/featurelist"><code translate="no" dir="ltr">class FeatureList</code></a>: A ProtocolMessage</p> <p><a href="../../train/featurelists"><code translate="no" dir="ltr">class FeatureLists</code></a>: A ProtocolMessage</p> <p><a href="../../train/features"><code translate="no" dir="ltr">class Features</code></a>: A ProtocolMessage</p> <p><a href="../../estimator/feedfnhook"><code translate="no" dir="ltr">class FeedFnHook</code></a>: Runs <code translate="no" dir="ltr">feed_fn</code> and sets the <code translate="no" dir="ltr">feed_dict</code> accordingly.</p> <p><a href="../../estimator/finalopshook"><code translate="no" dir="ltr">class FinalOpsHook</code></a>: A hook which evaluates <code translate="no" dir="ltr">Tensors</code> at the end of a session.</p> <p><a href="../../train/floatlist"><code translate="no" dir="ltr">class FloatList</code></a>: A ProtocolMessage</p> <p><a href="train/ftrloptimizer"><code translate="no" dir="ltr">class FtrlOptimizer</code></a>: Optimizer that implements the FTRL algorithm.</p> <p><a href="../../estimator/globalstepwaiterhook"><code translate="no" dir="ltr">class GlobalStepWaiterHook</code></a>: Delays execution until global step reaches <code translate="no" dir="ltr">wait_until_step</code>.</p> <p><a href="train/gradientdescentoptimizer"><code translate="no" dir="ltr">class GradientDescentOptimizer</code></a>: Optimizer that implements the gradient descent algorithm.</p> <p><a href="../../train/int64list"><code translate="no" dir="ltr">class Int64List</code></a>: A ProtocolMessage</p> <p><a href="../../train/jobdef"><code translate="no" dir="ltr">class JobDef</code></a>: A ProtocolMessage</p> <p><a href="../../estimator/loggingtensorhook"><code translate="no" dir="ltr">class LoggingTensorHook</code></a>: Prints the given tensors every N local steps, every N seconds, or at end.</p> <p><a href="train/looperthread"><code translate="no" dir="ltr">class LooperThread</code></a>: A thread that runs code repeatedly, optionally on a timer.</p> <p><a href="train/momentumoptimizer"><code translate="no" dir="ltr">class MomentumOptimizer</code></a>: Optimizer that implements the Momentum algorithm.</p> <p><a href="train/monitoredsession"><code translate="no" dir="ltr">class MonitoredSession</code></a>: Session-like object that handles initialization, recovery and hooks.</p> <p><a href="../../estimator/nanlossduringtrainingerror"><code translate="no" dir="ltr">class NanLossDuringTrainingError</code></a>: Unspecified run-time error.</p> <p><a href="../../estimator/nantensorhook"><code translate="no" dir="ltr">class NanTensorHook</code></a>: Monitors the loss tensor and stops training if loss is NaN.</p> <p><a href="train/optimizer"><code translate="no" dir="ltr">class Optimizer</code></a>: Base class for optimizers.</p> <p><a href="../../estimator/profilerhook"><code translate="no" dir="ltr">class ProfilerHook</code></a>: Captures CPU/GPU profiling information every N steps or seconds.</p> <p><a href="train/proximaladagradoptimizer"><code translate="no" dir="ltr">class ProximalAdagradOptimizer</code></a>: Optimizer that implements the Proximal Adagrad algorithm.</p> <p><a href="train/proximalgradientdescentoptimizer"><code translate="no" dir="ltr">class ProximalGradientDescentOptimizer</code></a>: Optimizer that implements the proximal gradient descent algorithm.</p> <p><a href="train/queuerunner"><code translate="no" dir="ltr">class QueueRunner</code></a>: Holds a list of enqueue operations for a queue, each to be run in a thread.</p> <p><a href="train/rmspropoptimizer"><code translate="no" dir="ltr">class RMSPropOptimizer</code></a>: Optimizer that implements the RMSProp algorithm (Tielemans et al.</p> <p><a href="train/saver"><code translate="no" dir="ltr">class Saver</code></a>: Saves and restores variables.</p> <p><a href="train/saverdef"><code translate="no" dir="ltr">class SaverDef</code></a>: A ProtocolMessage</p> <p><a href="train/scaffold"><code translate="no" dir="ltr">class Scaffold</code></a>: Structure to create or gather pieces commonly needed to train a model.</p> <p><a href="../../estimator/secondorsteptimer"><code translate="no" dir="ltr">class SecondOrStepTimer</code></a>: Timer that triggers at most once every N seconds or once every N steps.</p> <p><a href="../../train/sequenceexample"><code translate="no" dir="ltr">class SequenceExample</code></a>: A ProtocolMessage</p> <p><a href="../../distribute/server"><code translate="no" dir="ltr">class Server</code></a>: An in-process TensorFlow server, for use in distributed training.</p> <p><a href="../../train/serverdef"><code translate="no" dir="ltr">class ServerDef</code></a>: A ProtocolMessage</p> <p><a href="train/sessioncreator"><code translate="no" dir="ltr">class SessionCreator</code></a>: A factory for tf.Session.</p> <p><a href="train/sessionmanager"><code translate="no" dir="ltr">class SessionManager</code></a>: Training helper that restores from checkpoint and creates session.</p> <p><a href="../../estimator/sessionrunargs"><code translate="no" dir="ltr">class SessionRunArgs</code></a>: Represents arguments to be added to a <code translate="no" dir="ltr">Session.run()</code> call.</p> <p><a href="../../estimator/sessionruncontext"><code translate="no" dir="ltr">class SessionRunContext</code></a>: Provides information about the <code translate="no" dir="ltr">session.run()</code> call being made.</p> <p><a href="../../estimator/sessionrunhook"><code translate="no" dir="ltr">class SessionRunHook</code></a>: Hook to extend calls to MonitoredSession.run().</p> <p><a href="../../estimator/sessionrunvalues"><code translate="no" dir="ltr">class SessionRunValues</code></a>: Contains the results of <code translate="no" dir="ltr">Session.run()</code>.</p> <p><a href="train/singularmonitoredsession"><code translate="no" dir="ltr">class SingularMonitoredSession</code></a>: Session-like object that handles initialization, restoring, and hooks.</p> <p><a href="../../estimator/stepcounterhook"><code translate="no" dir="ltr">class StepCounterHook</code></a>: Hook that counts steps per second.</p> <p><a href="../../estimator/stopatstephook"><code translate="no" dir="ltr">class StopAtStepHook</code></a>: Hook that requests stop at a specified step.</p> <p><a href="../../estimator/summarysaverhook"><code translate="no" dir="ltr">class SummarySaverHook</code></a>: Saves summaries every N steps.</p> <p><a href="train/supervisor"><code translate="no" dir="ltr">class Supervisor</code></a>: A training helper that checkpoints models and computes summaries.</p> <p><a href="train/syncreplicasoptimizer"><code translate="no" dir="ltr">class SyncReplicasOptimizer</code></a>: Class to synchronize, aggregate gradients and pass them to the optimizer.</p> <p><a href="../../estimator/vocabinfo"><code translate="no" dir="ltr">class VocabInfo</code></a>: Vocabulary information for warm-starting.</p> <p><a href="train/workersessioncreator"><code translate="no" dir="ltr">class WorkerSessionCreator</code></a>: Creates a tf.compat.v1.Session for a worker.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="train/monitoredtrainingsession"><code translate="no" dir="ltr">MonitoredTrainingSession(...)</code></a>: Creates a <code translate="no" dir="ltr">MonitoredSession</code> for training.</p> <p><a href="train/newcheckpointreader"><code translate="no" dir="ltr">NewCheckpointReader(...)</code></a>: A function that returns a CheckPointReader.</p> <p><a href="train/add_queue_runner"><code translate="no" dir="ltr">add_queue_runner(...)</code></a>: Adds a <code translate="no" dir="ltr">QueueRunner</code> to a collection in the graph. (deprecated)</p> <p><a href="train/assert_global_step"><code translate="no" dir="ltr">assert_global_step(...)</code></a>: Asserts <code translate="no" dir="ltr">global_step_tensor</code> is a scalar int <code translate="no" dir="ltr">Variable</code> or <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="train/basic_train_loop"><code translate="no" dir="ltr">basic_train_loop(...)</code></a>: Basic loop to train a model.</p> <p><a href="train/batch"><code translate="no" dir="ltr">batch(...)</code></a>: Creates batches of tensors in <code translate="no" dir="ltr">tensors</code>. (deprecated)</p> <p><a href="train/batch_join"><code translate="no" dir="ltr">batch_join(...)</code></a>: Runs a list of tensors to fill a queue to create batches of examples. (deprecated)</p> <p><a href="train/checkpoint_exists"><code translate="no" dir="ltr">checkpoint_exists(...)</code></a>: Checks whether a V1 or V2 checkpoint exists with the specified prefix. (deprecated)</p> <p><a href="../../train/checkpoints_iterator"><code translate="no" dir="ltr">checkpoints_iterator(...)</code></a>: Continuously yield new checkpoint files as they appear.</p> <p><a href="train/cosine_decay"><code translate="no" dir="ltr">cosine_decay(...)</code></a>: Applies cosine decay to the learning rate.</p> <p><a href="train/cosine_decay_restarts"><code translate="no" dir="ltr">cosine_decay_restarts(...)</code></a>: Applies cosine decay with restarts to the learning rate.</p> <p><a href="train/create_global_step"><code translate="no" dir="ltr">create_global_step(...)</code></a>: Create global step tensor in graph.</p> <p><a href="train/do_quantize_training_on_graphdef"><code translate="no" dir="ltr">do_quantize_training_on_graphdef(...)</code></a>: A general quantization scheme is being developed in <code translate="no" dir="ltr">tf.contrib.quantize</code>. (deprecated)</p> <p><a href="train/exponential_decay"><code translate="no" dir="ltr">exponential_decay(...)</code></a>: Applies exponential decay to the learning rate.</p> <p><a href="train/export_meta_graph"><code translate="no" dir="ltr">export_meta_graph(...)</code></a>: Returns <code translate="no" dir="ltr">MetaGraphDef</code> proto.</p> <p><a href="train/generate_checkpoint_state_proto"><code translate="no" dir="ltr">generate_checkpoint_state_proto(...)</code></a>: Generates a checkpoint state proto.</p> <p><a href="train/get_checkpoint_mtimes"><code translate="no" dir="ltr">get_checkpoint_mtimes(...)</code></a>: Returns the mtimes (modification timestamps) of the checkpoints. (deprecated)</p> <p><a href="../../train/get_checkpoint_state"><code translate="no" dir="ltr">get_checkpoint_state(...)</code></a>: Returns CheckpointState proto from the "checkpoint" file.</p> <p><a href="train/get_global_step"><code translate="no" dir="ltr">get_global_step(...)</code></a>: Get the global step tensor.</p> <p><a href="train/get_or_create_global_step"><code translate="no" dir="ltr">get_or_create_global_step(...)</code></a>: Returns and create (if necessary) the global step tensor.</p> <p><a href="train/global_step"><code translate="no" dir="ltr">global_step(...)</code></a>: Small helper to get the global step.</p> <p><a href="train/import_meta_graph"><code translate="no" dir="ltr">import_meta_graph(...)</code></a>: Recreates a Graph saved in a <code translate="no" dir="ltr">MetaGraphDef</code> proto.</p> <p><a href="train/init_from_checkpoint"><code translate="no" dir="ltr">init_from_checkpoint(...)</code></a>: Replaces <a href="../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> initializers so they load from a checkpoint file.</p> <p><a href="train/input_producer"><code translate="no" dir="ltr">input_producer(...)</code></a>: Output the rows of <code translate="no" dir="ltr">input_tensor</code> to a queue for an input pipeline. (deprecated)</p> <p><a href="train/inverse_time_decay"><code translate="no" dir="ltr">inverse_time_decay(...)</code></a>: Applies inverse time decay to the initial learning rate.</p> <p><a href="../../train/latest_checkpoint"><code translate="no" dir="ltr">latest_checkpoint(...)</code></a>: Finds the filename of latest saved checkpoint file.</p> <p><a href="train/limit_epochs"><code translate="no" dir="ltr">limit_epochs(...)</code></a>: Returns tensor <code translate="no" dir="ltr">num_epochs</code> times and then raises an <code translate="no" dir="ltr">OutOfRange</code> error. (deprecated)</p> <p><a href="train/linear_cosine_decay"><code translate="no" dir="ltr">linear_cosine_decay(...)</code></a>: Applies linear cosine decay to the learning rate.</p> <p><a href="../../train/list_variables"><code translate="no" dir="ltr">list_variables(...)</code></a>: Lists the checkpoint keys and shapes of variables in a checkpoint.</p> <p><a href="../../train/load_checkpoint"><code translate="no" dir="ltr">load_checkpoint(...)</code></a>: Returns <code translate="no" dir="ltr">CheckpointReader</code> for checkpoint found in <code translate="no" dir="ltr">ckpt_dir_or_file</code>.</p> <p><a href="../../train/load_variable"><code translate="no" dir="ltr">load_variable(...)</code></a>: Returns the tensor value of the given variable in the checkpoint.</p> <p><a href="../../io/match_filenames_once"><code translate="no" dir="ltr">match_filenames_once(...)</code></a>: Save the list of files matching pattern, so it is only computed once.</p> <p><a href="train/maybe_batch"><code translate="no" dir="ltr">maybe_batch(...)</code></a>: Conditionally creates batches of tensors based on <code translate="no" dir="ltr">keep_input</code>. (deprecated)</p> <p><a href="train/maybe_batch_join"><code translate="no" dir="ltr">maybe_batch_join(...)</code></a>: Runs a list of tensors to conditionally fill a queue to create batches. (deprecated)</p> <p><a href="train/maybe_shuffle_batch"><code translate="no" dir="ltr">maybe_shuffle_batch(...)</code></a>: Creates batches by randomly shuffling conditionally-enqueued tensors. (deprecated)</p> <p><a href="train/maybe_shuffle_batch_join"><code translate="no" dir="ltr">maybe_shuffle_batch_join(...)</code></a>: Create batches by randomly shuffling conditionally-enqueued tensors. (deprecated)</p> <p><a href="train/natural_exp_decay"><code translate="no" dir="ltr">natural_exp_decay(...)</code></a>: Applies natural exponential decay to the initial learning rate.</p> <p><a href="train/noisy_linear_cosine_decay"><code translate="no" dir="ltr">noisy_linear_cosine_decay(...)</code></a>: Applies noisy linear cosine decay to the learning rate.</p> <p><a href="train/piecewise_constant"><code translate="no" dir="ltr">piecewise_constant(...)</code></a>: Piecewise constant from boundaries and interval values.</p> <p><a href="train/piecewise_constant"><code translate="no" dir="ltr">piecewise_constant_decay(...)</code></a>: Piecewise constant from boundaries and interval values.</p> <p><a href="train/polynomial_decay"><code translate="no" dir="ltr">polynomial_decay(...)</code></a>: Applies a polynomial decay to the learning rate.</p> <p><a href="train/range_input_producer"><code translate="no" dir="ltr">range_input_producer(...)</code></a>: Produces the integers from 0 to limit-1 in a queue. (deprecated)</p> <p><a href="train/remove_checkpoint"><code translate="no" dir="ltr">remove_checkpoint(...)</code></a>: Removes a checkpoint given by <code translate="no" dir="ltr">checkpoint_prefix</code>. (deprecated)</p> <p><a href="train/replica_device_setter"><code translate="no" dir="ltr">replica_device_setter(...)</code></a>: Return a <code translate="no" dir="ltr">device function</code> to use when building a Graph for replicas.</p> <p><a href="train/sdca_fprint"><code translate="no" dir="ltr">sdca_fprint(...)</code></a>: Computes fingerprints of the input strings.</p> <p><a href="train/sdca_optimizer"><code translate="no" dir="ltr">sdca_optimizer(...)</code></a>: Distributed version of Stochastic Dual Coordinate Ascent (SDCA) optimizer for</p> <p><a href="train/sdca_shrink_l1"><code translate="no" dir="ltr">sdca_shrink_l1(...)</code></a>: Applies L1 regularization shrink step on the parameters.</p> <p><a href="train/shuffle_batch"><code translate="no" dir="ltr">shuffle_batch(...)</code></a>: Creates batches by randomly shuffling tensors. (deprecated)</p> <p><a href="train/shuffle_batch_join"><code translate="no" dir="ltr">shuffle_batch_join(...)</code></a>: Create batches by randomly shuffling tensors. (deprecated)</p> <p><a href="train/slice_input_producer"><code translate="no" dir="ltr">slice_input_producer(...)</code></a>: Produces a slice of each <code translate="no" dir="ltr">Tensor</code> in <code translate="no" dir="ltr">tensor_list</code>. (deprecated)</p> <p><a href="train/start_queue_runners"><code translate="no" dir="ltr">start_queue_runners(...)</code></a>: Starts all queue runners collected in the graph. (deprecated)</p> <p><a href="train/string_input_producer"><code translate="no" dir="ltr">string_input_producer(...)</code></a>: Output strings (e.g. filenames) to a queue for an input pipeline. (deprecated)</p> <p><a href="train/summary_iterator"><code translate="no" dir="ltr">summary_iterator(...)</code></a>: Returns a iterator for reading <code translate="no" dir="ltr">Event</code> protocol buffers from an event file.</p> <p><a href="train/update_checkpoint_state"><code translate="no" dir="ltr">update_checkpoint_state(...)</code></a>: Updates the content of the 'checkpoint' file. (deprecated)</p> <p><a href="train/warm_start"><code translate="no" dir="ltr">warm_start(...)</code></a>: Warm-starts a model using the given settings.</p> <p><a href="../../io/write_graph"><code translate="no" dir="ltr">write_graph(...)</code></a>: Writes a graph proto to a file.</p>  <devsite-thumb-rating position="footer"> <template class="thumb-down-categories"> [{ "type": "thumb-down", "id": "missingTheInformationINeed", "label":"Missing the information I need" },{ "type": "thumb-down", "id": "tooComplicatedTooManySteps", "label":"Too complicated / too many steps" },{ "type": "thumb-down", "id": "outOfDate", "label":"Out of date" },{ "type": "thumb-down", "id": "samplesCodeIssue", "label":"Samples / code issue" },{ "type": "thumb-down", "id": "otherDown", "label":"Other" }] </template> <template class="thumb-up-categories"> [{ "type": "thumb-up", "id": "easyToUnderstand", "label":"Easy to understand" },{ "type": "thumb-up", "id": "solvedMyProblem", "label":"Solved my problem" },{ "type": "thumb-up", "id": "otherUp", "label":"Other" }] </template> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/train" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/train</a>
  </p>
</div>
