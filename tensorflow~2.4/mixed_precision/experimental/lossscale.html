<h1 class="devsite-page-title">tf.mixed_precision.experimental.LossScale</h1>       <p>Base class for all TF1 loss scales.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Main aliases</b> </p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale"><code translate="no" dir="ltr">tf.train.experimental.LossScale</code></a></p> <b>Compat aliases for migration</b> <p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.LossScale</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.experimental.LossScale</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/LossScale"><code translate="no" dir="ltr">tf.compat.v1.train.experimental.LossScale</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.mixed_precision.experimental.LossScale()
</pre>  <aside class="warning"><strong>Warning:</strong><span> This class is deprecated and will be unexposed from the TF 2 namespace starting in TensorFlow 2.5. In TensorFlow 2.5, this class will only be accessible as <a href="lossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.LossScale</code></a>. Additionally in 2.5, you will no longer be able to pass a <code translate="no" dir="ltr">LossScale</code> to a <a href="../../keras/mixed_precision/policy"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a>. All the functionality in this class has been merged into <a href="../../keras/mixed_precision/lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.LossScaleOptimizer</code></a>, so this class is no longer needed.</span></aside> <p>This is an abstract base class, so you cannot instantiate it directly. Instead, use one of its concrete subclasses:</p> <ul> <li><a href="dynamiclossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.DynamicLossScale</code></a></li> <li><a href="fixedlossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.FixedLossScale</code></a></li> </ul> <p>Loss scaling is a process that multiplies the loss by a multiplier called the loss scale, and divides each gradient by the same multiplier. The pseudocode for this process is:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">loss = ...
loss *= loss_scale
grads = gradients(loss, vars)
grads /= loss_scale
</pre> <p>Mathematically, loss scaling has no effect, but can help avoid numerical underflow in intermediate gradients when float16 tensors are used for mixed precision training. By multiplying the loss, each intermediate gradient will have the same multiplier applied.</p> <p>Instances of this class represent a loss scale. Calling instances of this class returns the loss scale as a scalar float32 tensor, while method <code translate="no" dir="ltr">update()</code> updates the loss scale depending on the values of the gradients. Optimizers use instances of this class to scale loss and gradients.</p> <p>In most functions that accept a LossScale, you can also pass an int (such as</p> <p>8) to create a <code translate="no" dir="ltr">FixedLossScale</code> or the string <code translate="no" dir="ltr">"dynamic"</code> to create a dynamic loss scale.</p> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="from_config" data-text="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/training/experimental/loss_scale.py#L206-L209">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config
)
</pre> <p>Creates the LossScale from its config.</p> <h3 id="get_config" data-text="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/training/experimental/loss_scale.py#L201-L204">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@abc.abstractmethod
get_config()
</pre> <p>Returns the config of this loss scale.</p> <h3 id="update" data-text="update"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/training/experimental/loss_scale.py#L101-L136">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@abc.abstractmethod
update(
    grads
)
</pre> <p>Updates the value of the loss scale.</p> <p>The loss scale will be potentially updated, based on the value of <code translate="no" dir="ltr">grads</code>. The tensor returned by calling this class is only updated when this function is evaluated.</p> <p>In eager mode, this directly updates the loss scale, so that calling <code translate="no" dir="ltr">__call__</code> will return the newly updated loss scale. In graph mode, this returns an op that, when evaluated, updates the loss scale.</p> <p>This function also returns a <code translate="no" dir="ltr">should_apply_gradients</code> bool. If False, gradients should not be applied to the variables that step, as nonfinite gradients were found, and the loss scale has been be updated to reduce the chance of finding nonfinite gradients in the next step. Some loss scale classes will always return True, as they cannot adjust themselves in response to nonfinite gradients.</p> <p>When a DistributionStrategy is used, this function may only be called in a cross-replica context.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">grads</code> </td> <td> A nested structure of unscaled gradients, each which is the gradient of the loss with respect to a weight. The gradients should have already been divided by the loss scale being before passed to this function. 'None' gradients are accepted, and are ignored. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> 
<tr> <td> <code translate="no" dir="ltr">update_op</code> </td> <td> In eager mode, None. In graph mode, an op to update the loss scale. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">should_apply_gradients</code> </td> <td> Either a bool or a scalar boolean tensor. If False, the caller should skip applying <code translate="no" dir="ltr">grads</code> to the variables this step. </td> </tr> </table> <h3 id="__call__" data-text="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/training/experimental/loss_scale.py#L96-L99">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@abc.abstractmethod
__call__()
</pre> <p>Returns the current loss scale as a scalar <code translate="no" dir="ltr">float32</code> tensor.</p>  <devsite-thumb-rating position="footer"> </devsite-thumb-rating><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mixed_precision/experimental/LossScale" class="_attribution-link" target="_blank">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/mixed_precision/experimental/LossScale</a>
  </p>
</div>
