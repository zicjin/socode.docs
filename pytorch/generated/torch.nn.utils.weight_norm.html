<h1 id="torch-nn-utils-weight-norm">torch.nn.utils.weight_norm</h1> <dl class="function"> <dt id="torch.nn.utils.weight_norm">
<code>torch.nn.utils.weight_norm(module, name='weight', dim=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/utils/weight_norm.html#weight_norm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies weight normalization to a parameter in the given module.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="bold">w</mi><mo>=</mo><mi>g</mi><mfrac><mi mathvariant="bold">v</mi><mrow><mi mathvariant="normal">∥</mi><mi mathvariant="bold">v</mi><mi mathvariant="normal">∥</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|} </annotation></semantics></math></span></span></span> </div>
<p>Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by <code>name</code> (e.g. <code>'weight'</code>) with two parameters: one specifying the magnitude (e.g. <code>'weight_g'</code>) and one specifying the direction (e.g. <code>'weight_v'</code>). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every <code>forward()</code> call.</p> <p>By default, with <code>dim=0</code>, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use <code>dim=None</code>.</p> <p>See <a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – containing module</li> <li>
<strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>, </em><em>optional</em>) – name of weight parameter</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – dimension over which to compute the norm</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>The original module with the weight norm hook</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')
&gt;&gt;&gt; m
Linear(in_features=20, out_features=40, bias=True)
&gt;&gt;&gt; m.weight_g.size()
torch.Size([40, 1])
&gt;&gt;&gt; m.weight_v.size()
torch.Size([40, 20])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.utils.weight_norm.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/generated/torch.nn.utils.weight_norm.html</a>
  </p>
</div>
