<h1 id="torch-tensor">torch.tensor</h1> <dl class="function"> <dt id="torch.tensor">
<code>torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor</code> </dt> <dd>
<p>Constructs a tensor with <code>data</code>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> always copies <code>data</code>. If you have a Tensor <code>data</code> and want to avoid a copy, use <a class="reference internal" href="../tensors#torch.Tensor.requires_grad_" title="torch.Tensor.requires_grad_"><code>torch.Tensor.requires_grad_()</code></a> or <a class="reference internal" href="../autograd#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a>. If you have a NumPy <code>ndarray</code> and want to avoid a copy, use <a class="reference internal" href="torch.as_tensor#torch.as_tensor" title="torch.as_tensor"><code>torch.as_tensor()</code></a>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>When data is a tensor <code>x</code>, <a class="reference internal" href="#torch.tensor" title="torch.tensor"><code>torch.tensor()</code></a> reads out ‘the data’ from whatever it is passed, and constructs a leaf variable. Therefore <code>torch.tensor(x)</code> is equivalent to <code>x.clone().detach()</code> and <code>torch.tensor(x, requires_grad=True)</code> is equivalent to <code>x.clone().detach().requires_grad_(True)</code>. The equivalents using <code>clone()</code> and <code>detach()</code> are recommended.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>data</strong> (<em>array_like</em>) – Initial data for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes#torch.torch.dtype" title="torch.torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if <code>None</code>, infers data type from <code>data</code>.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes#torch.torch.device" title="torch.torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if <code>None</code>, uses the current device for the default tensor type (see <a class="reference internal" href="torch.set_default_tensor_type#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <code>device</code> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> <li>
<strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: <code>False</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])
tensor([[ 0.1000,  1.2000],
        [ 2.2000,  3.1000],
        [ 4.9000,  5.2000]])

&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data
tensor([ 0,  1])

&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],
...              dtype=torch.float64,
...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor
tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')

&gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)
tensor(3.1416)

&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))
tensor([])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.tensor.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/generated/torch.tensor.html</a>
  </p>
</div>
