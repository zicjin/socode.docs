<h1 id="gaussiannllloss">GaussianNLLLoss</h1> <dl class="class"> <dt id="torch.nn.GaussianNLLLoss">
<code>class torch.nn.GaussianNLLLoss(*, full=False, eps=1e-06, reduction='mean')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/loss.html#GaussianNLLLoss"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gaussian negative log likelihood loss.</p> <p>The targets are treated as samples from Gaussian distributions with expectations and variances predicted by the neural network. For a D-dimensional <code>target</code> tensor modelled as having heteroscedastic Gaussian distributions with a D-dimensional tensor of expectations <code>input</code> and a D-dimensional tensor of positive variances <code>var</code> the loss is:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>loss</mtext><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><mrow><mo fence="true">(</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mtext>max</mtext><mrow><mo fence="true">(</mo><mtext>var</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo separator="true">,</mo><mtext> eps</mtext><mo fence="true">)</mo></mrow><mo fence="true">)</mo></mrow><mo>+</mo><mfrac><msup><mrow><mo fence="true">(</mo><mtext>input</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>−</mo><mtext>target</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo fence="true">)</mo></mrow><mn>2</mn></msup><mrow><mtext>max</mtext><mrow><mo fence="true">(</mo><mtext>var</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo separator="true">,</mo><mtext> eps</mtext><mo fence="true">)</mo></mrow></mrow></mfrac><mo fence="true">)</mo></mrow><mo>+</mo><mtext>const.</mtext></mrow><annotation encoding="application/x-tex">\text{loss} = \frac{1}{2}\sum_{i=1}^D \left(\log\left(\text{max}\left(\text{var}[i], \ \text{eps}\right)\right) + \frac{\left(\text{input}[i] - \text{target}[i]\right)^2} {\text{max}\left(\text{var}[i], \ \text{eps}\right)}\right) + \text{const.} </annotation></semantics></math></span></span></span> </div>
<p>where <code>eps</code> is used for stability. By default, the constant term of the loss function is omitted unless <code>full</code> is <code>True</code>. If <code>var</code> is a scalar (implying <code>target</code> tensor has homoscedastic Gaussian distributions) it is broadcasted to be the same size as the input.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>full</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – include the constant term in the loss calculation. Default: <code>False</code>.</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><em>optional</em>) – value used to clamp <code>var</code> (see note below), for stability. Default: 1e-6.</li> <li>
<strong>reduction</strong> (<em>string</em><em>, </em><em>optional</em>) – specifies the reduction to apply to the output:<code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the output is the average of all batch member losses, <code>'sum'</code>: the output is the sum of all batch member losses. Default: <code>'mean'</code>.</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span> </span> means any number of additional dimensions</li> <li>Target: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span> </span>, same shape as the input</li> <li>Var: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, 1)</annotation></semantics></math></span></span> </span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *)</annotation></semantics></math></span></span> </span>, same shape as the input</li> <li>Output: scalar if <code>reduction</code> is <code>'mean'</code> (default) or <code>'sum'</code>. If <code>reduction</code> is <code>'none'</code>, then <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span> </span>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; loss = nn.GaussianNLLLoss()
&gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; var = torch.ones(5, 2, requires_grad=True) #heteroscedastic
&gt;&gt;&gt; output = loss(input, target, var)
&gt;&gt;&gt; output.backward()


&gt;&gt;&gt; loss = nn.GaussianNLLLoss()
&gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)
&gt;&gt;&gt; target = torch.randn(5, 2)
&gt;&gt;&gt; var = torch.ones(5, 1, requires_grad=True) #homoscedastic
&gt;&gt;&gt; output = loss(input, target, var)
&gt;&gt;&gt; output.backward()
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The clamping of <code>var</code> is ignored with respect to autograd, and so the gradients are unaffected by it.</p> </div> <dl class="simple"> <dt>Reference:</dt>
<dd>
<p>Nix, D. A. and Weigend, A. S., “Estimating the mean and variance of the target probability distribution”, Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94), Orlando, FL, USA, 1994, pp. 55-60 vol.1, doi: 10.1109/ICNN.1994.374138.</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.GaussianNLLLoss.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/generated/torch.nn.GaussianNLLLoss.html</a>
  </p>
</div>
