<h1 id="torch-jit-freeze">torch.jit.freeze</h1> <dl class="function"> <dt id="torch.jit.freeze">
<code>torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/jit/_freeze.html#freeze"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Freezing a <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a> will clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, <code>forward</code> will be preserved, as well as attributes &amp; methods specified in <code>preserved_attrs</code>. Additionally, any attribute that is modified within a preserved method will be preserved.</p> <p>Freezing currently only accepts ScriptModules that are in eval mode.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mod</strong> (<a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a>) – a module to be frozen</li> <li>
<strong>preserved_attrs</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>]</em><em>]</em>) – a list of attributes to preserve in addition to the forward method.</li> <li>
<strong>modified in preserved methods will also be preserved.</strong> (<em>Attributes</em>) – </li> <li>
<strong>optimize_numerics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – If <code>True</code>, a set of optimization passes will be run that does not strictly</li> <li>
<strong>numerics. Full details of optimization can be found at torch.jit.optimize_frozen_module.</strong> (<em>preserve</em>) – </li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Frozen <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a>.</p> </dd> </dl> <p>Example (Freezing a simple module with a Parameter):</p> <pre data-language="python">    def forward(self, input):
        output = self.weight.mm(input)
        output = self.linear(output)
        return output

scripted_module = torch.jit.script(MyModule(2, 3).eval())
frozen_module = torch.jit.freeze(scripted_module)
# parameters have been removed and inlined into the Graph as constants
assert len(list(frozen_module.named_parameters())) == 0
# See the compiled graph as Python code
print(frozen_module.code)
</pre> <p>Example (Freezing a module with preserved attributes)</p> <pre data-language="python">    def forward(self, input):
        self.modified_tensor += 1
        return input + self.modified_tensor

scripted_module = torch.jit.script(MyModule2().eval())
frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=["version"])
# we've manually preserved `version`, so it still exists on the frozen module and can be modified
assert frozen_module.version == 1
frozen_module.version = 2
# `modified_tensor` is detected as being mutated in the forward, so freezing preserves
# it to retain model semantics
assert frozen_module(torch.tensor(1)) == torch.tensor(12)
# now that we've run it once, the next result will be incremented by one
assert frozen_module(torch.tensor(1)) == torch.tensor(13)
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If you’re not sure why an attribute is not being inlined as a constant, you can run <code>dump_alias_db</code> on frozen_module.forward.graph to see if freezing has detected the attribute is being modified.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.jit.freeze.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/generated/torch.jit.freeze.html</a>
  </p>
</div>
