<h1 id="torch-use-deterministic-algorithms">torch.use_deterministic_algorithms</h1> <dl class="function"> <dt id="torch.use_deterministic_algorithms">
<code>torch.use_deterministic_algorithms(d)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch.html#use_deterministic_algorithms"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets whether PyTorch operations must use “deterministic” algorithms. That is, algorithms which, given the same input, and when run on the same software and hardware, always produce the same output. When True, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw a :class:RuntimeError when called.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This feature is in beta, and its design and implementation may change in the future.</p> </div> <p>The following normally-nondeterministic operations will act deterministically when <code>d=True</code>:</p>  <ul class="simple"> <li>
<a class="reference internal" href="torch.nn.conv1d#torch.nn.Conv1d" title="torch.nn.Conv1d"><code>torch.nn.Conv1d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.conv2d#torch.nn.Conv2d" title="torch.nn.Conv2d"><code>torch.nn.Conv2d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.conv3d#torch.nn.Conv3d" title="torch.nn.Conv3d"><code>torch.nn.Conv3d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose1d#torch.nn.ConvTranspose1d" title="torch.nn.ConvTranspose1d"><code>torch.nn.ConvTranspose1d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose2d#torch.nn.ConvTranspose2d" title="torch.nn.ConvTranspose2d"><code>torch.nn.ConvTranspose2d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.nn.convtranspose3d#torch.nn.ConvTranspose3d" title="torch.nn.ConvTranspose3d"><code>torch.nn.ConvTranspose3d</code></a> when called on CUDA tensor</li> <li>
<a class="reference internal" href="torch.bmm#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a> when called on sparse-dense CUDA tensors</li> <li>
<code>torch.__getitem__()</code> backward when <code>self</code> is a CPU tensor and <code>indices</code> is a list of tensors</li> <li>
<code>torch.index_put()</code> with <code>accumulate=True</code> when called on a CPU tensor</li> </ul>  <p>The following normally-nondeterministic operations will throw a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.9)"><code>RuntimeError</code></a> when <code>d=True</code>:</p>  <ul class="simple"> <li>
<a class="reference internal" href="torch.nn.avgpool3d#torch.nn.AvgPool3d" title="torch.nn.AvgPool3d"><code>torch.nn.AvgPool3d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.adaptiveavgpool2d#torch.nn.AdaptiveAvgPool2d" title="torch.nn.AdaptiveAvgPool2d"><code>torch.nn.AdaptiveAvgPool2d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.adaptiveavgpool3d#torch.nn.AdaptiveAvgPool3d" title="torch.nn.AdaptiveAvgPool3d"><code>torch.nn.AdaptiveAvgPool3d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.maxpool3d#torch.nn.MaxPool3d" title="torch.nn.MaxPool3d"><code>torch.nn.MaxPool3d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.adaptivemaxpool2d#torch.nn.AdaptiveMaxPool2d" title="torch.nn.AdaptiveMaxPool2d"><code>torch.nn.AdaptiveMaxPool2d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.fractionalmaxpool2d#torch.nn.FractionalMaxPool2d" title="torch.nn.FractionalMaxPool2d"><code>torch.nn.FractionalMaxPool2d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<code>torch.nn.FractionalMaxPool3d</code> when called on a CUDA tensor that requires grad</li> <li>
<p><a class="reference internal" href="../nn.functional#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>torch.nn.functional.interpolate()</code></a> when called on a CUDA tensor that requires grad and one of the following modes is used:</p> <ul> <li><code>linear</code></li> <li><code>bilinear</code></li> <li><code>bicubic</code></li> <li><code>trilinear</code></li> </ul> </li> <li>
<a class="reference internal" href="torch.nn.reflectionpad1d#torch.nn.ReflectionPad1d" title="torch.nn.ReflectionPad1d"><code>torch.nn.ReflectionPad1d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.reflectionpad2d#torch.nn.ReflectionPad2d" title="torch.nn.ReflectionPad2d"><code>torch.nn.ReflectionPad2d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.replicationpad1d#torch.nn.ReplicationPad1d" title="torch.nn.ReplicationPad1d"><code>torch.nn.ReplicationPad1d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.replicationpad2d#torch.nn.ReplicationPad2d" title="torch.nn.ReplicationPad2d"><code>torch.nn.ReplicationPad2d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.replicationpad3d#torch.nn.ReplicationPad3d" title="torch.nn.ReplicationPad3d"><code>torch.nn.ReplicationPad3d</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.nllloss#torch.nn.NLLLoss" title="torch.nn.NLLLoss"><code>torch.nn.NLLLoss</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.ctcloss#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code>torch.nn.CTCLoss</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.nn.embeddingbag#torch.nn.EmbeddingBag" title="torch.nn.EmbeddingBag"><code>torch.nn.EmbeddingBag</code></a> when called on a CUDA tensor that requires grad</li> <li>
<code>torch.scatter_add_()</code> when called on a CUDA tensor</li> <li>
<code>torch.index_add_()</code> when called on a CUDA tensor</li> <li><code>torch.index_copy()</code></li> <li>
<a class="reference internal" href="torch.index_select#torch.index_select" title="torch.index_select"><code>torch.index_select()</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.repeat_interleave#torch.repeat_interleave" title="torch.repeat_interleave"><code>torch.repeat_interleave()</code></a> when called on a CUDA tensor that requires grad</li> <li>
<a class="reference internal" href="torch.histc#torch.histc" title="torch.histc"><code>torch.histc()</code></a> when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.bincount#torch.bincount" title="torch.bincount"><code>torch.bincount()</code></a> when called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.kthvalue#torch.kthvalue" title="torch.kthvalue"><code>torch.kthvalue()</code></a> with called on a CUDA tensor</li> <li>
<a class="reference internal" href="torch.median#torch.median" title="torch.median"><code>torch.median()</code></a> with indices output when called on a CUDA tensor</li> </ul>  <p>A handful of CUDA operations are nondeterministic if the CUDA version is 10.2 or greater, unless the environment variable <code>CUBLAS_WORKSPACE_CONFIG=:4096:8</code> or <code>CUBLAS_WORKSPACE_CONFIG=:16:8</code> is set. See the CUDA documentation for more details: <a class="reference external" href="https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility">https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility</a> If one of these environment variable configurations is not set, a <a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.9)"><code>RuntimeError</code></a> will be raised from these operations when called with CUDA tensors:</p>  <ul class="simple"> <li><a class="reference internal" href="torch.mm#torch.mm" title="torch.mm"><code>torch.mm()</code></a></li> <li><a class="reference internal" href="torch.mv#torch.mv" title="torch.mv"><code>torch.mv()</code></a></li> <li><a class="reference internal" href="torch.bmm#torch.bmm" title="torch.bmm"><code>torch.bmm()</code></a></li> </ul>  <p>Note that deterministic operations tend to have worse performance than non-deterministic operations.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>d</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)"><code>bool</code></a>) – If True, force operations to be deterministic. If False, allow non-deterministic operations.</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.use_deterministic_algorithms.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/generated/torch.use_deterministic_algorithms.html</a>
  </p>
</div>
