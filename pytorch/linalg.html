<h1 id="torch-linalg">torch.linalg</h1> <p>Common linear algebra operations.</p> <p>This module is in BETA. New functions are still being added, and some functions may change in future PyTorch releases. See the documentation of each function for details.</p>  <h2 id="functions">Functions</h2> <dl class="function"> <dt id="torch.linalg.cholesky">
<code>torch.linalg.cholesky(input, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the Cholesky decomposition of a Hermitian (or symmetric for real-valued matrices) positive-definite matrix or the Cholesky decompositions for a batch of such matrices. Each decomposition has the form:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>L</mi><msup><mi>L</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">\text{input} = LL^H</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> </span> is a lower-triangular matrix and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">L^H</annotation></semantics></math></span></span> </span> is the conjugate transpose of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> </span>, which is just a transpose for the case of real-valued input matrices. In code it translates to <code>input = L @ L.t()</code> if <code>input</code> is real-valued and <code>input = L @ L.conj().t()</code> if <code>input</code> is complex-valued. The batch of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span></span> </span> matrices is returned.</p> <p>Supports real-valued and complex-valued inputs.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>LAPACK’s <code>potrf</code> is used for CPU inputs, and MAGMA’s <code>potrf</code> is used for CUDA inputs.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <code>input</code> is not a Hermitian positive-definite matrix, or if it’s a batch of matrices and one or more of them is not a Hermitian positive-definite matrix, then a RuntimeError will be thrown. If <code>input</code> is a batch of matrices, then the error message will include the batch index of the first matrix that is not Hermitian positive-definite.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, n, n)</annotation></semantics></math></span></span> </span> consisting of Hermitian positive-definite <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span></span> </span> matrices, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span> </span> is zero or more batch dimensions.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default: <code>None</code></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(2, 2, dtype=torch.complex128)
&gt;&gt;&gt; a = torch.mm(a, a.t().conj())  # creates a Hermitian positive-definite matrix
&gt;&gt;&gt; l = torch.linalg.cholesky(a)
&gt;&gt;&gt; a
tensor([[2.5266+0.0000j, 1.9586-2.0626j],
        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; l
tensor([[1.5895+0.0000j, 0.0000+0.0000j],
        [1.2322+1.2976j, 2.4928+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; torch.mm(l, l.t().conj())
tensor([[2.5266+0.0000j, 1.9586-2.0626j],
        [1.9586+2.0626j, 9.4160+0.0000j]], dtype=torch.complex128)

&gt;&gt;&gt; a = torch.randn(3, 2, 2, dtype=torch.float64)
&gt;&gt;&gt; a = torch.matmul(a, a.transpose(-2, -1))  # creates a symmetric positive-definite matrix
&gt;&gt;&gt; l = torch.linalg.cholesky(a)
&gt;&gt;&gt; a
tensor([[[ 1.1629,  2.0237],
        [ 2.0237,  6.6593]],

        [[ 0.4187,  0.1830],
        [ 0.1830,  0.1018]],

        [[ 1.9348, -2.5744],
        [-2.5744,  4.6386]]], dtype=torch.float64)
&gt;&gt;&gt; l
tensor([[[ 1.0784,  0.0000],
        [ 1.8766,  1.7713]],

        [[ 0.6471,  0.0000],
        [ 0.2829,  0.1477]],

        [[ 1.3910,  0.0000],
        [-1.8509,  1.1014]]], dtype=torch.float64)
&gt;&gt;&gt; torch.allclose(torch.matmul(l, l.transpose(-2, -1)), a)
True
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.cond">
<code>torch.linalg.cond(input, p=None, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the condition number of a matrix <code>input</code>, or of each matrix in a batched <code>input</code>, using the matrix norm defined by <code>p</code>.</p> <p>For norms <code>{‘fro’, ‘nuc’, inf, -inf, 1, -1}</code> this is defined as the matrix norm of <code>input</code> times the matrix norm of the inverse of <code>input</code> computed using <a class="reference internal" href="#torch.linalg.norm" title="torch.linalg.norm"><code>torch.linalg.norm()</code></a>. While for norms <code>{None, 2, -2}</code> this is defined as the ratio between the largest and smallest singular values computed using <a class="reference internal" href="#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>.</p> <p>This function supports float, double, cfloat and cdouble dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function may synchronize that device with the CPU depending on which norm <code>p</code> is used.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>For norms <code>{None, 2, -2}</code>, <code>input</code> may be a non-square matrix or batch of non-square matrices. For other norms, however, <code>input</code> must be a square matrix or a batch of square matrices, and if this requirement is not satisfied a RuntimeError will be thrown.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>For norms <code>{‘fro’, ‘nuc’, inf, -inf, 1, -1}</code> if <code>input</code> is a non-invertible matrix then a tensor containing infinity will be returned. If <code>input</code> is a batch of matrices and one or more of them is not invertible then a RuntimeError will be thrown.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input matrix of size <code>(m, n)</code> or the batch of matrices of size <code>(*, m, n)</code> where <code>*</code> is one or more batch dimensions.</li> <li>
<p><strong>p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><em>inf</em><em>, </em><em>-inf</em><em>, </em><em>'fro'</em><em>, </em><em>'nuc'</em><em>, </em><em>optional</em>) – </p>
<p>the type of the matrix norm to use in the computations. inf refers to <code>float('inf')</code>, numpy’s <code>inf</code> object, or any equivalent object. The following norms can be used:</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>p</p></th> <th class="head"><p>norm for matrices</p></th> </tr> </thead>  <tr>
<td><p>None</p></td> <td><p>ratio of the largest singular value to the smallest singular value</p></td> </tr> <tr>
<td><p>’fro’</p></td> <td><p>Frobenius norm</p></td> </tr> <tr>
<td><p>’nuc’</p></td> <td><p>nuclear norm</p></td> </tr> <tr>
<td><p>inf</p></td> <td><p>max(sum(abs(x), dim=1))</p></td> </tr> <tr>
<td><p>-inf</p></td> <td><p>min(sum(abs(x), dim=1))</p></td> </tr> <tr>
<td><p>1</p></td> <td><p>max(sum(abs(x), dim=0))</p></td> </tr> <tr>
<td><p>-1</p></td> <td><p>min(sum(abs(x), dim=0))</p></td> </tr> <tr>
<td><p>2</p></td> <td><p>ratio of the largest singular value to the smallest singular value</p></td> </tr> <tr>
<td><p>-2</p></td> <td><p>ratio of the smallest singular value to the largest singular value</p></td> </tr>  </table> <p>Default: <code>None</code></p> </li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – tensor to write the output to. Default is <code>None</code>.</p> </dd> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>The condition number of <code>input</code>. The output dtype is always real valued even for complex inputs (e.g. float if <code>input</code> is cfloat).</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(3, 4, 4, dtype=torch.complex64)
&gt;&gt;&gt; torch.linalg.cond(a)
&gt;&gt;&gt; a = torch.tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])
&gt;&gt;&gt; torch.linalg.cond(a)
tensor([1.4142])
&gt;&gt;&gt; torch.linalg.cond(a, 'fro')
tensor(3.1623)
&gt;&gt;&gt; torch.linalg.cond(a, 'nuc')
tensor(9.2426)
&gt;&gt;&gt; torch.linalg.cond(a, float('inf'))
tensor(2.)
&gt;&gt;&gt; torch.linalg.cond(a, float('-inf'))
tensor(1.)
&gt;&gt;&gt; torch.linalg.cond(a, 1)
tensor(2.)
&gt;&gt;&gt; torch.linalg.cond(a, -1)
tensor(1.)
&gt;&gt;&gt; torch.linalg.cond(a, 2)
tensor([1.4142])
&gt;&gt;&gt; torch.linalg.cond(a, -2)
tensor([0.7071])

&gt;&gt;&gt; a = torch.randn(2, 3, 3)
&gt;&gt;&gt; a
tensor([[[-0.9204,  1.1140,  1.2055],
        [ 0.3988, -0.2395, -0.7441],
        [-0.5160,  0.3115,  0.2619]],

        [[-2.2128,  0.9241,  2.1492],
        [-1.1277,  2.7604, -0.8760],
        [ 1.2159,  0.5960,  0.0498]]])
&gt;&gt;&gt; torch.linalg.cond(a)
tensor([[9.5917],
        [3.2538]])

&gt;&gt;&gt; a = torch.randn(2, 3, 3, dtype=torch.complex64)
&gt;&gt;&gt; a
tensor([[[-0.4671-0.2137j, -0.1334-0.9508j,  0.6252+0.1759j],
        [-0.3486-0.2991j, -0.1317+0.1252j,  0.3025-0.1604j],
        [-0.5634+0.8582j,  0.1118-0.4677j, -0.1121+0.7574j]],

        [[ 0.3964+0.2533j,  0.9385-0.6417j, -0.0283-0.8673j],
        [ 0.2635+0.2323j, -0.8929-1.1269j,  0.3332+0.0733j],
        [ 0.1151+0.1644j, -1.1163+0.3471j, -0.5870+0.1629j]]])
&gt;&gt;&gt; torch.linalg.cond(a)
tensor([[4.6245],
        [4.5671]])
&gt;&gt;&gt; torch.linalg.cond(a, 1)
tensor([9.2589, 9.3486])
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.det">
<code>torch.linalg.det(input) → Tensor</code> </dt> <dd>
<p>Computes the determinant of a square matrix <code>input</code>, or of each square matrix in a batched <code>input</code>.</p> <p>This function supports float, double, cfloat and cdouble dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The determinant is computed using LU factorization. LAPACK’s <code>getrf</code> is used for CPU inputs, and MAGMA’s <code>getrf</code> is used for CUDA inputs.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Backward through <code>det</code> internally uses <a class="reference internal" href="#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> when <code>input</code> is not invertible. In this case, double backward through <code>det</code> will be unstable when <code>input</code> doesn’t have distinct singular values. See <a class="reference internal" href="#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> for more details.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input matrix of size <code>(n, n)</code> or the batch of matrices of size <code>(*, n, n)</code> where <code>*</code> is one or more batch dimensions.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; a
tensor([[ 0.9478,  0.9158, -1.1295],
        [ 0.9701,  0.7346, -1.8044],
        [-0.2337,  0.0557,  0.6929]])
&gt;&gt;&gt; torch.linalg.det(a)
tensor(0.0934)

&gt;&gt;&gt; a = torch.randn(3, 2, 2)
&gt;&gt;&gt; a
tensor([[[ 0.9254, -0.6213],
         [-0.5787,  1.6843]],

        [[ 0.3242, -0.9665],
         [ 0.4539, -0.0887]],

        [[ 1.1336, -0.4025],
         [-0.7089,  0.9032]]])
&gt;&gt;&gt; torch.linalg.det(a)
tensor([1.1990, 0.4099, 0.7386])
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.slogdet">
<code>torch.linalg.slogdet(input, *, out=None) -&gt; (Tensor, Tensor)</code> </dt> <dd>
<p>Calculates the sign and natural logarithm of the absolute value of a square matrix’s determinant, or of the absolute values of the determinants of a batch of square matrices <code>input</code>. The determinant can be computed with <code>sign * exp(logabsdet)</code>.</p> <p>Supports input of float, double, cfloat and cdouble datatypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The determinant is computed using LU factorization. LAPACK’s <code>getrf</code> is used for CPU inputs, and MAGMA’s <code>getrf</code> is used for CUDA inputs.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>For matrices that have zero determinant, this returns <code>(0, -inf)</code>. If <code>input</code> is batched then the entries in the result tensors corresponding to matrices with the zero determinant have sign 0 and the natural logarithm of the absolute value of the determinant -inf.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input matrix of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n, n)</annotation></semantics></math></span></span> </span> or the batch of matrices of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, n, n)</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∗</mo></mrow><annotation encoding="application/x-tex">*</annotation></semantics></math></span></span> </span> is one or more batch dimensions.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – tuple of two tensors to write the output to.</p> </dd> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>A namedtuple (sign, logabsdet) containing the sign of the determinant and the natural logarithm of the absolute value of determinant, respectively.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(3, 3)
&gt;&gt;&gt; A
tensor([[ 0.0032, -0.2239, -1.1219],
        [-0.6690,  0.1161,  0.4053],
        [-1.6218, -0.9273, -0.0082]])
&gt;&gt;&gt; torch.linalg.det(A)
tensor(-0.7576)
&gt;&gt;&gt; torch.linalg.logdet(A)
tensor(nan)
&gt;&gt;&gt; torch.linalg.slogdet(A)
torch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.eigh">
<code>torch.linalg.eigh(input, UPLO='L', *, out=None) -&gt; (Tensor, Tensor)</code> </dt> <dd>
<p>Computes the eigenvalues and eigenvectors of a complex Hermitian (or real symmetric) matrix <code>input</code>, or of each such matrix in a batched <code>input</code>.</p> <p>For a single matrix <code>input</code>, the tensor of eigenvalues <code>w</code> and the tensor of eigenvectors <code>V</code> decompose the <code>input</code> such that <code>input = V diag(w) Vᴴ</code>, where <code>Vᴴ</code> is the transpose of <code>V</code> for real-valued <code>input</code>, or the conjugate transpose of <code>V</code> for complex-valued <code>input</code>.</p> <p>Since the matrix or matrices in <code>input</code> are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When <code>UPLO</code> is “L”, its default value, only the lower triangular part of each matrix is used in the computation. When <code>UPLO</code> is “U” only the upper triangular part of each matrix is used.</p> <p>Supports input of float, double, cfloat and cdouble dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues/eigenvectors are computed using LAPACK’s <code>syevd</code> and <code>heevd</code> routines for CPU inputs, and MAGMA’s <code>syevd</code> and <code>heevd</code> routines for CUDA inputs.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues of real symmetric or complex Hermitian matrices are always real.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvectors of matrices are not unique, so any eigenvector multiplied by a constant remains a valid eigenvector. This function may compute different eigenvector representations on different device types. Usually the difference is only in the sign of the eigenvector.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="#torch.linalg.eigvalsh" title="torch.linalg.eigvalsh"><code>torch.linalg.eigvalsh()</code></a> for a related function that computes only eigenvalues. However, that function is not differentiable.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the Hermitian <code>n times n</code> matrix or the batch of such matrices of size <code>(*, n, n)</code> where <code>*</code> is one or more batch dimensions.</li> <li>
<strong>UPLO</strong> (<em>'L'</em><em>, </em><em>'U'</em><em>, </em><em>optional</em>) – controls whether to use the upper-triangular or the lower-triangular part of <code>input</code> in the computations. Default is <code>'L'</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – tuple of two tensors to write the output to. Default is <code>None</code>.</p> </dd> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<p>A namedtuple (eigenvalues, eigenvectors) containing</p>  <ul class="simple"> <li>
<dl class="simple"> <dt>
<code>eigenvalues (Tensor): Shape (*, m).</code> </dt>
<dd>
<p>The eigenvalues in ascending order.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>eigenvectors (Tensor): Shape (*, m, m).</code> </dt>
<dd>
<p>The orthonormal eigenvectors of the <code>input</code>.</p> </dd> </dl> </li> </ul>  </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p>(<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>)</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(2, 2, dtype=torch.complex128)
&gt;&gt;&gt; a = a + a.t().conj()  # creates a Hermitian matrix
&gt;&gt;&gt; a
tensor([[2.9228+0.0000j, 0.2029-0.0862j],
        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; w, v = torch.linalg.eigh(a)
&gt;&gt;&gt; w
tensor([0.3277, 2.9415], dtype=torch.float64)
&gt;&gt;&gt; v
tensor([[-0.0846+-0.0000j, -0.9964+0.0000j],
        [ 0.9170+0.3898j, -0.0779-0.0331j]], dtype=torch.complex128)
&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(w.to(v.dtype).diag_embed(), v.t().conj())), a)
True

&gt;&gt;&gt; a = torch.randn(3, 2, 2, dtype=torch.float64)
&gt;&gt;&gt; a = a + a.transpose(-2, -1)  # creates a symmetric matrix
&gt;&gt;&gt; w, v = torch.linalg.eigh(a)
&gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(w.diag_embed(), v.transpose(-2, -1))), a)
True
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.eigvalsh">
<code>torch.linalg.eigvalsh(input, UPLO='L', *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the eigenvalues of a complex Hermitian (or real symmetric) matrix <code>input</code>, or of each such matrix in a batched <code>input</code>. The eigenvalues are returned in ascending order.</p> <p>Since the matrix or matrices in <code>input</code> are assumed to be Hermitian, the imaginary part of their diagonals is always treated as zero. When <code>UPLO</code> is “L”, its default value, only the lower triangular part of each matrix is used in the computation. When <code>UPLO</code> is “U” only the upper triangular part of each matrix is used.</p> <p>Supports input of float, double, cfloat and cdouble dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues are computed using LAPACK’s <code>syevd</code> and <code>heevd</code> routines for CPU inputs, and MAGMA’s <code>syevd</code> and <code>heevd</code> routines for CUDA inputs.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues of real symmetric or complex Hermitian matrices are always real.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function doesn’t support backpropagation, please use <a class="reference internal" href="#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> instead, which also computes the eigenvectors.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> for a related function that computes both eigenvalues and eigenvectors.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the Hermitian <code>n times n</code> matrix or the batch of such matrices of size <code>(*, n, n)</code> where <code>*</code> is one or more batch dimensions.</li> <li>
<strong>UPLO</strong> (<em>'L'</em><em>, </em><em>'U'</em><em>, </em><em>optional</em>) – controls whether to use the upper-triangular or the lower-triangular part of <code>input</code> in the computations. Default is <code>'L'</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – tensor to write the output to. Default is <code>None</code>.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(2, 2, dtype=torch.complex128)
&gt;&gt;&gt; a = a + a.t().conj()  # creates a Hermitian matrix
&gt;&gt;&gt; a
tensor([[2.9228+0.0000j, 0.2029-0.0862j],
        [0.2029+0.0862j, 0.3464+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; w = torch.linalg.eigvalsh(a)
&gt;&gt;&gt; w
tensor([0.3277, 2.9415], dtype=torch.float64)

&gt;&gt;&gt; a = torch.randn(3, 2, 2, dtype=torch.float64)
&gt;&gt;&gt; a = a + a.transpose(-2, -1)  # creates a symmetric matrix
&gt;&gt;&gt; a
tensor([[[ 2.8050, -0.3850],
        [-0.3850,  3.2376]],

        [[-1.0307, -2.7457],
        [-2.7457, -1.7517]],

        [[ 1.7166,  2.2207],
        [ 2.2207, -2.0898]]], dtype=torch.float64)
&gt;&gt;&gt; w = torch.linalg.eigvalsh(a)
&gt;&gt;&gt; w
tensor([[ 2.5797,  3.4629],
        [-4.1605,  1.3780],
        [-3.1113,  2.7381]], dtype=torch.float64)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.matrix_rank">
<code>torch.linalg.matrix_rank(input, tol=None, hermitian=False, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the numerical rank of a matrix <code>input</code>, or of each matrix in a batched <code>input</code>.</p> <p>The matrix rank is computed as the number of singular values (or absolute eigenvalues when <code>hermitian</code> is <code>True</code>) that are greater than the specified <code>tol</code> threshold.</p> <p>If <code>tol</code> is not specified, <code>tol</code> is set to <code>S.max(dim=-1)*max(input.shape[-2:])*eps</code>, where <code>S</code> is the singular values (or absolute eigenvalues when <code>hermitian</code> is <code>True</code>), and <code>eps</code> is the epsilon value for the datatype of <code>input</code>. The epsilon value can be obtained using the <code>eps</code> attribute of <code>torch.finfo</code>.</p> <p>Supports input of float, double, cfloat and cdouble dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The matrix rank is computed using singular value decomposition (see <a class="reference internal" href="#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>) by default. If <code>hermitian</code> is <code>True</code>, then <code>input</code> is assumed to be Hermitian (symmetric if real-valued), and the computation is done by obtaining the eigenvalues (see <a class="reference internal" href="#torch.linalg.eigvalsh" title="torch.linalg.eigvalsh"><code>torch.linalg.eigvalsh()</code></a>).</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input matrix of size <code>(m, n)</code> or the batch of matrices of size <code>(*, m, n)</code> where <code>*</code> is one or more batch dimensions.</li> <li>
<strong>tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><em>optional</em>) – the tolerance value. Default is <code>None</code>
</li> <li>
<strong>hermitian</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – indicates whether <code>input</code> is Hermitian. Default is <code>False</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – tensor to write the output to. Default is <code>None</code>.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.eye(10)
&gt;&gt;&gt; torch.linalg.matrix_rank(a)
tensor(10)
&gt;&gt;&gt; b = torch.eye(10)
&gt;&gt;&gt; b[0, 0] = 0
&gt;&gt;&gt; torch.linalg.matrix_rank(b)
tensor(9)

&gt;&gt;&gt; a = torch.randn(4, 3, 2)
&gt;&gt;&gt; torch.linalg.matrix_rank(a)
tensor([2, 2, 2, 2])

&gt;&gt;&gt; a = torch.randn(2, 4, 2, 3)
&gt;&gt;&gt; torch.linalg.matrix_rank(a)
tensor([[2, 2, 2, 2],
        [2, 2, 2, 2]])

&gt;&gt;&gt; a = torch.randn(2, 4, 3, 3, dtype=torch.complex64)
&gt;&gt;&gt; torch.linalg.matrix_rank(a)
tensor([[3, 3, 3, 3],
        [3, 3, 3, 3]])
&gt;&gt;&gt; torch.linalg.matrix_rank(a, hermitian=True)
tensor([[3, 3, 3, 3],
        [3, 3, 3, 3]])
&gt;&gt;&gt; torch.linalg.matrix_rank(a, tol=1.0)
tensor([[3, 2, 2, 2],
        [1, 2, 1, 2]])
&gt;&gt;&gt; torch.linalg.matrix_rank(a, tol=1.0, hermitian=True)
tensor([[2, 2, 2, 1],
        [1, 2, 2, 2]])
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.norm">
<code>torch.linalg.norm(input, ord=None, dim=None, keepdim=False, *, out=None, dtype=None) → Tensor</code> </dt> <dd>
<p>Returns the matrix norm or vector norm of a given tensor.</p> <p>This function can calculate one of eight different types of matrix norms, or one of an infinite number of vector norms, depending on both the number of reduction dimensions and the value of the <code>ord</code> parameter.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – The input tensor. If dim is None, x must be 1-D or 2-D, unless <code>ord</code> is None. If both <code>dim</code> and <code>ord</code> are None, the 2-norm of the input flattened to 1-D will be returned. Its data type must be either a floating point or complex type. For complex inputs, the norm is calculated on of the absolute values of each element. If the input is complex and neither <code>dtype</code> nor <code>out</code> is specified, the result’s data type will be the corresponding floating point type (e.g. float if <code>input</code> is complexfloat).</li> <li>
<p><strong>ord</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><em>inf</em><em>, </em><em>-inf</em><em>, </em><em>'fro'</em><em>, </em><em>'nuc'</em><em>, </em><em>optional</em>) – </p>
<p>The order of norm. inf refers to <code>float('inf')</code>, numpy’s <code>inf</code> object, or any equivalent object. The following norms can be calculated:</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>ord</p></th> <th class="head"><p>norm for matrices</p></th> <th class="head"><p>norm for vectors</p></th> </tr> </thead>  <tr>
<td><p>None</p></td> <td><p>Frobenius norm</p></td> <td><p>2-norm</p></td> </tr> <tr>
<td><p>’fro’</p></td> <td><p>Frobenius norm</p></td> <td><p>– not supported –</p></td> </tr> <tr>
<td><p>‘nuc’</p></td> <td><p>nuclear norm</p></td> <td><p>– not supported –</p></td> </tr> <tr>
<td><p>inf</p></td> <td><p>max(sum(abs(x), dim=1))</p></td> <td><p>max(abs(x))</p></td> </tr> <tr>
<td><p>-inf</p></td> <td><p>min(sum(abs(x), dim=1))</p></td> <td><p>min(abs(x))</p></td> </tr> <tr>
<td><p>0</p></td> <td><p>– not supported –</p></td> <td><p>sum(x != 0)</p></td> </tr> <tr>
<td><p>1</p></td> <td><p>max(sum(abs(x), dim=0))</p></td> <td><p>as below</p></td> </tr> <tr>
<td><p>-1</p></td> <td><p>min(sum(abs(x), dim=0))</p></td> <td><p>as below</p></td> </tr> <tr>
<td><p>2</p></td> <td><p>2-norm (largest sing. value)</p></td> <td><p>as below</p></td> </tr> <tr>
<td><p>-2</p></td> <td><p>smallest singular value</p></td> <td><p>as below</p></td> </tr> <tr>
<td><p>other</p></td> <td><p>– not supported –</p></td> <td><p>sum(abs(x)**ord)**(1./ord)</p></td> </tr>  </table> <p>Default: <code>None</code></p> </li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>2-tuple of python:ints</em><em>, </em><em>2-list of python:ints</em><em>, </em><em>optional</em>) – If <code>dim</code> is an int, vector norm will be calculated over the specified dimension. If <code>dim</code> is a 2-tuple of ints, matrix norm will be calculated over the specified dimensions. If <code>dim</code> is None, matrix norm will be calculated when the input tensor has two dimensions, and vector norm will be calculated when the input tensor has one dimension. Default: <code>None</code>
</li> <li>
<strong>keepdim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – If set to True, the reduced dimensions are retained in the result as dimensions with size one. Default: <code>False</code>
</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default: <code>None</code>
</li> <li>
<strong>dtype</strong> (<code>torch.dtype</code>, optional) – If specified, the input tensor is cast to <code>dtype</code> before performing the operation, and the returned tensor’s type will be <code>dtype</code>. If this argument is used in conjunction with the <code>out</code> argument, the output tensor’s type must match this argument or a RuntimeError will be raised. Default: <code>None</code>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from torch import linalg as LA
&gt;&gt;&gt; a = torch.arange(9, dtype=torch.float) - 4
&gt;&gt;&gt; a
tensor([-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])
&gt;&gt;&gt; b = a.reshape((3, 3))
&gt;&gt;&gt; b
tensor([[-4., -3., -2.],
        [-1.,  0.,  1.],
        [ 2.,  3.,  4.]])

&gt;&gt;&gt; LA.norm(a)
tensor(7.7460)
&gt;&gt;&gt; LA.norm(b)
tensor(7.7460)
&gt;&gt;&gt; LA.norm(b, 'fro')
tensor(7.7460)
&gt;&gt;&gt; LA.norm(a, float('inf'))
tensor(4.)
&gt;&gt;&gt; LA.norm(b, float('inf'))
tensor(9.)
&gt;&gt;&gt; LA.norm(a, -float('inf'))
tensor(0.)
&gt;&gt;&gt; LA.norm(b, -float('inf'))
tensor(2.)

&gt;&gt;&gt; LA.norm(a, 1)
tensor(20.)
&gt;&gt;&gt; LA.norm(b, 1)
tensor(7.)
&gt;&gt;&gt; LA.norm(a, -1)
tensor(0.)
&gt;&gt;&gt; LA.norm(b, -1)
tensor(6.)
&gt;&gt;&gt; LA.norm(a, 2)
tensor(7.7460)
&gt;&gt;&gt; LA.norm(b, 2)
tensor(7.3485)

&gt;&gt;&gt; LA.norm(a, -2)
tensor(0.)
&gt;&gt;&gt; LA.norm(b.double(), -2)
tensor(1.8570e-16, dtype=torch.float64)
&gt;&gt;&gt; LA.norm(a, 3)
tensor(5.8480)
&gt;&gt;&gt; LA.norm(a, -3)
tensor(0.)
</pre> <p>Using the <code>dim</code> argument to compute vector norms:</p> <pre data-language="python">&gt;&gt;&gt; c = torch.tensor([[1., 2., 3.],
...                   [-1, 1, 4]])
&gt;&gt;&gt; LA.norm(c, dim=0)
tensor([1.4142, 2.2361, 5.0000])
&gt;&gt;&gt; LA.norm(c, dim=1)
tensor([3.7417, 4.2426])
&gt;&gt;&gt; LA.norm(c, ord=1, dim=1)
tensor([6., 6.])
</pre> <p>Using the <code>dim</code> argument to compute matrix norms:</p> <pre data-language="python">&gt;&gt;&gt; m = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)
&gt;&gt;&gt; LA.norm(m, dim=(1,2))
tensor([ 3.7417, 11.2250])
&gt;&gt;&gt; LA.norm(m[0, :, :]), LA.norm(m[1, :, :])
(tensor(3.7417), tensor(11.2250))
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.pinv">
<code>torch.linalg.pinv(input, rcond=1e-15, hermitian=False, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the pseudo-inverse (also known as the Moore-Penrose inverse) of a matrix <code>input</code>, or of each matrix in a batched <code>input</code>.</p> <p>The singular values (or the absolute values of the eigenvalues when <code>hermitian</code> is <code>True</code>) that are below the specified <code>rcond</code> threshold are treated as zero and discarded in the computation.</p> <p>Supports input of float, double, cfloat and cdouble datatypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The pseudo-inverse is computed using singular value decomposition (see <a class="reference internal" href="#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>) by default. If <code>hermitian</code> is <code>True</code>, then <code>input</code> is assumed to be Hermitian (symmetric if real-valued), and the computation of the pseudo-inverse is done by obtaining the eigenvalues and eigenvectors (see <a class="reference internal" href="#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a>).</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If singular value decomposition or eigenvalue decomposition algorithms do not converge then a RuntimeError will be thrown.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input matrix of size <code>(m, n)</code> or the batch of matrices of size <code>(*, m, n)</code> where <code>*</code> is one or more batch dimensions.</li> <li>
<strong>rcond</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – the tolerance value to determine the cutoff for small singular values. Must be broadcastable to the singular values of <code>input</code> as returned by <a class="reference internal" href="generated/torch.svd#torch.svd" title="torch.svd"><code>torch.svd()</code></a>. Default is <code>1e-15</code>.</li> <li>
<strong>hermitian</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – indicates whether <code>input</code> is Hermitian. Default is <code>False</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default is <code>None</code>.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; input = torch.randn(3, 5)
&gt;&gt;&gt; input
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
&gt;&gt;&gt; torch.linalg.pinv(input)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])

Batched linalg.pinv example
&gt;&gt;&gt; a = torch.randn(2, 6, 3)
&gt;&gt;&gt; b = torch.linalg.pinv(a)
&gt;&gt;&gt; torch.matmul(b, a)
tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],
        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],
        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],

        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],
        [-2.2352e-07,  1.0000e+00,  1.1921e-07],
        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])

Hermitian input example
&gt;&gt;&gt; a = torch.randn(3, 3, dtype=torch.complex64)
&gt;&gt;&gt; a = a + a.t().conj()  # creates a Hermitian matrix
&gt;&gt;&gt; b = torch.linalg.pinv(a, hermitian=True)
&gt;&gt;&gt; torch.matmul(b, a)
tensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,
        5.9605e-08-2.3842e-07j],
        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,
        -4.7684e-07+1.1921e-07j],
        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,
        1.0000e+00-1.7897e-07j]])

Non-default rcond example
&gt;&gt;&gt; rcond = 0.5
&gt;&gt;&gt; a = torch.randn(3, 3)
&gt;&gt;&gt; torch.linalg.pinv(a)
tensor([[ 0.2971, -0.4280, -2.0111],
        [-0.0090,  0.6426, -0.1116],
        [-0.7832, -0.2465,  1.0994]])
&gt;&gt;&gt; torch.linalg.pinv(a, rcond)
tensor([[-0.2672, -0.2351, -0.0539],
        [-0.0211,  0.6467, -0.0698],
        [-0.4400, -0.3638, -0.0910]])

Matrix-wise rcond example
&gt;&gt;&gt; a = torch.randn(5, 6, 2, 3, 3)
&gt;&gt;&gt; rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]
&gt;&gt;&gt; torch.linalg.pinv(a, rcond)
&gt;&gt;&gt; rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'
&gt;&gt;&gt; torch.linalg.pinv(a, rcond)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.svd">
<code>torch.linalg.svd(input, full_matrices=True, compute_uv=True, *, out=None) -&gt; (Tensor, Tensor, Tensor)</code> </dt> <dd>
<p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>.” The singular value decomposition is represented as a namedtuple <code>(U, S, Vh)</code>, such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo>=</mo><mi>U</mi><mo mathvariant="normal" lspace="0.22em" rspace="0.22em">@</mo><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><mo>×</mo><mi>V</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">input = U \mathbin{@} diag(S) \times Vh</annotation></semantics></math></span></span> </span>. If <code>input</code> is a batch of tensors, then <code>U</code>, <code>S</code>, and <code>Vh</code> are also batched with the same batch dimensions as <code>input</code>.</p> <p>If <code>full_matrices</code> is <code>False</code> (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of <code>input</code> are <code>m</code> and <code>n</code>, then the returned <code>U</code> and <code>V</code> matrices will contain only <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy="false">(</mo><mi>n</mi><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">min(n, m)</annotation></semantics></math></span></span> </span> orthonormal columns.</p> <p>If <code>compute_uv</code> is <code>False</code>, the returned <code>U</code> and <code>Vh</code> will be empy tensors with no elements and the same device as <code>input</code>. The <code>full_matrices</code> argument has no effect when <code>compute_uv</code> is False.</p> <p>The dtypes of <code>U</code> and <code>V</code> are the same as <code>input</code>’s. <code>S</code> will always be real-valued, even if <code>input</code> is complex.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Unlike NumPy’s <code>linalg.svd</code>, this always returns a namedtuple of three tensors, even when <code>compute_uv=False</code>. This behavior may change in a future PyTorch release.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The singular values are returned in descending order. If <code>input</code> is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The implementation of SVD on CPU uses the LAPACK routine <code>?gesdd</code> (a divide-and-conquer algorithm) instead of <code>?gesvd</code> for speed. Analogously, the SVD on GPU uses the cuSOLVER routines <code>gesvdj</code> and <code>gesvdjBatched</code> on CUDA 10.1.243 and later, and uses the MAGMA routine <code>gesdd</code> on earlier versions of CUDA.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The returned matrix <code>U</code> will be transposed, i.e. with strides <code>U.contiguous().transpose(-2, -1).stride()</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Gradients computed using <code>U</code> and <code>Vh</code> may be unstable if <code>input</code> is not full rank or has non-unique singular values.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>full_matrices</code> = <code>True</code>, the gradients on <code>U[..., :, min(m, n):]</code> and <code>V[..., :, min(m, n):]</code> will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>S</code> tensor can only be used to compute gradients if <code>compute_uv</code> is True.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Since <code>U</code> and <code>V</code> of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>ϕ</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{i \phi}</annotation></semantics></math></span></span> </span> while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different <code>U</code> and <code>V</code> tensors.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span> where <code>*</code> is zero or more batch dimensions consisting of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math></span></span> </span> matrices.</li> <li>
<strong>full_matrices</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – controls whether to compute the full or reduced decomposition, and consequently the shape of returned <code>U</code> and <code>V</code>. Defaults to True.</li> <li>
<strong>compute_uv</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – whether to compute <code>U</code> and <code>V</code> or not. Defaults to True.</li> <li>
<strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – a tuple of three tensors to use for the outputs. If compute_uv=False, the 1st and 3rd arguments must be tensors, but they are ignored. E.g. you can pass <code>(torch.Tensor(), out_S, torch.Tensor())</code>
</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[-0.3357, -0.2987, -1.1096],
        [ 1.4894,  1.0016, -0.4572],
        [-1.9401,  0.7437,  2.0968],
        [ 0.1515,  1.3812,  1.5491],
        [-1.8489, -0.5907, -2.5673]])
&gt;&gt;&gt;
&gt;&gt;&gt; # reconstruction in the full_matrices=False case
&gt;&gt;&gt; u, s, vh = torch.linalg.svd(a, full_matrices=False)
&gt;&gt;&gt; u.shape, s.shape, vh.shape
(torch.Size([5, 3]), torch.Size([3]), torch.Size([3, 3]))
&gt;&gt;&gt; torch.dist(a, u @ torch.diag(s) @ vh)
tensor(1.0486e-06)
&gt;&gt;&gt;
&gt;&gt;&gt; # reconstruction in the full_matrices=True case
&gt;&gt;&gt; u, s, vh = torch.linalg.svd(a)
&gt;&gt;&gt; u.shape, s.shape, vh.shape
(torch.Size([5, 5]), torch.Size([3]), torch.Size([3, 3]))
&gt;&gt;&gt; torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)
&gt;&gt;&gt; torch.dist(a, u[:, :3] @ torch.diag(s) @ vh)
tensor(1.0486e-06)
&gt;&gt;&gt;
&gt;&gt;&gt; # extra dimensions
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, vh = torch.linalg.svd(a_big, full_matrices=False)
&gt;&gt;&gt; torch.dist(a_big, u @ torch.diag_embed(s) @ vh)
tensor(3.0957e-06)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.solve">
<code>torch.linalg.solve(input, other, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the solution <code>x</code> to the matrix equation <code>matmul(input, x) = other</code> with a square matrix, or batches of such matrices, <code>input</code> and one or more right-hand side vectors <code>other</code>. If <code>input</code> is batched and <code>other</code> is not, then <code>other</code> is broadcast to have the same batch dimensions as <code>input</code>. The resulting tensor has the same shape as the (possibly broadcast) <code>other</code>.</p> <p>Supports input of <code>float</code>, <code>double</code>, <code>cfloat</code> and <code>cdouble</code> dtypes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <code>input</code> is a non-square or non-invertible matrix, or a batch containing non-square matrices or one or more non-invertible matrices, then a RuntimeError will be thrown.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the square <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n \times n</annotation></semantics></math></span></span> </span> matrix or the batch of such matrices of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, n, n)</annotation></semantics></math></span></span> </span> where <code>*</code> is one or more batch dimensions.</li> <li>
<strong>other</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – right-hand side tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, n)</annotation></semantics></math></span></span> </span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>n</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, n, k)</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> </span> is the number of right-hand side vectors.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default: <code>None</code></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.eye(3)
&gt;&gt;&gt; b = torch.randn(3)
&gt;&gt;&gt; x = torch.linalg.solve(A, b)
&gt;&gt;&gt; torch.allclose(A @ x, b)
True
</pre> <p>Batched input:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; b = torch.randn(3, 1)
&gt;&gt;&gt; x = torch.linalg.solve(A, b)
&gt;&gt;&gt; torch.allclose(A @ x, b)
True
&gt;&gt;&gt; b = torch.rand(3) # b is broadcast internally to (*A.shape[:-2], 3)
&gt;&gt;&gt; x = torch.linalg.solve(A, b)
&gt;&gt;&gt; x.shape
torch.Size([2, 3])
&gt;&gt;&gt; Ax = A @ x.unsqueeze(-1)
&gt;&gt;&gt; torch.allclose(Ax, b.unsqueeze(-1).expand_as(Ax))
True
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.tensorinv">
<code>torch.linalg.tensorinv(input, ind=2, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes a tensor <code>input_inv</code> such that <code>tensordot(input_inv, input, ind) == I_n</code> (inverse tensor equation), where <code>I_n</code> is the n-dimensional identity tensor and <code>n</code> is equal to <code>input.ndim</code>. The resulting tensor <code>input_inv</code> has shape equal to <code>input.shape[ind:] + input.shape[:ind]</code>.</p> <p>Supports input of <code>float</code>, <code>double</code>, <code>cfloat</code> and <code>cdouble</code> data types.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <code>input</code> is not invertible or does not satisfy the requirement <code>prod(input.shape[ind:]) == prod(input.shape[:ind])</code>, then a RuntimeError will be thrown.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>input</code> is a 2-dimensional tensor and <code>ind=1</code>, this function computes the (multiplicative) inverse of <code>input</code>, equivalent to calling <a class="reference internal" href="generated/torch.inverse#torch.inverse" title="torch.inverse"><code>torch.inverse()</code></a>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – A tensor to invert. Its shape must satisfy <code>prod(input.shape[:ind]) == prod(input.shape[ind:])</code>.</li> <li>
<strong>ind</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – A positive integer that describes the inverse tensor equation. See <a class="reference internal" href="generated/torch.tensordot#torch.tensordot" title="torch.tensordot"><code>torch.tensordot()</code></a> for details. Default: 2.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default: <code>None</code></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.eye(4 * 6).reshape((4, 6, 8, 3))
&gt;&gt;&gt; ainv = torch.linalg.tensorinv(a, ind=2)
&gt;&gt;&gt; ainv.shape
torch.Size([8, 3, 4, 6])
&gt;&gt;&gt; b = torch.randn(4, 6)
&gt;&gt;&gt; torch.allclose(torch.tensordot(ainv, b), torch.linalg.tensorsolve(a, b))
True

&gt;&gt;&gt; a = torch.randn(4, 4)
&gt;&gt;&gt; a_tensorinv = torch.linalg.tensorinv(a, ind=1)
&gt;&gt;&gt; a_inv = torch.inverse(a)
&gt;&gt;&gt; torch.allclose(a_tensorinv, a_inv)
True
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.tensorsolve">
<code>torch.linalg.tensorsolve(input, other, dims=None, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes a tensor <code>x</code> such that <code>tensordot(input, x, dims=x.ndim) = other</code>. The resulting tensor <code>x</code> has the same shape as <code>input[other.ndim:]</code>.</p> <p>Supports real-valued and complex-valued inputs.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <code>input</code> does not satisfy the requirement <code>prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim])</code> after (optionally) moving the dimensions using <code>dims</code>, then a RuntimeError will be thrown.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – “left-hand-side” tensor, it must satisfy the requirement <code>prod(input.shape[other.ndim:]) == prod(input.shape[:other.ndim])</code>.</li> <li>
<strong>other</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – “right-hand-side” tensor of shape <code>input.shape[other.ndim]</code>.</li> <li>
<strong>dims</strong> (<em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – dimensions of <code>input</code> to be moved before the computation. Equivalent to calling <code>input = movedim(input, dims, range(len(dims) - input.ndim, 0))</code>. If None (default), no dimensions are moved.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default: <code>None</code></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.eye(2 * 3 * 4).reshape((2 * 3, 4, 2, 3, 4))
&gt;&gt;&gt; b = torch.randn(2 * 3, 4)
&gt;&gt;&gt; x = torch.linalg.tensorsolve(a, b)
&gt;&gt;&gt; x.shape
torch.Size([2, 3, 4])
&gt;&gt;&gt; torch.allclose(torch.tensordot(a, x, dims=x.ndim), b)
True

&gt;&gt;&gt; a = torch.randn(6, 4, 4, 3, 2)
&gt;&gt;&gt; b = torch.randn(4, 3, 2)
&gt;&gt;&gt; x = torch.linalg.tensorsolve(a, b, dims=(0, 2))
&gt;&gt;&gt; x.shape
torch.Size([6, 4])
&gt;&gt;&gt; a = a.permute(1, 3, 4, 0, 2)
&gt;&gt;&gt; a.shape[b.ndim:]
torch.Size([6, 4])
&gt;&gt;&gt; torch.allclose(torch.tensordot(a, x, dims=x.ndim), b, atol=1e-6)
True
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.inv">
<code>torch.linalg.inv(input, *, out=None) → Tensor</code> </dt> <dd>
<p>Computes the multiplicative inverse matrix of a square matrix <code>input</code>, or of each square matrix in a batched <code>input</code>. The result satisfies the relation:</p> <p><code>matmul(inv(input),input)</code> = <code>matmul(input,inv(input))</code> = <code>eye(input.shape[0]).expand_as(input)</code>.</p> <p>Supports input of float, double, cfloat and cdouble data types.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When given inputs on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The inverse matrix is computed using LAPACK’s <code>getrf</code> and <code>getri</code> routines for CPU inputs. For CUDA inputs, cuSOLVER’s <code>getrf</code> and <code>getrs</code> routines as well as cuBLAS’ <code>getrf</code> and <code>getri</code> routines are used if CUDA version &gt;= 10.1.243, otherwise MAGMA’s <code>getrf</code> and <code>getri</code> routines are used instead.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <code>input</code> is a non-invertible matrix or non-square matrix, or batch with at least one such matrix, then a RuntimeError will be thrown.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the square <code>(n, n)</code> matrix or the batch of such matrices of size <code>(*, n, n)</code> where <code>*</code> is one or more batch dimensions.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The output tensor. Ignored if <code>None</code>. Default is <code>None</code>.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.rand(4, 4)
&gt;&gt;&gt; y = torch.linalg.inv(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z
tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],
        [ 0.0000,  1.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  1.0000,  0.0000],
        [ 0.0000, -0.0000, -0.0000,  1.0000]])
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero
tensor(1.1921e-07)

&gt;&gt;&gt; # Batched inverse example
&gt;&gt;&gt; x = torch.randn(2, 3, 4, 4)
&gt;&gt;&gt; y = torch.linalg.inv(x)
&gt;&gt;&gt; z = torch.matmul(x, y)
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero
tensor(1.9073e-06)

&gt;&gt;&gt; x = torch.rand(4, 4, dtype=torch.cdouble)
&gt;&gt;&gt; y = torch.linalg.inv(x)
&gt;&gt;&gt; z = torch.mm(x, y)
&gt;&gt;&gt; z
tensor([[ 1.0000e+00+0.0000e+00j, -1.3878e-16+3.4694e-16j,
        5.5511e-17-1.1102e-16j,  0.0000e+00-1.6653e-16j],
        [ 5.5511e-16-1.6653e-16j,  1.0000e+00+6.9389e-17j,
        2.2204e-16-1.1102e-16j, -2.2204e-16+1.1102e-16j],
        [ 3.8858e-16-1.2490e-16j,  2.7756e-17+3.4694e-17j,
        1.0000e+00+0.0000e+00j, -4.4409e-16+5.5511e-17j],
        [ 4.4409e-16+5.5511e-16j, -3.8858e-16+1.8041e-16j,
        2.2204e-16+0.0000e+00j,  1.0000e+00-3.4694e-16j]],
    dtype=torch.complex128)
&gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4, dtype=torch.cdouble))) # Max non-zero
tensor(7.5107e-16, dtype=torch.float64)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.linalg.qr">
<code>torch.linalg.qr(input, mode='reduced', *, out=None) -&gt; (Tensor, Tensor)</code> </dt> <dd>
<p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>, and returns a namedtuple (Q, R) of tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span></span> </span> with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span> </span> being an orthogonal matrix or batch of orthogonal matrices and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span></span> </span> being an upper triangular matrix or batch of upper triangular matrices.</p> <p>Depending on the value of <code>mode</code> this function returns the reduced or complete QR factorization. See below for a list of valid modes.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><strong>Differences with</strong> <code>numpy.linalg.qr</code>:</p> <ul class="simple"> <li>
<code>mode='raw'</code> is not implemented</li> <li>unlike <code>numpy.linalg.qr</code>, this function always returns a tuple of two tensors. When <code>mode='r'</code>, the <code>Q</code> tensor is an empty tensor. This behavior may change in a future PyTorch release.</li> </ul> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Backpropagation is not supported for <code>mode='r'</code>. Use <code>mode='reduced'</code> instead.</p> <p>Backpropagation is also not supported if the first <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">(</mo><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\min(input.size(-1), input.size(-2))</annotation></semantics></math></span></span> </span> columns of any matrix in <code>input</code> are not linearly independent. While no error will be thrown when this occurs the values of the “gradient” produced may be anything. This behavior may change in the future.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span> where <code>*</code> is zero or more batch dimensions consisting of matrices of dimension <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math></span></span> </span>.</li> <li>
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>, </em><em>optional</em>) – </p>
<p>if <code>k = min(m, n)</code> then:</p> <ul> <li>
<code>'reduced'</code> : returns <code>(Q, R)</code> with dimensions (m, k), (k, n) (default)</li> <li>
<code>'complete'</code>: returns <code>(Q, R)</code> with dimensions (m, m), (m, n)</li> <li>
<code>'r'</code>: computes only <code>R</code>; returns <code>(Q, R)</code> where <code>Q</code> is empty and <code>R</code> has dimensions (k, n)</li> </ul> </li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – tuple of <code>Q</code> and <code>R</code> tensors. The dimensions of <code>Q</code> and <code>R</code> are detailed in the description of <code>mode</code> above.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.linalg.qr(a)
&gt;&gt;&gt; q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
&gt;&gt;&gt; r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
&gt;&gt;&gt; torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
&gt;&gt;&gt; torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
&gt;&gt;&gt; q2, r2 = torch.linalg.qr(a, mode='r')
&gt;&gt;&gt; q2
tensor([])
&gt;&gt;&gt; torch.equal(r, r2)
True
&gt;&gt;&gt; a = torch.randn(3, 4, 5)
&gt;&gt;&gt; q, r = torch.linalg.qr(a, mode='complete')
&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
True
&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/linalg.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/linalg.html</a>
  </p>
</div>
