<h1 id="torch-nn-qat">torch.nn.qat</h1> <p>This module implements versions of the key nn modules <strong>Conv2d()</strong> and <strong>Linear()</strong> which run in FP32 but with rounding applied to simulate the effect of INT8 quantization.</p>  <h2 id="conv2d">Conv2d</h2> <dl class="class"> <dt id="torch.nn.qat.Conv2d">
<code>class torch.nn.qat.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', qconfig=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/qat/modules/conv.html#Conv2d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A Conv2d module attached with FakeQuantize modules for weight, used for quantization aware training.</p> <p>We adopt the same interface as <code>torch.nn.Conv2d</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d">https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d</a> for documentation.</p> <p>Similar to <code>torch.nn.Conv2d</code>, with FakeQuantize modules initialized to default.</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<p><strong>~Conv2d.weight_fake_quant</strong> – fake quant module for weight</p> </dd> </dl> <dl class="method"> <dt id="torch.nn.qat.Conv2d.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/qat/modules/conv.html#Conv2d.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a qat module from a float module or qparams_dict</p> <p>Args: <code>mod</code> a float module, either produced by torch.quantization utilities or directly from user</p> </dd>
</dl> </dd>
</dl>   <h2 id="linear">Linear</h2> <dl class="class"> <dt id="torch.nn.qat.Linear">
<code>class torch.nn.qat.Linear(in_features, out_features, bias=True, qconfig=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/qat/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A linear module attached with FakeQuantize modules for weight, used for quantization aware training.</p> <p>We adopt the same interface as <code>torch.nn.Linear</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Linear">https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</a> for documentation.</p> <p>Similar to <code>torch.nn.Linear</code>, with FakeQuantize modules initialized to default.</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<p><strong>~Linear.weight</strong> – fake quant module for weight</p> </dd> </dl> <dl class="method"> <dt id="torch.nn.qat.Linear.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/qat/modules/linear.html#Linear.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a qat module from a float module or qparams_dict</p> <p>Args: <code>mod</code> a float module, either produced by torch.quantization utilities or directly from user</p> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/torch.nn.qat.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/torch.nn.qat.html</a>
  </p>
</div>
