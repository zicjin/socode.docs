<h1 id="ddp-communication-hooks">DDP Communication Hooks</h1> <p>DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.">DistributedDataParallel</a>. A few built-in communication hooks are provided, and users can easily apply any of these hooks to optimize communication. Besides, the hook interface can also support user-defined communication strategies for more advanced use cases.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>DDP communication hook is experimental and subject to change.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>DDP communication hooks can only support single process single device mode on NCCL backend.</p> </div>  <h2 id="how-to-use-a-communication-hook">How to Use a Communication Hook?</h2> <p>To use a communication hook, the user just needs to let the DDP model register the hook before the training loop as below.</p> <dl class="simple"> <dt>
<code>torch.nn.parallel.DistributedDataParallel.register_comm_hook().</code> </dt>
<dd>
<dl class="field-list simple"> <dt class="field-odd">noindex</dt>  </dl> </dd> </dl>   <h2 id="default-communication-hooks">Default Communication Hooks</h2> <p>Default communication hooks are simple <strong>stateless</strong> hooks, so the input state in <code>register_comm_hook</code> is either a process group or <code>None</code>.</p> <dl class="function" id="module-torch.distributed.algorithms.ddp_comm_hooks.default_hooks"> <dt id="torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook">
<code>torch.distributed.algorithms.ddp_comm_hooks.default_hooks.allreduce_hook(process_group, bucket)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.html#allreduce_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This DDP communication hook just calls <code>allreduce</code> using <code>GradBucket</code> tensors. Once gradient tensors are aggregated across all workers, its <code>then</code> callback takes the mean and returns the result. If user registers this hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won’t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.</p> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; ddp_model.register_comm_hook(process_group, allreduce_hook)
</pre> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook">
<code>torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook(process_group, bucket)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributed/algorithms/ddp_comm_hooks/default_hooks.html#fp16_compress_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This DDP communication hook implements a simple gradient compression approach that converts <code>GradBucket</code> tensors whose type is assumed to be <code>torch.float32</code> to half-precision floating point format (<code>torch.float16</code>). It allreduces those <code>float16</code> gradient tensors. Once compressed gradient tensors are allreduced, its then callback called <code>decompress</code> converts the aggregated result back to <code>float32</code> and takes the mean.</p> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; ddp_model.register_comm_hook(process_group, fp16_compress_hook)
</pre> </dd> </dl> </dd>
</dl>   <h2 id="powersgd-communication-hook">PowerSGD Communication Hook</h2> <p>PowerSGD (<a class="reference external" href="https://arxiv.org/abs/1905.13727">Vogels et al., NeurIPS 2019</a>) is a gradient compression algorithm, which can provide very high compression rates and accelerate bandwidth-bound distributed training. This algorithm needs to maintain both some hyperparameters and the internal state. Therefore, PowerSGD communication hook is a <strong>stateful</strong> hook, and the user needs to provide a state object defined as below.</p>  <h3 id="powersgd-state">PowerSGD State</h3> <dl class="class"> <dt id="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState">
<code>class torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState(process_group, matrix_approximation_rank=1, start_powerSGD_iter=10, use_error_feedback=True, warm_start=True, random_seed=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.html#PowerSGDState"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Stores both the algorithm’s hyperparameters and the internal state for all the gradients during the training. Particularly, <code>matrix_approximation_rank</code> and <code>start_powerSGD_iter</code> are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters <code>use_error_feedback</code> and <code>warm_start</code> on.</p> <ol class="arabic"> <li>
<p><code>matrix_approximation_rank</code> controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.</p>  <p>1.1. If <code>matrix_approximation_rank</code> is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.</p> <p>1.2. The increase of <code>matrix_approximation_rank</code> can substantially increase the computation costs of the compression, and the accuracy may not be futher improved beyond a certain <code>matrix_approximation_rank</code> threshold.</p>  </li> </ol> <p>To tune <code>matrix_approximation_rank</code>, we suggest to start from 1 and increase by factors of 2 (like an expoential grid search, 1, 2, 4, …), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.</p> <ol class="arabic simple" start="2"> <li>
<code>start_powerSGD_iter</code> defers PowerSGD compression util step <code>start_powerSGD_iter</code>, and vanilla allreduce runs prior to step <code>start_powerSGD_iter</code>. This hybrid scheme of <strong>vanilla allreduce + PowerSGD</strong> can effectively improve the accuracy, even a relatively small <code>matrix_approximation_rank</code> is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.</li> </ol> <p>To tune <code>start_powerSGD_iter</code>, we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If error feedback or warm-up is enabled, the minimum value of <code>start_powerSGD_iter</code> allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.</p> </div> </dd>
</dl>   <h3 id="powersgd-hooks">PowerSGD Hooks</h3> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>PowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The current implementation may cause gradient overflow for FP16 input.</p> </div> <dl class="function"> <dt id="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook">
<code>torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook(state, bucket)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.html#powerSGD_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This DDP communication hook implements PowerSGD gradient compression algorithm described in the <a class="reference external" href="https://arxiv.org/abs/1905.13727">paper</a>. Once gradient tensors are aggregated across all workers, this hook applies compression as follows:</p> <ol class="arabic"> <li>Views the input flattened 1D gradient tensor as two groups of per-parameter tensors: high-rank tensors and vector-like rank-1 tensors (for biases).</li> <li>
<p>Handles rank-1 tensors by allreducing them without compression:</p>  <p>2.1. Allocate contiguous memory for those rank-1 tensors, and allreduces all the rank-1 tensors as a batch, without compression;</p> <p>2.2. Copies the individual rank-1 tensors from the contiguous memory back to the input tensor.</p>  </li> <li>
<p>Handles high-rank tensors by PowerSGD compression:</p>  <p>3.1. For each high-rank tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;</p> <p>3.2. Computes each P in Ps, which is equal to MQ;</p> <p>3.3. Allreduces Ps as a batch;</p> <p>3.4. Orthogonalizes each P in Ps;</p> <p>3.5. Computes each Q in Qs, which is approximately equal to M^TP;</p> <p>3.6. Allreduces Qs as a batch;</p> <p>3.7. Computes each M among all the high-rank tensors, which is approximately equal to PQ^T.</p>  </li> </ol> <p>Note that this communication hook enforces vanilla allreduce for the first <code>state.start_powerSGD_iter</code> iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>state</strong> (<a class="reference internal" href="#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState" title="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState">PowerSGDState</a>) – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune <code>matrix_approximation_rank`</code> and <code>start_powerSGD_iter</code>.</li> <li>
<strong>bucket</strong> (<em>dist._GradBucket</em>) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Future handler of the communication, which updates the gradients in place.</p> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)
&gt;&gt;&gt; ddp_model.register_comm_hook(state, powerSGD_hook)
</pre> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook">
<code>torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.batched_powerSGD_hook(state, bucket)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributed/algorithms/ddp_comm_hooks/powerSGD_hook.html#batched_powerSGD_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>This DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the <a class="reference external" href="https://arxiv.org/abs/1905.13727">paper</a>. This variant does not compress the gradients layer by layer, but instead compresses the flattened input tensor that batches all the gradients. Therefore, it is <strong>faster</strong> than <a class="reference internal" href="#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook" title="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook"><code>powerSGD_hook()</code></a>, but usually results in a <strong>much lower accuracy</strong>, unless <code>matrix_approximation_rank</code> is 1.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Increasing <code>matrix_approximation_rank</code> here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider <a class="reference internal" href="#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook" title="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.powerSGD_hook"><code>powerSGD_hook()</code></a> first, and only consider this variant when a satisfactory accuracy can be achieved when <code>matrix_approximation_rank</code> is 1.</p> </div> <p>Once gradient tensors are aggregated across all workers, this hook applies compression as follows:</p> <ol class="arabic simple"> <li>Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings;</li> <li>Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;</li> <li>Computes P, which is equal to MQ;</li> <li>Allreduces P;</li> <li>Orthogonalizes P;</li> <li>Computes Q, which is approximately equal to M^TP;</li> <li>Allreduces Q;</li> <li>Computes M, which is approximately equal to PQ^T.</li> <li>Truncates the input tensor to the original length.</li> </ol> <p>Note that this communication hook enforces vanilla allreduce for the first <code>state.start_powerSGD_iter</code> iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>state</strong> (<a class="reference internal" href="#torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState" title="torch.distributed.algorithms.ddp_comm_hooks.powerSGD_hook.PowerSGDState">PowerSGDState</a>) – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune <code>matrix_approximation_rank</code> and <code>start_powerSGD_iter</code>.</li> <li>
<strong>bucket</strong> (<em>dist._GradBucket</em>) – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode at this time, only exactly one tensor is stored in this bucket.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Future handler of the communication, which updates the gradients in place.</p> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1)
&gt;&gt;&gt; ddp_model.register_comm_hook(state, batched_powerSGD_hook)
</pre> </dd> </dl> </dd>
</dl>    <h2 id="acknowledgements">Acknowledgements</h2> <p>Many thanks to PowerSGD paper author <strong>Thijs Vogels</strong> for the code review on PowerSGD communication hook, as well as the <a class="reference external" href="https://observablehq.com/@tvogels/powersgd-benchmark">comparison experiments</a>, which show that the performance of PowerSGD communication hook is on par with the implementation in the original <a class="reference external" href="https://arxiv.org/abs/1905.13727">paper</a>.</p><div class="_attribution">
  <p class="_attribution-p">
    <a href="https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html" class="_attribution-link" target="_blank">https://pytorch.org/docs/1.8.0/ddp_comm_hooks.html</a>
  </p>
</div>
