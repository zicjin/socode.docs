<h1 class="devsite-page-title">tf.data.experimental.parallel_interleave</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.data.experimental.parallel_interleave"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/experimental/parallel_interleave">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/interleave_ops.py#L37-L99">  View source on GitHub </a> </td>
</table>  <p>A parallel version of the <a href="../dataset#interleave"><code translate="no" dir="ltr">Dataset.interleave()</code></a> transformation. (deprecated)</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use <a href="../dataset#interleave"><code translate="no" dir="ltr">tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)</code></a> instead. If sloppy execution is desired, use <a href="../options#experimental_deterministic"><code translate="no" dir="ltr">tf.data.Options.experimental_deterministic</code></a>.</span></aside> <p><code translate="no" dir="ltr">parallel_interleave()</code> maps <code translate="no" dir="ltr">map_func</code> across its input to produce nested datasets, and outputs their elements interleaved. Unlike <a href="../dataset#interleave"><code translate="no" dir="ltr">tf.data.Dataset.interleave</code></a>, it gets elements from <code translate="no" dir="ltr">cycle_length</code> nested datasets in parallel, which increases the throughput, especially in the presence of stragglers. Furthermore, the <code translate="no" dir="ltr">sloppy</code> argument can be used to improve performance, by relaxing the requirement that the outputs are produced in a deterministic order, and allowing the implementation to skip over nested datasets whose elements are not readily available when requested.</p> <h4 id="example_usage">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Preprocess 4 files concurrently.
filenames = tf.data.Dataset.list_files("/path/to/data/train*.tfrecords")
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
</pre> <p>WARNING: If <code translate="no" dir="ltr">sloppy</code> is <code translate="no" dir="ltr">True</code>, the order of produced elements is not deterministic.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">map_func</code></b>: A function mapping a nested structure of tensors to a <code translate="no" dir="ltr">Dataset</code>.</li> <li>
<b><code translate="no" dir="ltr">cycle_length</code></b>: The number of input <code translate="no" dir="ltr">Dataset</code>s to interleave from in parallel.</li> <li>
<b><code translate="no" dir="ltr">block_length</code></b>: The number of consecutive elements to pull from an input <code translate="no" dir="ltr">Dataset</code> before advancing to the next input <code translate="no" dir="ltr">Dataset</code>.</li> <li>
<b><code translate="no" dir="ltr">sloppy</code></b>: If false, elements are produced in deterministic order. Otherwise, the implementation is allowed, for the sake of expediency, to produce elements in a non-deterministic order.</li> <li>
<b><code translate="no" dir="ltr">buffer_output_elements</code></b>: The number of elements each iterator being interleaved should buffer (similar to the <code translate="no" dir="ltr">.prefetch()</code> transformation for each interleaved iterator).</li> <li>
<b><code translate="no" dir="ltr">prefetch_input_elements</code></b>: The number of input elements to transform to iterators before they are needed for interleaving.</li> </ul> <h4 id="returns">Returns:</h4> <p>A <code translate="no" dir="ltr">Dataset</code> transformation function, which can be passed to <a href="../dataset#apply"><code translate="no" dir="ltr">tf.data.Dataset.apply</code></a>.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="parallel_interleave"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.parallel_interleave</code></a></li> <li><a href="parallel_interleave"><code translate="no" dir="ltr">tf.compat.v2.data.experimental.parallel_interleave</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/parallel_interleave" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/data/experimental/parallel_interleave</a>
  </p>
</div>
