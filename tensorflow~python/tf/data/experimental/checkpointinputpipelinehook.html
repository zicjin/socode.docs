<h1 class="devsite-page-title">tf.data.experimental.CheckpointInputPipelineHook</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.data.experimental.CheckpointInputPipelineHook"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="after_create_session"> <meta itemprop="property" content="after_run"> <meta itemprop="property" content="before_run"> <meta itemprop="property" content="begin"> <meta itemprop="property" content="end"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/data/experimental/CheckpointInputPipelineHook">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L93-L244">  View source on GitHub </a> </td>
</table>  <h2 id="class_checkpointinputpipelinehook_2">Class <code translate="no" dir="ltr">CheckpointInputPipelineHook</code>
</h2> <p>Checkpoints input pipeline state every N steps or seconds.</p> <p>Inherits From: <a href="../../estimator/sessionrunhook"><code translate="no" dir="ltr">SessionRunHook</code></a></p>  <p>This hook saves the state of the iterators in the <code translate="no" dir="ltr">Graph</code> so that when training is resumed the input pipeline continues from where it left off. This could potentially avoid overfitting in certain pipelines where the number of training steps per eval are small compared to the dataset size or if the training pipeline is pre-empted.</p> <p>Differences from <code translate="no" dir="ltr">CheckpointSaverHook</code>:</p> <ol> <li>Saves only the input pipelines in the "iterators" collection and not the global variables or other saveable objects.</li> <li>Does not write the <code translate="no" dir="ltr">GraphDef</code> and <code translate="no" dir="ltr">MetaGraphDef</code> to the summary.</li> </ol> <p>Example of checkpointing the training pipeline:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.data.experimental.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  # Note: We do not pass the hook here.
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
</pre> <p>This hook should be used if the input pipeline state needs to be saved separate from the model checkpoint. Doing so may be useful for a few reasons:</p> <ol> <li>The input pipeline checkpoint may be large, if there are large shuffle or prefetch buffers for instance, and may bloat the checkpoint size.</li> <li>If the input pipeline is shared between training and validation, restoring the checkpoint during validation may override the validation input pipeline.</li> </ol> <p>For saving the input pipeline checkpoint alongside the model weights use <a href="make_saveable_from_iterator"><code translate="no" dir="ltr">tf.data.experimental.make_saveable_from_iterator</code></a> directly to create a <code translate="no" dir="ltr">SaveableObject</code> and add to the <code translate="no" dir="ltr">SAVEABLE_OBJECTS</code> collection. Note, however, that you will need to be careful not to restore the training iterator during eval. You can do that by not adding the iterator to the SAVEABLE_OBJECTS collector when building the eval graph.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L138-L184">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(estimator)
</pre> <p>Initializes a <code translate="no" dir="ltr">CheckpointInputPipelineHook</code>.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">estimator</code></b>: Estimator.</li> </ul> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: One of <code translate="no" dir="ltr">save_steps</code> or <code translate="no" dir="ltr">save_secs</code> should be set.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: At most one of saver or scaffold should be set.</li> </ul> <h2 id="methods_2">Methods</h2> <h3 id="after_create_session"><code translate="no" dir="ltr">after_create_session</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/session_run_hook.py#L112-L127">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">after_create_session(
    session,
    coord
)
</pre> <p>Called when new TensorFlow session is created.</p> <p>This is called to signal the hooks that a new session has been created. This has two essential differences with the situation in which <code translate="no" dir="ltr">begin</code> is called:</p> <ul> <li>When this is called, the graph is finalized and ops can no longer be added to the graph.</li> <li>This method will also be called as a result of recovering a wrapped session, not only at the beginning of the overall session.</li> </ul> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">session</code></b>: A TensorFlow Session that has been created.</li> <li>
<b><code translate="no" dir="ltr">coord</code></b>: A Coordinator object which keeps track of all threads.</li> </ul> <h3 id="after_run"><code translate="no" dir="ltr">after_run</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L240-L241">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">after_run(
    run_context,
    run_values
)
</pre> <p>Called after each call to run().</p> <p>The <code translate="no" dir="ltr">run_values</code> argument contains results of requested ops/tensors by <code translate="no" dir="ltr">before_run()</code>.</p> <p>The <code translate="no" dir="ltr">run_context</code> argument is the same one send to <code translate="no" dir="ltr">before_run</code> call. <code translate="no" dir="ltr">run_context.request_stop()</code> can be called to stop the iteration.</p> <p>If <code translate="no" dir="ltr">session.run()</code> raises any exceptions then <code translate="no" dir="ltr">after_run()</code> is not called.</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">run_context</code></b>: A <code translate="no" dir="ltr">SessionRunContext</code> object.</li> <li>
<b><code translate="no" dir="ltr">run_values</code></b>: A SessionRunValues object.</li> </ul> <h3 id="before_run"><code translate="no" dir="ltr">before_run</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L234-L238">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">before_run(run_context)
</pre> <p>Called before each call to run().</p> <p>You can return from this call a <code translate="no" dir="ltr">SessionRunArgs</code> object indicating ops or tensors to add to the upcoming <code translate="no" dir="ltr">run()</code> call. These ops/tensors will be run together with the ops/tensors originally passed to the original run() call. The run args you return can also contain feeds to be added to the run() call.</p> <p>The <code translate="no" dir="ltr">run_context</code> argument is a <code translate="no" dir="ltr">SessionRunContext</code> that provides information about the upcoming <code translate="no" dir="ltr">run()</code> call: the originally requested op/tensors, the TensorFlow Session.</p> <p>At this point graph is finalized and you can not add ops.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">run_context</code></b>: A <code translate="no" dir="ltr">SessionRunContext</code> object.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>None or a <code translate="no" dir="ltr">SessionRunArgs</code> object.</p> <h3 id="begin"><code translate="no" dir="ltr">begin</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L186-L197">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">begin()
</pre> <p>Called once before using the session.</p> <p>When called, the default graph is the one that will be launched in the session. The hook can modify the graph by adding new operations to it. After the <code translate="no" dir="ltr">begin()</code> call the graph will be finalized and the other callbacks can not modify the graph anymore. Second call of <code translate="no" dir="ltr">begin()</code> on the same graph, should not change the graph.</p> <h3 id="end"><code translate="no" dir="ltr">end</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/data/experimental/ops/iterator_ops.py#L243-L244">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">end(session)
</pre> <p>Called at the end of session.</p> <p>The <code translate="no" dir="ltr">session</code> argument can be used in case the hook wants to run final ops, such as saving a last checkpoint.</p> <p>If <code translate="no" dir="ltr">session.run()</code> raises exception other than OutOfRangeError or StopIteration then <code translate="no" dir="ltr">end()</code> is not called. Note the difference between <code translate="no" dir="ltr">end()</code> and <code translate="no" dir="ltr">after_run()</code> behavior when <code translate="no" dir="ltr">session.run()</code> raises OutOfRangeError or StopIteration. In that case <code translate="no" dir="ltr">end()</code> is called but <code translate="no" dir="ltr">after_run()</code> is not called.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">session</code></b>: A TensorFlow Session that will be soon closed.</li> </ul> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="checkpointinputpipelinehook"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.CheckpointInputPipelineHook</code></a></li> <li><a href="checkpointinputpipelinehook"><code translate="no" dir="ltr">tf.compat.v2.data.experimental.CheckpointInputPipelineHook</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/CheckpointInputPipelineHook" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/data/experimental/CheckpointInputPipelineHook</a>
  </p>
</div>
