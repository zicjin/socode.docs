<h1 class="devsite-page-title">tf.tpu.experimental.DeviceAssignment</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.tpu.experimental.DeviceAssignment"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="core_assignment"> <meta itemprop="property" content="num_cores_per_replica"> <meta itemprop="property" content="num_replicas"> <meta itemprop="property" content="topology"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="build"> <meta itemprop="property" content="coordinates"> <meta itemprop="property" content="host_device"> <meta itemprop="property" content="lookup_replicas"> <meta itemprop="property" content="tpu_device"> <meta itemprop="property" content="tpu_ordinal"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/tpu/experimental/DeviceAssignment">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L59-L175">  View source on GitHub </a> </td>
</table>  <h2 id="class_deviceassignment">Class <code translate="no" dir="ltr">DeviceAssignment</code>
</h2> <p>Mapping from logical cores in a computation to the physical TPU topology.</p>  <p>Prefer to use the <a href="deviceassignment#build"><code translate="no" dir="ltr">DeviceAssignment.build()</code></a> helper to construct a <code translate="no" dir="ltr">DeviceAssignment</code>; it is easier if less flexible than constructing a <code translate="no" dir="ltr">DeviceAssignment</code> directly.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L67-L102">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    topology,
    core_assignment
)
</pre> <p>Constructs a <code translate="no" dir="ltr">DeviceAssignment</code> object.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">topology</code></b>: A <code translate="no" dir="ltr">Topology</code> object that describes the physical TPU topology.</li> <li>
<b><code translate="no" dir="ltr">core_assignment</code></b>: A logical to physical core mapping, represented as a rank 3 numpy array. See the description of the <code translate="no" dir="ltr">core_assignment</code> property for more details.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">topology</code> is not <code translate="no" dir="ltr">Topology</code> object.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">core_assignment</code> is not a rank 3 numpy array.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="core_assignment"><code translate="no" dir="ltr">core_assignment</code></h3> <p>The logical to physical core mapping.</p> <h4 id="returns">Returns:</h4> <p>An integer numpy array of rank 3, with shape <code translate="no" dir="ltr">[num_replicas, num_cores_per_replica, topology_rank]</code>. Maps (replica, logical core) pairs to physical topology coordinates.</p> <h3 id="num_cores_per_replica"><code translate="no" dir="ltr">num_cores_per_replica</code></h3> <p>The number of cores per replica.</p> <h3 id="num_replicas"><code translate="no" dir="ltr">num_replicas</code></h3> <p>The number of replicas of the computation.</p> <h3 id="topology"><code translate="no" dir="ltr">topology</code></h3> <p>A <code translate="no" dir="ltr">Topology</code> that describes the TPU topology.</p> <h2 id="methods">Methods</h2> <h3 id="build"><code translate="no" dir="ltr">build</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L169-L175">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@staticmethod
build(
    topology,
    computation_shape=None,
    computation_stride=None,
    num_replicas=1
)
</pre> <h3 id="coordinates"><code translate="no" dir="ltr">coordinates</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L130-L132">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">coordinates(
    replica,
    logical_core
)
</pre> <p>Returns the physical topology coordinates of a logical core.</p> <h3 id="host_device"><code translate="no" dir="ltr">host_device</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L159-L162">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">host_device(
    replica=0,
    logical_core=0,
    job=None
)
</pre> <p>Returns the CPU device attached to a logical core.</p> <h3 id="lookup_replicas"><code translate="no" dir="ltr">lookup_replicas</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L134-L152">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">lookup_replicas(
    task_id,
    logical_core
)
</pre> <p>Lookup replica ids by task number and logical core.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">task_id</code></b>: TensorFlow task number.</li> <li>
<b><code translate="no" dir="ltr">logical_core</code></b>: An integer, identifying a logical core.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A sorted list of the replicas that are attached to that task and logical_core.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If no replica exists in the task which contains the logical core.</li> </ul> <h3 id="tpu_device"><code translate="no" dir="ltr">tpu_device</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L164-L167">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tpu_device(
    replica=0,
    logical_core=0,
    job=None
)
</pre> <p>Returns the name of the TPU device assigned to a logical core.</p> <h3 id="tpu_ordinal"><code translate="no" dir="ltr">tpu_ordinal</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/device_assignment.py#L154-L157">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tpu_ordinal(
    replica=0,
    logical_core=0
)
</pre> <p>Returns the ordinal of the TPU device assigned to a logical core.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="deviceassignment"><code translate="no" dir="ltr">tf.compat.v1.tpu.experimental.DeviceAssignment</code></a></li> <li><a href="deviceassignment"><code translate="no" dir="ltr">tf.compat.v2.tpu.experimental.DeviceAssignment</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/DeviceAssignment" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/DeviceAssignment</a>
  </p>
</div>
