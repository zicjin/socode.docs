<h1 class="devsite-page-title">tf.Tensor</h1>    <devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>  <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.Tensor"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="device"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="graph"> <meta itemprop="property" content="name"> <meta itemprop="property" content="op"> <meta itemprop="property" content="shape"> <meta itemprop="property" content="value_index"> <meta itemprop="property" content="__abs__"> <meta itemprop="property" content="__add__"> <meta itemprop="property" content="__and__"> <meta itemprop="property" content="__bool__"> <meta itemprop="property" content="__div__"> <meta itemprop="property" content="__eq__"> <meta itemprop="property" content="__floordiv__"> <meta itemprop="property" content="__ge__"> <meta itemprop="property" content="__getitem__"> <meta itemprop="property" content="__gt__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="__invert__"> <meta itemprop="property" content="__iter__"> <meta itemprop="property" content="__le__"> <meta itemprop="property" content="__len__"> <meta itemprop="property" content="__lt__"> <meta itemprop="property" content="__matmul__"> <meta itemprop="property" content="__mod__"> <meta itemprop="property" content="__mul__"> <meta itemprop="property" content="__ne__"> <meta itemprop="property" content="__neg__"> <meta itemprop="property" content="__nonzero__"> <meta itemprop="property" content="__or__"> <meta itemprop="property" content="__pow__"> <meta itemprop="property" content="__radd__"> <meta itemprop="property" content="__rand__"> <meta itemprop="property" content="__rdiv__"> <meta itemprop="property" content="__rfloordiv__"> <meta itemprop="property" content="__rmatmul__"> <meta itemprop="property" content="__rmod__"> <meta itemprop="property" content="__rmul__"> <meta itemprop="property" content="__ror__"> <meta itemprop="property" content="__rpow__"> <meta itemprop="property" content="__rsub__"> <meta itemprop="property" content="__rtruediv__"> <meta itemprop="property" content="__rxor__"> <meta itemprop="property" content="__sub__"> <meta itemprop="property" content="__truediv__"> <meta itemprop="property" content="__xor__"> <meta itemprop="property" content="consumers"> <meta itemprop="property" content="eval"> <meta itemprop="property" content="experimental_ref"> <meta itemprop="property" content="get_shape"> <meta itemprop="property" content="set_shape"> <meta itemprop="property" content="OVERLOADABLE_OPERATORS"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L288-L843">  View source on GitHub </a> </td>
</table>  <h2 id="class_tensor_2">Class <code translate="no" dir="ltr">Tensor</code>
</h2> <p>Represents one of the outputs of an <code translate="no" dir="ltr">Operation</code>.</p>  <p>A <code translate="no" dir="ltr">Tensor</code> is a symbolic handle to one of the outputs of an <code translate="no" dir="ltr">Operation</code>. It does not hold the values of that operation's output, but instead provides a means of computing those values in a TensorFlow <a href="compat/v1/session"><code translate="no" dir="ltr">tf.compat.v1.Session</code></a>.</p> <p>This class has two primary purposes:</p> <ol> <li><p>A <code translate="no" dir="ltr">Tensor</code> can be passed as an input to another <code translate="no" dir="ltr">Operation</code>. This builds a dataflow connection between operations, which enables TensorFlow to execute an entire <code translate="no" dir="ltr">Graph</code> that represents a large, multi-step computation.</p></li> <li><p>After the graph has been launched in a session, the value of the <code translate="no" dir="ltr">Tensor</code> can be computed by passing it to <code translate="no" dir="ltr">tf.Session.run</code>. <code translate="no" dir="ltr">t.eval()</code> is a shortcut for calling <code translate="no" dir="ltr">tf.compat.v1.get_default_session().run(t)</code>.</p></li> </ol> <p>In the following example, <code translate="no" dir="ltr">c</code>, <code translate="no" dir="ltr">d</code>, and <code translate="no" dir="ltr">e</code> are symbolic <code translate="no" dir="ltr">Tensor</code> objects, whereas <code translate="no" dir="ltr">result</code> is a numpy array that stores a concrete value:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Build a dataflow graph.
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)

# Construct a `Session` to execute the graph.
sess = tf.compat.v1.Session()

# Execute the graph and store the value that `e` represents in `result`.
result = sess.run(e)
</pre> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L370-L395">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    op,
    value_index,
    dtype
)
</pre> <p>Creates a new <code translate="no" dir="ltr">Tensor</code>.</p> <h4 id="args_30">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">op</code></b>: An <code translate="no" dir="ltr">Operation</code>. <code translate="no" dir="ltr">Operation</code> that computes this tensor.</li> <li>
<b><code translate="no" dir="ltr">value_index</code></b>: An <code translate="no" dir="ltr">int</code>. Index of the operation's endpoint that produces this tensor.</li> <li>
<b><code translate="no" dir="ltr">dtype</code></b>: A <code translate="no" dir="ltr">DType</code>. Type of elements stored in this tensor.</li> </ul> <h4 id="raises_10">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If the op is not an <code translate="no" dir="ltr">Operation</code>.</li> </ul> <h2 id="properties_2">Properties</h2> <h3 id="device"><code translate="no" dir="ltr">device</code></h3> <p>The name of the device on which this tensor will be produced, or None.</p> <h3 id="dtype"><code translate="no" dir="ltr">dtype</code></h3> <p>The <code translate="no" dir="ltr">DType</code> of elements in this tensor.</p> <h3 id="graph"><code translate="no" dir="ltr">graph</code></h3> <p>The <code translate="no" dir="ltr">Graph</code> that contains this tensor.</p> <h3 id="name"><code translate="no" dir="ltr">name</code></h3> <p>The string name of this tensor.</p> <h3 id="op"><code translate="no" dir="ltr">op</code></h3> <p>The <code translate="no" dir="ltr">Operation</code> that produces this tensor as an output.</p> <h3 id="shape"><code translate="no" dir="ltr">shape</code></h3> <p>Returns the <code translate="no" dir="ltr">TensorShape</code> that represents the shape of this tensor.</p> <p>The shape is computed using shape inference functions that are registered in the Op for each <code translate="no" dir="ltr">Operation</code>. See <a href="tensorshape"><code translate="no" dir="ltr">tf.TensorShape</code></a> for more details of what a shape represents.</p> <p>The inferred shape of a tensor is used to provide shape information without having to launch the graph in a session. This can be used for debugging, and providing early error messages. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">c = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

print(c.shape)
==&gt; TensorShape([Dimension(2), Dimension(3)])

d = tf.constant([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])

print(d.shape)
==&gt; TensorShape([Dimension(4), Dimension(2)])

# Raises a ValueError, because `c` and `d` do not have compatible
# inner dimensions.
e = tf.matmul(c, d)

f = tf.matmul(c, d, transpose_a=True, transpose_b=True)

print(f.shape)
==&gt; TensorShape([Dimension(3), Dimension(4)])
</pre> <p>In some cases, the inferred shape may have unknown dimensions. If the caller has additional information about the values of these dimensions, <a href="tensor#set_shape"><code translate="no" dir="ltr">Tensor.set_shape()</code></a> can be used to augment the inferred shape.</p> <h4 id="returns_29">Returns:</h4> <p>A <code translate="no" dir="ltr">TensorShape</code> representing the shape of this tensor.</p> <h3 id="value_index"><code translate="no" dir="ltr">value_index</code></h3> <p>The index of this tensor in the outputs of its <code translate="no" dir="ltr">Operation</code>.</p> <h2 id="methods_2">Methods</h2> <h3 id="__abs__"><code translate="no" dir="ltr">__abs__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L248-L281">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__abs__(
    x,
    name=None
)
</pre> <p>Computes the absolute value of a tensor.</p> <p>Given a tensor of integer or floating-point values, this operation returns a tensor of the same type, where each element contains the absolute value of the corresponding element in the input.</p> <p>Given a tensor <code translate="no" dir="ltr">x</code> of complex numbers, this operation returns a tensor of type <code translate="no" dir="ltr">float32</code> or <code translate="no" dir="ltr">float64</code> that is the absolute value of each element in <code translate="no" dir="ltr">x</code>. All elements in <code translate="no" dir="ltr">x</code> must be complex numbers of the form \(a + bj\). The absolute value is computed as \( \sqrt{a^2 + b^2}\). For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])
tf.abs(x)  # [5.25594902, 6.60492229]
</pre> <h4 id="args_31">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">SparseTensor</code> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code> or <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_30">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">SparseTensor</code> the same size, type, and sparsity as <code translate="no" dir="ltr">x</code> with absolute values. Note, for <code translate="no" dir="ltr">complex64</code> or <code translate="no" dir="ltr">complex128</code> input, the returned <code translate="no" dir="ltr">Tensor</code> will be of type <code translate="no" dir="ltr">float32</code> or <code translate="no" dir="ltr">float64</code>, respectively.</p> <p>If <code translate="no" dir="ltr">x</code> is a <code translate="no" dir="ltr">SparseTensor</code>, returns <code translate="no" dir="ltr">SparseTensor(x.indices, tf.math.abs(x.values, ...), x.dense_shape)</code></p> <h3 id="__add__"><code translate="no" dir="ltr">__add__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__add__(
    x,
    y
)
</pre> <p>Dispatches to add for strings and add_v2 for all other types.</p> <h3 id="__and__"><code translate="no" dir="ltr">__and__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__and__(
    x,
    y
)
</pre> <p>Returns the truth value of x AND y element-wise.</p> <p><em>NOTE</em>: <a href="math/logical_and"><code translate="no" dir="ltr">math.logical_and</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_32">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_31">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__bool__"><code translate="no" dir="ltr">__bool__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L739-L757">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__bool__()
</pre> <p>Dummy method to prevent a tensor from being used as a Python <code translate="no" dir="ltr">bool</code>.</p> <p>This overload raises a <code translate="no" dir="ltr">TypeError</code> when the user inadvertently treats a <code translate="no" dir="ltr">Tensor</code> as a boolean (most commonly in an <code translate="no" dir="ltr">if</code> or <code translate="no" dir="ltr">while</code> statement), in code that was not converted by AutoGraph. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">if tf.constant(True):  # Will raise.
  # ...

if tf.constant(5) &lt; tf.constant(7):  # Will raise.
  # ...
</pre> <h4 id="raises_11">Raises:</h4> <p><code translate="no" dir="ltr">TypeError</code>.</p> <h3 id="__div__"><code translate="no" dir="ltr">__div__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__div__(
    x,
    y
)
</pre> <p>Divide two values using Python 2 semantics.</p> <p>Used for Tensor.<strong>div</strong>.</p> <h4 id="args_33">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: <code translate="no" dir="ltr">Tensor</code> numerator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: <code translate="no" dir="ltr">Tensor</code> denominator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_32">Returns:</h4> <p><code translate="no" dir="ltr">x / y</code> returns the quotient of x and y.</p> <h3 id="__eq__"><code translate="no" dir="ltr">__eq__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L1343-L1356">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__eq__(other)
</pre> <p>Compares two tensors element-wise for equality.</p> <h3 id="__floordiv__"><code translate="no" dir="ltr">__floordiv__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__floordiv__(
    x,
    y
)
</pre> <p>Divides <code translate="no" dir="ltr">x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <a href="raggedtensor#__div__"><code translate="no" dir="ltr">tf.compat.v1.div(x,y)</code></a> for integers, but uses <code translate="no" dir="ltr">tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code translate="no" dir="ltr">x // y</code> floor division in Python 3 and in Python 2.7 with <code translate="no" dir="ltr">from __future__ import division</code>.</p> <p><code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> must have the same type, and the result will have the same type as well.</p> <h4 id="args_34">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: <code translate="no" dir="ltr">Tensor</code> numerator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: <code translate="no" dir="ltr">Tensor</code> denominator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_33">Returns:</h4> <p><code translate="no" dir="ltr">x / y</code> rounded down.</p> <h4 id="raises_12">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If the inputs are complex.</li> </ul> <h3 id="__ge__"><code translate="no" dir="ltr">__ge__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__ge__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &gt;= y) element-wise.</p> <p><em>NOTE</em>: <a href="math/greater_equal"><code translate="no" dir="ltr">math.greater_equal</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="example_6">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==&gt; [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==&gt; [True, False, True, True]
</pre> <h4 id="args_35">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">uint32</code>, <code translate="no" dir="ltr">uint64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_34">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__getitem__"><code translate="no" dir="ltr">__getitem__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/array_ops.py#L759-L898">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__getitem__(
    tensor,
    slice_spec,
    var=None
)
</pre> <p>Overload for Tensor.<strong>getitem</strong>.</p> <p>This operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.</p> <h4 id="some_useful_examples_2">Some useful examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Strip leading and trailing 2 elements
foo = tf.constant([1,2,3,4,5,6])
print(foo[2:-2].eval())  # =&gt; [3,4]

# Skip every other row and reverse the order of the columns
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[::2,::-1].eval())  # =&gt; [[3,2,1], [9,8,7]]

# Use scalar tensors as indices on both dimensions
print(foo[tf.constant(0), tf.constant(2)].eval())  # =&gt; 3

# Insert another dimension
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval()) # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[:, tf.newaxis, :].eval()) # =&gt; [[[1,2,3]], [[4,5,6]], [[7,8,9]]]
print(foo[:, :, tf.newaxis].eval()) # =&gt; [[[1],[2],[3]], [[4],[5],[6]],
[[7],[8],[9]]]

# Ellipses (3 equivalent operations)
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[tf.newaxis, :, :].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis, ...].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]
print(foo[tf.newaxis].eval())  # =&gt; [[[1,2,3], [4,5,6], [7,8,9]]]

# Masks
foo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])
print(foo[foo &gt; 2].eval())  # =&gt; [3, 4, 5, 6, 7, 8, 9]
</pre> <h4 id="notes_2">Notes:</h4> <ul> <li>
<code translate="no" dir="ltr">tf.newaxis</code> is <code translate="no" dir="ltr">None</code> as in NumPy.</li> <li>An implicit ellipsis is placed at the end of the <code translate="no" dir="ltr">slice_spec</code>
</li> <li>NumPy advanced indexing is currently not supported.</li> </ul> <h4 id="args_36">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">tensor</code></b>: An ops.Tensor object.</li> <li>
<b><code translate="no" dir="ltr">slice_spec</code></b>: The arguments to Tensor.<strong>getitem</strong>.</li> <li>
<b><code translate="no" dir="ltr">var</code></b>: In the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).</li> </ul> <h4 id="returns_35">Returns:</h4> <p>The appropriate slice of "tensor", based on "slice_spec".</p> <h4 id="raises_13">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If a slice range is negative size.</li> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If the slice indices aren't int, slice, ellipsis, tf.newaxis or scalar int32/int64 tensors.</li> </ul> <h3 id="__gt__"><code translate="no" dir="ltr">__gt__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__gt__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &gt; y) element-wise.</p> <p><em>NOTE</em>: <a href="math/greater"><code translate="no" dir="ltr">math.greater</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="example_7">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==&gt; [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==&gt; [False, False, True]
</pre> <h4 id="args_37">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">uint32</code>, <code translate="no" dir="ltr">uint64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_36">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__invert__"><code translate="no" dir="ltr">__invert__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__invert__(
    x,
    name=None
)
</pre> <p>Returns the truth value of NOT x element-wise.</p> <h4 id="args_38">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_37">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__iter__"><code translate="no" dir="ltr">__iter__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L537-L550">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__iter__()
</pre> <h3 id="__le__"><code translate="no" dir="ltr">__le__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__le__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &lt;= y) element-wise.</p> <p><em>NOTE</em>: <a href="math/less_equal"><code translate="no" dir="ltr">math.less_equal</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="example_8">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==&gt; [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==&gt; [True, True, True]
</pre> <h4 id="args_39">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">uint32</code>, <code translate="no" dir="ltr">uint64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_38">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__len__"><code translate="no" dir="ltr">__len__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L730-L733">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__len__()
</pre> <h3 id="__lt__"><code translate="no" dir="ltr">__lt__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__lt__(
    x,
    y,
    name=None
)
</pre> <p>Returns the truth value of (x &lt; y) element-wise.</p> <p><em>NOTE</em>: <a href="math/less"><code translate="no" dir="ltr">math.less</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="example_9">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==&gt; [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==&gt; [False, True, True]
</pre> <h4 id="args_40">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">uint32</code>, <code translate="no" dir="ltr">uint64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_39">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__matmul__"><code translate="no" dir="ltr">__matmul__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__matmul__(
    x,
    y
)
</pre> <p>Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.</p> <p>Both matrices must be of the same type. The supported types are: <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</p> <p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code translate="no" dir="ltr">a_is_sparse</code> or <code translate="no" dir="ltr">b_is_sparse</code> flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code translate="no" dir="ltr">bfloat16</code> or <code translate="no" dir="ltr">float32</code>.</p> <p>A simple 2-D tensor matrix multiplication:</p> <blockquote>   <p>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) a # 2-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[1,"> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) b # 2-D tensor <tf.tensor: shape="(3," dtype="int32," numpy="array([["> c = tf.matmul(a, b) c # <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code> <tf.tensor: shape="(2," dtype="int32," numpy="array([["></tf.tensor:></tf.tensor:></tf.tensor:></p>   </blockquote> <p>A batch matrix multiplication with batch shape [2]</p> <blockquote>   <p>a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]) a # 3-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[["> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2]) b # 3-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[[13,"> c = tf.matmul(a, b) c # <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code> <tf.tensor: shape="(2," dtype="int32," numpy="array([[["></tf.tensor:></tf.tensor:></tf.tensor:></p>   </blockquote> <p>Since python &gt;= 3.5 the @ operator is supported (see <a href="https://www.python.org/dev/peps/pep-0465/">PEP 465</a>). In TensorFlow, it simply calls the <a href="linalg/matmul"><code translate="no" dir="ltr">tf.matmul()</code></a> function, so the following lines are equivalent:</p> <blockquote>   <p>d = a @ b @ [[10], [11]] d = tf.matmul(tf.matmul(a, b), [[10], [11]])</p>   </blockquote> <h4 id="args_41">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">a</code></b>: <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code> and rank &gt; 1.</li> <li>
<b><code translate="no" dir="ltr">b</code></b>: <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with same type and rank as <code translate="no" dir="ltr">a</code>.</li> <li>
<b><code translate="no" dir="ltr">transpose_a</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">transpose_b</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">adjoint_a</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is conjugated and transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">adjoint_b</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is conjugated and transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">a_is_sparse</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is treated as a sparse matrix.</li> <li>
<b><code translate="no" dir="ltr">b_is_sparse</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is treated as a sparse matrix.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Name for the operation (optional).</li> </ul> <h4 id="returns_40">Returns:</h4> <p>A <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of the same type as <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code> where each inner-most matrix is the product of the corresponding matrices in <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>, e.g. if all transpose or adjoint attributes are <code translate="no" dir="ltr">False</code>:</p> <p><code translate="no" dir="ltr">output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])</code>, for all indices <code translate="no" dir="ltr">i</code>, <code translate="no" dir="ltr">j</code>.</p> <ul> <li>
<b><code translate="no" dir="ltr">Note</code></b>: This is matrix product, not element-wise product.</li> </ul> <h4 id="raises_14">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">transpose_a</code> and <code translate="no" dir="ltr">adjoint_a</code>, or <code translate="no" dir="ltr">transpose_b</code> and <code translate="no" dir="ltr">adjoint_b</code> are both set to <code translate="no" dir="ltr">True</code>.</li> </ul> <h3 id="__mod__"><code translate="no" dir="ltr">__mod__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__mod__(
    x,
    y
)
</pre> <p>Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code translate="no" dir="ltr">floor(x / y) * y + mod(x, y) = x</code>.</p> <p><em>NOTE</em>: <a href="math/floormod"><code translate="no" dir="ltr">math.floormod</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_42">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_41">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">x</code>.</p> <h3 id="__mul__"><code translate="no" dir="ltr">__mul__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__mul__(
    x,
    y
)
</pre> <p>Dispatches cwise mul for "Dense<em>Dense" and "Dense</em>Sparse".</p> <h3 id="__ne__"><code translate="no" dir="ltr">__ne__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L1359-L1370">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__ne__(other)
</pre> <p>Compares two tensors element-wise for equality.</p> <h3 id="__neg__"><code translate="no" dir="ltr">__neg__</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__neg__(
    x,
    name=None
)
</pre> <p>Computes numerical negative value element-wise.</p> <p>I.e., \(y = -x\).</p> <h4 id="args_43">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_42">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">x</code>.</p> <p>If <code translate="no" dir="ltr">x</code> is a <code translate="no" dir="ltr">SparseTensor</code>, returns <code translate="no" dir="ltr">SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)</code></p> <h3 id="__nonzero__"><code translate="no" dir="ltr">__nonzero__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L759-L767">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__nonzero__()
</pre> <p>Dummy method to prevent a tensor from being used as a Python <code translate="no" dir="ltr">bool</code>.</p> <p>This is the Python 2.x counterpart to <code translate="no" dir="ltr">__bool__()</code> above.</p> <h4 id="raises_15">Raises:</h4> <p><code translate="no" dir="ltr">TypeError</code>.</p> <h3 id="__or__"><code translate="no" dir="ltr">__or__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__or__(
    x,
    y
)
</pre> <p>Returns the truth value of x OR y element-wise.</p> <p><em>NOTE</em>: <a href="math/logical_or"><code translate="no" dir="ltr">math.logical_or</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_44">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_43">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__pow__"><code translate="no" dir="ltr">__pow__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__pow__(
    x,
    y
)
</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code translate="no" dir="ltr">x</code> and a tensor <code translate="no" dir="ltr">y</code>, this operation computes \(x^y\) for corresponding elements in <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code>. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</pre> <h4 id="args_45">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, or <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, or <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_44">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>.</p> <h3 id="__radd__"><code translate="no" dir="ltr">__radd__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__radd__(
    y,
    x
)
</pre> <p>Dispatches to add for strings and add_v2 for all other types.</p> <h3 id="__rand__"><code translate="no" dir="ltr">__rand__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rand__(
    y,
    x
)
</pre> <p>Returns the truth value of x AND y element-wise.</p> <p><em>NOTE</em>: <a href="math/logical_and"><code translate="no" dir="ltr">math.logical_and</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_46">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_45">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__rdiv__"><code translate="no" dir="ltr">__rdiv__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rdiv__(
    y,
    x
)
</pre> <p>Divide two values using Python 2 semantics.</p> <p>Used for Tensor.<strong>div</strong>.</p> <h4 id="args_47">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: <code translate="no" dir="ltr">Tensor</code> numerator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: <code translate="no" dir="ltr">Tensor</code> denominator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_46">Returns:</h4> <p><code translate="no" dir="ltr">x / y</code> returns the quotient of x and y.</p> <h3 id="__rfloordiv__"><code translate="no" dir="ltr">__rfloordiv__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rfloordiv__(
    y,
    x
)
</pre> <p>Divides <code translate="no" dir="ltr">x / y</code> elementwise, rounding toward the most negative integer.</p> <p>The same as <a href="raggedtensor#__div__"><code translate="no" dir="ltr">tf.compat.v1.div(x,y)</code></a> for integers, but uses <code translate="no" dir="ltr">tf.floor(tf.compat.v1.div(x,y))</code> for floating point arguments so that the result is always an integer (though possibly an integer represented as floating point). This op is generated by <code translate="no" dir="ltr">x // y</code> floor division in Python 3 and in Python 2.7 with <code translate="no" dir="ltr">from __future__ import division</code>.</p> <p><code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> must have the same type, and the result will have the same type as well.</p> <h4 id="args_48">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: <code translate="no" dir="ltr">Tensor</code> numerator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: <code translate="no" dir="ltr">Tensor</code> denominator of real numeric type.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_47">Returns:</h4> <p><code translate="no" dir="ltr">x / y</code> rounded down.</p> <h4 id="raises_16">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If the inputs are complex.</li> </ul> <h3 id="__rmatmul__"><code translate="no" dir="ltr">__rmatmul__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rmatmul__(
    y,
    x
)
</pre> <p>Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p>The inputs must, following any transpositions, be tensors of rank &gt;= 2 where the inner 2 dimensions specify valid matrix multiplication dimensions, and any further outer dimensions specify matching batch size.</p> <p>Both matrices must be of the same type. The supported types are: <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</p> <p>Either matrix can be transposed or adjointed (conjugated and transposed) on the fly by setting one of the corresponding flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default.</p> <p>If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding <code translate="no" dir="ltr">a_is_sparse</code> or <code translate="no" dir="ltr">b_is_sparse</code> flag to <code translate="no" dir="ltr">True</code>. These are <code translate="no" dir="ltr">False</code> by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes <code translate="no" dir="ltr">bfloat16</code> or <code translate="no" dir="ltr">float32</code>.</p> <p>A simple 2-D tensor matrix multiplication:</p> <blockquote>   <p>a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) a # 2-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[1,"> b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) b # 2-D tensor <tf.tensor: shape="(3," dtype="int32," numpy="array([["> c = tf.matmul(a, b) c # <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code> <tf.tensor: shape="(2," dtype="int32," numpy="array([["></tf.tensor:></tf.tensor:></tf.tensor:></p>   </blockquote> <p>A batch matrix multiplication with batch shape [2]</p> <blockquote>   <p>a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]) a # 3-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[["> b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2]) b # 3-D tensor <tf.tensor: shape="(2," dtype="int32," numpy="array([[[13,"> c = tf.matmul(a, b) c # <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code> <tf.tensor: shape="(2," dtype="int32," numpy="array([[["></tf.tensor:></tf.tensor:></tf.tensor:></p>   </blockquote> <p>Since python &gt;= 3.5 the @ operator is supported (see <a href="https://www.python.org/dev/peps/pep-0465/">PEP 465</a>). In TensorFlow, it simply calls the <a href="linalg/matmul"><code translate="no" dir="ltr">tf.matmul()</code></a> function, so the following lines are equivalent:</p> <blockquote>   <p>d = a @ b @ [[10], [11]] d = tf.matmul(tf.matmul(a, b), [[10], [11]])</p>   </blockquote> <h4 id="args_49">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">a</code></b>: <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code> and rank &gt; 1.</li> <li>
<b><code translate="no" dir="ltr">b</code></b>: <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with same type and rank as <code translate="no" dir="ltr">a</code>.</li> <li>
<b><code translate="no" dir="ltr">transpose_a</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">transpose_b</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">adjoint_a</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is conjugated and transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">adjoint_b</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is conjugated and transposed before multiplication.</li> <li>
<b><code translate="no" dir="ltr">a_is_sparse</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">a</code> is treated as a sparse matrix.</li> <li>
<b><code translate="no" dir="ltr">b_is_sparse</code></b>: If <code translate="no" dir="ltr">True</code>, <code translate="no" dir="ltr">b</code> is treated as a sparse matrix.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Name for the operation (optional).</li> </ul> <h4 id="returns_48">Returns:</h4> <p>A <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> of the same type as <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code> where each inner-most matrix is the product of the corresponding matrices in <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>, e.g. if all transpose or adjoint attributes are <code translate="no" dir="ltr">False</code>:</p> <p><code translate="no" dir="ltr">output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j])</code>, for all indices <code translate="no" dir="ltr">i</code>, <code translate="no" dir="ltr">j</code>.</p> <ul> <li>
<b><code translate="no" dir="ltr">Note</code></b>: This is matrix product, not element-wise product.</li> </ul> <h4 id="raises_17">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">transpose_a</code> and <code translate="no" dir="ltr">adjoint_a</code>, or <code translate="no" dir="ltr">transpose_b</code> and <code translate="no" dir="ltr">adjoint_b</code> are both set to <code translate="no" dir="ltr">True</code>.</li> </ul> <h3 id="__rmod__"><code translate="no" dir="ltr">__rmod__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rmod__(
    y,
    x
)
</pre> <p>Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p>true, this follows Python semantics in that the result here is consistent with a flooring divide. E.g. <code translate="no" dir="ltr">floor(x / y) * y + mod(x, y) = x</code>.</p> <p><em>NOTE</em>: <a href="math/floormod"><code translate="no" dir="ltr">math.floormod</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_50">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_49">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">x</code>.</p> <h3 id="__rmul__"><code translate="no" dir="ltr">__rmul__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rmul__(
    y,
    x
)
</pre> <p>Dispatches cwise mul for "Dense<em>Dense" and "Dense</em>Sparse".</p> <h3 id="__ror__"><code translate="no" dir="ltr">__ror__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__ror__(
    y,
    x
)
</pre> <p>Returns the truth value of x OR y element-wise.</p> <p><em>NOTE</em>: <a href="math/logical_or"><code translate="no" dir="ltr">math.logical_or</code></a> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_51">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_50">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">bool</code>.</p> <h3 id="__rpow__"><code translate="no" dir="ltr">__rpow__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rpow__(
    y,
    x
)
</pre> <p>Computes the power of one value to another.</p> <p>Given a tensor <code translate="no" dir="ltr">x</code> and a tensor <code translate="no" dir="ltr">y</code>, this operation computes \(x^y\) for corresponding elements in <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code>. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
</pre> <h4 id="args_52">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, or <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float16</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, or <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_51">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>.</p> <h3 id="__rsub__"><code translate="no" dir="ltr">__rsub__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rsub__(
    y,
    x
)
</pre> <p>Returns x - y element-wise.</p> <p><em>NOTE</em>: <code translate="no" dir="ltr">Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_53">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_52">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">x</code>.</p> <h3 id="__rtruediv__"><code translate="no" dir="ltr">__rtruediv__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rtruediv__(
    y,
    x
)
</pre> <h3 id="__rxor__"><code translate="no" dir="ltr">__rxor__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L925-L928">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__rxor__(
    y,
    x
)
</pre> <p>Logical XOR function.</p> <p>x ^ y = (x | y) &amp; ~(x &amp; y)</p> <p>Inputs are tensor and if the tensors contains more than one element, an element-wise logical XOR is computed.</p> <h4 id="usage_3">Usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([False, False, True, True], dtype = tf.bool)
y = tf.constant([False, True, False, True], dtype = tf.bool)
z = tf.logical_xor(x, y, name="LogicalXor")
#  here z = [False  True  True False]
</pre> <h4 id="args_54">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> type bool.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type bool.</li> </ul> <h4 id="returns_53">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type bool with the same size as that of x or y.</p> <h3 id="__sub__"><code translate="no" dir="ltr">__sub__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__sub__(
    x,
    y
)
</pre> <p>Returns x - y element-wise.</p> <p><em>NOTE</em>: <code translate="no" dir="ltr">Subtract</code> supports broadcasting. More about broadcasting <a href="http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html">here</a></p> <h4 id="args_55">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must be one of the following types: <code translate="no" dir="ltr">bfloat16</code>, <code translate="no" dir="ltr">half</code>, <code translate="no" dir="ltr">float32</code>, <code translate="no" dir="ltr">float64</code>, <code translate="no" dir="ltr">uint8</code>, <code translate="no" dir="ltr">int8</code>, <code translate="no" dir="ltr">uint16</code>, <code translate="no" dir="ltr">int16</code>, <code translate="no" dir="ltr">int32</code>, <code translate="no" dir="ltr">int64</code>, <code translate="no" dir="ltr">complex64</code>, <code translate="no" dir="ltr">complex128</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code>. Must have the same type as <code translate="no" dir="ltr">x</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_54">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>. Has the same type as <code translate="no" dir="ltr">x</code>.</p> <h3 id="__truediv__"><code translate="no" dir="ltr">__truediv__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__truediv__(
    x,
    y
)
</pre> <h3 id="__xor__"><code translate="no" dir="ltr">__xor__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/math_ops.py#L899-L915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__xor__(
    x,
    y
)
</pre> <p>Logical XOR function.</p> <p>x ^ y = (x | y) &amp; ~(x &amp; y)</p> <p>Inputs are tensor and if the tensors contains more than one element, an element-wise logical XOR is computed.</p> <h4 id="usage_4">Usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant([False, False, True, True], dtype = tf.bool)
y = tf.constant([False, True, False, True], dtype = tf.bool)
z = tf.logical_xor(x, y, name="LogicalXor")
#  here z = [False  True  True False]
</pre> <h4 id="args_56">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: A <code translate="no" dir="ltr">Tensor</code> type bool.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: A <code translate="no" dir="ltr">Tensor</code> of type bool.</li> </ul> <h4 id="returns_55">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> of type bool with the same size as that of x or y.</p> <h3 id="consumers"><code translate="no" dir="ltr">consumers</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L644-L656">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">consumers()
</pre> <p>Returns a list of <code translate="no" dir="ltr">Operation</code>s that consume this tensor.</p> <h4 id="returns_56">Returns:</h4> <p>A list of <code translate="no" dir="ltr">Operation</code>s.</p> <h3 id="eval"><code translate="no" dir="ltr">eval</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L769-L790">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">eval(
    feed_dict=None,
    session=None
)
</pre> <p>Evaluates this tensor in a <code translate="no" dir="ltr">Session</code>.</p> <p>Calling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.</p> <p><em>N.B.</em> Before invoking <a href="tensor#eval"><code translate="no" dir="ltr">Tensor.eval()</code></a>, its graph must have been launched in a session, and either a default session must be available, or <code translate="no" dir="ltr">session</code> must be specified explicitly.</p> <h4 id="args_57">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">feed_dict</code></b>: A dictionary that maps <code translate="no" dir="ltr">Tensor</code> objects to feed values. See <code translate="no" dir="ltr">tf.Session.run</code> for a description of the valid feed values.</li> <li>
<b><code translate="no" dir="ltr">session</code></b>: (Optional.) The <code translate="no" dir="ltr">Session</code> to be used to evaluate this tensor. If none, the default session will be used.</li> </ul> <h4 id="returns_57">Returns:</h4> <p>A numpy array corresponding to the value of this tensor.</p> <h3 id="experimental_ref"><code translate="no" dir="ltr">experimental_ref</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L792-L843">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_ref()
</pre> <p>Returns a hashable reference object to this Tensor.</p> <aside class="warning"><strong>Warning:</strong><span> Experimental API that could be changed or removed.</span></aside> <p>The primary usecase for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as <code translate="no" dir="ltr">tensor.__hash__()</code> is no longer available starting Tensorflow 2.0.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">import tensorflow as tf

x = tf.constant(5)
y = tf.constant(10)
z = tf.constant(10)

# The followings will raise an exception starting 2.0
# TypeError: Tensor is unhashable if Tensor equality is enabled.
tensor_set = {x, y, z}
tensor_dict = {x: 'five', y: 'ten', z: 'ten'}
</pre> <p>Instead, we can use <code translate="no" dir="ltr">tensor.experimental_ref()</code>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tensor_set = {x.experimental_ref(),
              y.experimental_ref(),
              z.experimental_ref()}

print(x.experimental_ref() in tensor_set)
==&gt; True

tensor_dict = {x.experimental_ref(): 'five',
               y.experimental_ref(): 'ten',
               z.experimental_ref(): 'ten'}

print(tensor_dict[y.experimental_ref()])
==&gt; ten
</pre> <p>Also, the reference object provides <code translate="no" dir="ltr">.deref()</code> function that returns the original Tensor.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant(5)
print(x.experimental_ref().deref())
==&gt; tf.Tensor(5, shape=(), dtype=int32)
</pre> <h3 id="get_shape"><code translate="no" dir="ltr">get_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L572-L574">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_shape()
</pre> <p>Alias of Tensor.shape.</p> <h3 id="set_shape"><code translate="no" dir="ltr">set_shape</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/framework/ops.py#L576-L637">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">set_shape(shape)
</pre> <p>Updates the shape of this tensor.</p> <p>This method can be called multiple times, and will merge the given <code translate="no" dir="ltr">shape</code> with the current shape of this tensor. It can be used to provide additional information about the shape of this tensor that cannot be inferred from the graph alone. For example, this can be used to provide additional information about the shapes of images:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">_, image_data = tf.compat.v1.TFRecordReader(...).read(...)
image = tf.image.decode_png(image_data, channels=3)

# The height and width dimensions of `image` are data dependent, and
# cannot be computed without executing the op.
print(image.shape)
==&gt; TensorShape([Dimension(None), Dimension(None), Dimension(3)])

# We know that each image in this dataset is 28 x 28 pixels.
image.set_shape([28, 28, 3])
print(image.shape)
==&gt; TensorShape([Dimension(28), Dimension(28), Dimension(3)])
</pre> <p>NOTE: This shape is not enforced at runtime. Setting incorrect shapes can result in inconsistencies between the statically-known graph and the runtime value of tensors. For runtime validation of the shape, use <a href="ensure_shape"><code translate="no" dir="ltr">tf.ensure_shape</code></a> instead.</p> <h4 id="args_58">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">shape</code></b>: A <code translate="no" dir="ltr">TensorShape</code> representing the shape of this tensor, a <code translate="no" dir="ltr">TensorShapeProto</code>, a list, a tuple, or None.</li> </ul> <h4 id="raises_18">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">shape</code> is not compatible with the current shape of this tensor.</li> </ul> <h2 id="class_members_2">Class Members</h2> <ul> <li>
<code translate="no" dir="ltr">OVERLOADABLE_OPERATORS</code> 
</li> </ul> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="tensor"><code translate="no" dir="ltr">tf.compat.v1.Tensor</code></a></li> <li><a href="tensor"><code translate="no" dir="ltr">tf.compat.v2.Tensor</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/Tensor</a>
  </p>
</div>
