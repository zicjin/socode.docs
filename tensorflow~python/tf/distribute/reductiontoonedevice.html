<h1 class="devsite-page-title">tf.distribute.ReductionToOneDevice</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.ReductionToOneDevice"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="batch_reduce"> <meta itemprop="property" content="broadcast"> <meta itemprop="property" content="reduce"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distribute/ReductionToOneDevice">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L407-L448">  View source on GitHub </a> </td>
</table>  <h2 id="class_reductiontoonedevice">Class <code translate="no" dir="ltr">ReductionToOneDevice</code>
</h2> <p>Always do reduction to one device first and then do broadcasting.</p> <p>Inherits From: <a href="crossdeviceops"><code translate="no" dir="ltr">CrossDeviceOps</code></a></p>  <p>Batch reduction is done by reduction on each element one by one.</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">mirrored_strategy = tf.distribute.MirroredStrategy(
  cross_device_ops=tf.distribute.ReductionToOneDevice())
</pre> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L418-L429">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    reduce_to_device=None,
    accumulation_fn=None
)
</pre> <p>Initializes with a device to reduce to and a way to accumulate.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_to_device</code></b>: the intermediate device to reduce to. If None, reduce to the first device in <code translate="no" dir="ltr">destinations</code> of the <code translate="no" dir="ltr">reduce()</code> method.</li> <li>
<b><code translate="no" dir="ltr">accumulation_fn</code></b>: a function that does accumulation. If None, then <a href="../math/add_n"><code translate="no" dir="ltr">tf.math.add_n</code></a> is used.</li> </ul> <h2 id="methods">Methods</h2> <h3 id="batch_reduce"><code translate="no" dir="ltr">batch_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L284-L327">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">batch_reduce(
    reduce_op,
    value_destination_pairs
)
</pre> <p>Reduce PerReplica objects in a batch.</p> <p>Reduce each first element in <code translate="no" dir="ltr">value_destination_pairs</code> to each second element which indicates the destinations.</p> <p>This can be faster than multiple individual <code translate="no" dir="ltr">reduce</code>s because we can fuse several tensors into one or multiple packs before reduction.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: An instance of <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> that indicates how the <code translate="no" dir="ltr">per_replica_value</code> will be reduced.</li> <li>
<b><code translate="no" dir="ltr">value_destination_pairs</code></b>: a list or a tuple of PerReplica objects (or tensors with device set if there is one device) and destinations.</li> </ul> <h4 id="returns">Returns:</h4> <p>a list of Mirrored objects.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if <code translate="no" dir="ltr">value_destination_pairs</code> is not an iterable of tuples of PerReplica objects and destinations.</li> </ul> <h3 id="broadcast"><code translate="no" dir="ltr">broadcast</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L329-L340">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">broadcast(
    tensor,
    destinations
)
</pre> <p>Broadcast the <code translate="no" dir="ltr">tensor</code> to destinations.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">tensor</code></b>: the tensor to broadcast.</li> <li>
<b><code translate="no" dir="ltr">destinations</code></b>: the broadcast destinations.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>a Mirrored object.</p> <h3 id="reduce"><code translate="no" dir="ltr">reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L248-L282">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">reduce(
    reduce_op,
    per_replica_value,
    destinations
)
</pre> <p>Reduce <code translate="no" dir="ltr">per_replica_value</code> to <code translate="no" dir="ltr">destinations</code>.</p> <p>It runs the reduction operation defined by <code translate="no" dir="ltr">reduce_op</code> and put the result on <code translate="no" dir="ltr">destinations</code>.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: An instance of <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> that indicates how per_replica_value will be reduced.</li> <li>
<b><code translate="no" dir="ltr">per_replica_value</code></b>: a PerReplica object or a tensor with device set.</li> <li>
<b><code translate="no" dir="ltr">destinations</code></b>: the reduction destinations.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>a Mirrored object.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if per_replica_value can't be converted to a PerReplica object or if destinations aren't strings, Variables or DistributedValues</li> </ul> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="reductiontoonedevice"><code translate="no" dir="ltr">tf.compat.v1.distribute.ReductionToOneDevice</code></a></li> <li><a href="reductiontoonedevice"><code translate="no" dir="ltr">tf.compat.v2.distribute.ReductionToOneDevice</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice</a>
  </p>
</div>
