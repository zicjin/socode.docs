<h1 class="devsite-page-title">tf.distribute.ReplicaContext</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.ReplicaContext"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="devices"> <meta itemprop="property" content="num_replicas_in_sync"> <meta itemprop="property" content="replica_id_in_sync_group"> <meta itemprop="property" content="strategy"> <meta itemprop="property" content="__enter__"> <meta itemprop="property" content="__exit__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="all_reduce"> <meta itemprop="property" content="merge_call"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distribute/ReplicaContext">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1884-L2029">  View source on GitHub </a> </td>
</table>  <h2 id="class_replicacontext">Class <code translate="no" dir="ltr">ReplicaContext</code>
</h2> <p><a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> API when in a replica context.</p>  <p>You can use <a href="get_replica_context"><code translate="no" dir="ltr">tf.distribute.get_replica_context</code></a> to get an instance of <code translate="no" dir="ltr">ReplicaContext</code>. This should be inside your replicated step function, such as in a <a href="strategy#experimental_run_v2"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_run_v2</code></a> call.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1892-L1897">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    strategy,
    replica_id_in_sync_group
)
</pre> <p>Initialize self. See help(type(self)) for accurate signature.</p> <h2 id="properties">Properties</h2> <h3 id="devices"><code translate="no" dir="ltr">devices</code></h3> <p>The devices this replica is to be executed on, as a tuple of strings.</p> <h3 id="num_replicas_in_sync"><code translate="no" dir="ltr">num_replicas_in_sync</code></h3> <p>Returns number of replicas over which gradients are aggregated.</p> <h3 id="replica_id_in_sync_group"><code translate="no" dir="ltr">replica_id_in_sync_group</code></h3> <p>Returns the id of the replica being defined.</p> <p>This identifies the replica that is part of a sync group. Currently we assume that all sync groups contain the same number of replicas. The value of the replica id can range from 0 to <code translate="no" dir="ltr">num_replica_in_sync</code> - 1.</p> <h3 id="strategy"><code translate="no" dir="ltr">strategy</code></h3> <p>The current <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object.</p> <h2 id="methods">Methods</h2> <h3 id="__enter__"><code translate="no" dir="ltr">__enter__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1899-L1909">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__enter__()
</pre> <h3 id="__exit__"><code translate="no" dir="ltr">__exit__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1911-L1915">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__exit__(
    exception_type,
    exception_value,
    traceback
)
</pre> <h3 id="all_reduce"><code translate="no" dir="ltr">all_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1987-L2029">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">all_reduce(
    reduce_op,
    value
)
</pre> <p>All-reduces the given <code translate="no" dir="ltr">value Tensor</code> nest across replicas.</p> <p>If <code translate="no" dir="ltr">all_reduce</code> is called in any replica, it must be called in all replicas. The nested structure and <code translate="no" dir="ltr">Tensor</code> shapes must be identical in all replicas.</p> <p>IMPORTANT: The ordering of communications must be identical in all replicas.</p> <p>Example with two replicas: Replica 0 <code translate="no" dir="ltr">value</code>: {'a': 1, 'b': [40, 1]} Replica 1 <code translate="no" dir="ltr">value</code>: {'a': 3, 'b': [ 2, 98]}</p> <p>If <code translate="no" dir="ltr">reduce_op</code> == <code translate="no" dir="ltr">SUM</code>: Result (on all replicas): {'a': 4, 'b': [42, 99]}</p> <p>If <code translate="no" dir="ltr">reduce_op</code> == <code translate="no" dir="ltr">MEAN</code>: Result (on all replicas): {'a': 2, 'b': [21, 49.5]}</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: Reduction type, an instance of <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> enum.</li> <li>
<b><code translate="no" dir="ltr">value</code></b>: The nested structure of <code translate="no" dir="ltr">Tensor</code>s to all-reduce. The structure must be compatible with <a href="../nest"><code translate="no" dir="ltr">tf.nest</code></a>.</li> </ul> <h4 id="returns">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code> nest with the reduced <code translate="no" dir="ltr">value</code>s from each replica.</p> <h3 id="merge_call"><code translate="no" dir="ltr">merge_call</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L1917-L1949">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">merge_call(
    merge_fn,
    args=(),
    kwargs=None
)
</pre> <p>Merge args across replicas and run <code translate="no" dir="ltr">merge_fn</code> in a cross-replica context.</p> <p>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to <code translate="no" dir="ltr">strategy.experimental_run_v2(step_fn, ...)</code>.</p> <p>See <a href="strategy#experimental_run_v2"><code translate="no" dir="ltr">tf.distribute.Strategy.experimental_run_v2</code></a> for an explanation.</p> <p>If not inside a distributed scope, this is equivalent to:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.get_strategy()
with cross-replica-context(strategy):
  return merge_fn(strategy, *args, **kwargs)
</pre> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">merge_fn</code></b>: Function that joins arguments from threads that are given as PerReplica. It accepts <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object as the first argument.</li> <li>
<b><code translate="no" dir="ltr">args</code></b>: List or tuple with positional per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>.</li> <li>
<b><code translate="no" dir="ltr">kwargs</code></b>: Dict with keyword per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>The return value of <code translate="no" dir="ltr">merge_fn</code>, except for <code translate="no" dir="ltr">PerReplica</code> values which are unpacked.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="replicacontext"><code translate="no" dir="ltr">tf.compat.v1.distribute.ReplicaContext</code></a></li> <li><a href="replicacontext"><code translate="no" dir="ltr">tf.compat.v2.distribute.ReplicaContext</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext</a>
  </p>
</div>
