<h1 class="devsite-page-title">tf.distribute.NcclAllReduce</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.NcclAllReduce"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="batch_reduce"> <meta itemprop="property" content="broadcast"> <meta itemprop="property" content="reduce"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distribute/NcclAllReduce">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L807-L829">  View source on GitHub </a> </td>
</table>  <h2 id="class_ncclallreduce">Class <code translate="no" dir="ltr">NcclAllReduce</code>
</h2> <p>Reduction using NCCL all-reduce.</p>  <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L810-L829">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(num_packs=1)
</pre> <p>NCCL all-reduce implementation of CrossDeviceOps.</p> <p>It uses Nvidia NCCL for all-reduce. Before performing all-reduce, tensors will be repacked or aggregated for more efficient cross-device transportation.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">num_packs</code></b>: values will be packed in this many splits. <code translate="no" dir="ltr">num_packs</code> should be greater than or equals 0. When it is zero, no packing will be done.</li> </ul> <h4 id="raises">Raises:</h4> <p>ValueError if <code translate="no" dir="ltr">num_packs</code> is negative.</p> <h2 id="methods">Methods</h2> <h3 id="batch_reduce"><code translate="no" dir="ltr">batch_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L284-L327">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">batch_reduce(
    reduce_op,
    value_destination_pairs
)
</pre> <p>Reduce PerReplica objects in a batch.</p> <p>Reduce each first element in <code translate="no" dir="ltr">value_destination_pairs</code> to each second element which indicates the destinations.</p> <p>This can be faster than multiple individual <code translate="no" dir="ltr">reduce</code>s because we can fuse several tensors into one or multiple packs before reduction.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: An instance of <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> that indicates how the <code translate="no" dir="ltr">per_replica_value</code> will be reduced.</li> <li>
<b><code translate="no" dir="ltr">value_destination_pairs</code></b>: a list or a tuple of PerReplica objects (or tensors with device set if there is one device) and destinations.</li> </ul> <h4 id="returns">Returns:</h4> <p>a list of Mirrored objects.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if <code translate="no" dir="ltr">value_destination_pairs</code> is not an iterable of tuples of PerReplica objects and destinations.</li> </ul> <h3 id="broadcast"><code translate="no" dir="ltr">broadcast</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L329-L340">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">broadcast(
    tensor,
    destinations
)
</pre> <p>Broadcast the <code translate="no" dir="ltr">tensor</code> to destinations.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">tensor</code></b>: the tensor to broadcast.</li> <li>
<b><code translate="no" dir="ltr">destinations</code></b>: the broadcast destinations.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>a Mirrored object.</p> <h3 id="reduce"><code translate="no" dir="ltr">reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/cross_device_ops.py#L248-L282">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">reduce(
    reduce_op,
    per_replica_value,
    destinations
)
</pre> <p>Reduce <code translate="no" dir="ltr">per_replica_value</code> to <code translate="no" dir="ltr">destinations</code>.</p> <p>It runs the reduction operation defined by <code translate="no" dir="ltr">reduce_op</code> and put the result on <code translate="no" dir="ltr">destinations</code>.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: An instance of <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> that indicates how per_replica_value will be reduced.</li> <li>
<b><code translate="no" dir="ltr">per_replica_value</code></b>: a PerReplica object or a tensor with device set.</li> <li>
<b><code translate="no" dir="ltr">destinations</code></b>: the reduction destinations.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>a Mirrored object.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if per_replica_value can't be converted to a PerReplica object or if destinations aren't strings, Variables or DistributedValues</li> </ul> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="ncclallreduce"><code translate="no" dir="ltr">tf.compat.v1.distribute.NcclAllReduce</code></a></li> <li><a href="ncclallreduce"><code translate="no" dir="ltr">tf.compat.v2.distribute.NcclAllReduce</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce</a>
  </p>
</div>
