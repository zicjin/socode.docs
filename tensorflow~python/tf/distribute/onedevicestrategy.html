<h1 class="devsite-page-title">tf.distribute.OneDeviceStrategy</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.distribute.OneDeviceStrategy"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="extended"> <meta itemprop="property" content="num_replicas_in_sync"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="experimental_distribute_dataset"> <meta itemprop="property" content="experimental_distribute_datasets_from_function"> <meta itemprop="property" content="experimental_local_results"> <meta itemprop="property" content="experimental_make_numpy_dataset"> <meta itemprop="property" content="experimental_run_v2"> <meta itemprop="property" content="reduce"> <meta itemprop="property" content="scope"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/distribute/OneDeviceStrategy">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L41-L229">  View source on GitHub </a> </td>
</table>  <h2 id="class_onedevicestrategy">Class <code translate="no" dir="ltr">OneDeviceStrategy</code>
</h2> <p>A distribution strategy for running on a single device.</p> <p>Inherits From: <a href="strategy"><code translate="no" dir="ltr">Strategy</code></a></p> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/distribute/save_and_load">Save and load a model using a distribution strategy</a></li> </ul> <p>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via <code translate="no" dir="ltr">strategy.experimental_run_v2</code> will also be placed on the specified device as well.</p> <p>Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines.</p> <h4 id="for_example">For example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")

with strategy.scope():
  v = tf.Variable(1.0)
  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.experimental_run_v2(step_fn, args=(i,))
print(result)  # 90
</pre> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L72-L80">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(device)
</pre> <p>Creates a <code translate="no" dir="ltr">OneDeviceStrategy</code>.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">device</code></b>: Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: "/cpu:0", "/gpu:0", "/device:CPU:0", "/device:GPU:0"</li> </ul> <h2 id="properties">Properties</h2> <h3 id="extended"><code translate="no" dir="ltr">extended</code></h3> <p><a href="strategyextended"><code translate="no" dir="ltr">tf.distribute.StrategyExtended</code></a> with additional methods.</p> <h3 id="num_replicas_in_sync"><code translate="no" dir="ltr">num_replicas_in_sync</code></h3> <p>Returns number of replicas over which gradients are aggregated.</p> <h2 id="methods">Methods</h2> <h3 id="experimental_distribute_dataset"><code translate="no" dir="ltr">experimental_distribute_dataset</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L82-L108">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_distribute_dataset(dataset)
</pre> <p>Distributes a tf.data.Dataset instance provided via dataset.</p> <p>In this case, there is only one device, so this is only a thin wrapper around the input dataset. It will, however, prefetch the input data to the specified device. The returned distributed dataset can be iterated over similar to how regular datasets can.</p> <p>NOTE: Currently, the user cannot add any more transformations to a distributed dataset.</p> <h4 id="example">Example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.OneDeviceStrategy()
dataset = tf.data.Dataset.range(10).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
for x in dist_dataset:
  print(x)  # [0, 1], [2, 3],...
</pre> <p>Args: dataset: <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> to be prefetched to device.</p> <h4 id="returns">Returns:</h4> <p>A "distributed <code translate="no" dir="ltr">Dataset</code>" that the caller can iterate over.</p> <h3 id="experimental_distribute_datasets_from_function"><code translate="no" dir="ltr">experimental_distribute_datasets_from_function</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L110-L148">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_distribute_datasets_from_function(dataset_fn)
</pre> <p>Distributes <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> instances created by calls to <code translate="no" dir="ltr">dataset_fn</code>.</p> <p><code translate="no" dir="ltr">dataset_fn</code> will be called once for each worker in the strategy. In this case, we only have one worker and one device so <code translate="no" dir="ltr">dataset_fn</code> is called once.</p> <p>The <code translate="no" dir="ltr">dataset_fn</code> should take an <a href="inputcontext"><code translate="no" dir="ltr">tf.distribute.InputContext</code></a> instance where information about batching and input replication can be accessed:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.experimental_distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.experimental_run_v2(replica_fn, args=(batch,))
</pre> <p>IMPORTANT: The <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> returned by <code translate="no" dir="ltr">dataset_fn</code> should have a per-replica batch size, unlike <code translate="no" dir="ltr">experimental_distribute_dataset</code>, which uses the global batch size. This may be computed using <code translate="no" dir="ltr">input_context.get_per_replica_batch_size</code>.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">dataset_fn</code></b>: A function taking a <a href="inputcontext"><code translate="no" dir="ltr">tf.distribute.InputContext</code></a> instance and returning a <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a>.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A "distributed <code translate="no" dir="ltr">Dataset</code>", which the caller can iterate over like regular datasets.</p> <h3 id="experimental_local_results"><code translate="no" dir="ltr">experimental_local_results</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L150-L164">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_local_results(value)
</pre> <p>Returns the list of all local per-replica values contained in <code translate="no" dir="ltr">value</code>.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, the <code translate="no" dir="ltr">value</code> is always expected to be a single value, so the result is just the value in a tuple.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">value</code></b>: A value returned by <code translate="no" dir="ltr">experimental_run()</code>, <code translate="no" dir="ltr">experimental_run_v2()</code>, <code translate="no" dir="ltr">extended.call_for_each_replica()</code>, or a variable created in <code translate="no" dir="ltr">scope</code>.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>A tuple of values contained in <code translate="no" dir="ltr">value</code>. If <code translate="no" dir="ltr">value</code> represents a single value, this returns <code translate="no" dir="ltr">(value,).</code></p> <h3 id="experimental_make_numpy_dataset"><code translate="no" dir="ltr">experimental_make_numpy_dataset</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/distribute_lib.py#L578-L604">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_make_numpy_dataset(numpy_input)
</pre> <p>Makes a <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> for input provided via a numpy array.</p> <p>This avoids adding <code translate="no" dir="ltr">numpy_input</code> as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.</p> <p>Note that you will likely need to use <code translate="no" dir="ltr">experimental_distribute_dataset</code> with the returned dataset to further distribute it with the strategy.</p> <h4 id="example_2">Example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">numpy_input = np.ones([10], dtype=np.float32)
dataset = strategy.experimental_make_numpy_dataset(numpy_input)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
</pre> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">numpy_input</code></b>: A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> behavior.</li> </ul> <h4 id="returns_4">Returns:</h4> <p>A <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> representing <code translate="no" dir="ltr">numpy_input</code>.</p> <h3 id="experimental_run_v2"><code translate="no" dir="ltr">experimental_run_v2</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L166-L180">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">experimental_run_v2(
    fn,
    args=(),
    kwargs=None
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> on each replica, with the given arguments.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, <code translate="no" dir="ltr">fn</code> is simply called within a device scope for the given device, with the provided arguments.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">fn</code></b>: The function to run. The output must be a <a href="../nest"><code translate="no" dir="ltr">tf.nest</code></a> of <code translate="no" dir="ltr">Tensor</code>s.</li> <li>
<b><code translate="no" dir="ltr">args</code></b>: (Optional) Positional arguments to <code translate="no" dir="ltr">fn</code>.</li> <li>
<b><code translate="no" dir="ltr">kwargs</code></b>: (Optional) Keyword arguments to <code translate="no" dir="ltr">fn</code>.</li> </ul> <h4 id="returns_5">Returns:</h4> <p>Return value from running <code translate="no" dir="ltr">fn</code>.</p> <h3 id="reduce"><code translate="no" dir="ltr">reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L182-L213">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">reduce(
    reduce_op,
    value,
    axis
)
</pre> <p>Reduce <code translate="no" dir="ltr">value</code> across replicas.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, there is only one replica, so if axis=None, value is simply returned. If axis is specified as something other than None, such as axis=0, value is reduced along that axis and returned.</p> <h4 id="example_3">Example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">t = tf.range(10)

result = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=None).numpy()
# result: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

result = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=0).numpy()
# result: 45
</pre> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">reduce_op</code></b>: A <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined.</li> <li>
<b><code translate="no" dir="ltr">value</code></b>: A "per replica" value, e.g. returned by <code translate="no" dir="ltr">experimental_run_v2</code> to be combined into a single tensor.</li> <li>
<b><code translate="no" dir="ltr">axis</code></b>: Specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or <code translate="no" dir="ltr">None</code> to only reduce across replicas (e.g. if the tensor has no batch dimension).</li> </ul> <h4 id="returns_6">Returns:</h4> <p>A <code translate="no" dir="ltr">Tensor</code>.</p> <h3 id="scope"><code translate="no" dir="ltr">scope</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/distribute/one_device_strategy.py#L215-L229">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">scope()
</pre> <p>Returns a context manager selecting this Strategy as current.</p> <p>Inside a <code translate="no" dir="ltr">with strategy.scope():</code> code block, this thread will use a variable creator set by <code translate="no" dir="ltr">strategy</code>, and will enter its "cross-replica context".</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, all variables created inside <code translate="no" dir="ltr">strategy.scope()</code> will be on <code translate="no" dir="ltr">device</code> specified at strategy construction time. See example in the docs for this class.</p> <h4 id="returns_7">Returns:</h4> <p>A context manager to use for creating variables with this strategy.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="onedevicestrategy"><code translate="no" dir="ltr">tf.compat.v2.distribute.OneDeviceStrategy</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy</a>
  </p>
</div>
