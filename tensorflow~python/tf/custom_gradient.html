<h1 class="devsite-page-title">tf.custom_gradient</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.custom_gradient"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/custom_gradient">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/custom_gradient.py#L85-L214">  View source on GitHub </a> </td>
</table>  <p>Decorator to define a function with a custom gradient.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.custom_gradient(f=None)
</pre>  <p>This decorator allows fine grained control over the gradients of a sequence for operations. This may be useful for multiple reasons, including providing a more efficient or numerically stable gradient for a sequence of operations.</p> <p>For example, consider the following function that commonly occurs in the computation of cross entropy and log likelihoods:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def log1pexp(x):
  return tf.math.log(1 + tf.exp(x))
</pre> <p>Due to numerical instability, the gradient this function evaluated at x=100 is NaN. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">x = tf.constant(100.)
y = log1pexp(x)
dy = tf.gradients(y, x) # Will be NaN when evaluated.
</pre> <p>The gradient expression can be analytically simplified to provide numerical stability:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def log1pexp(x):
  e = tf.exp(x)
  def grad(dy):
    return dy * (1 - 1 / (1 + e))
  return tf.math.log(1 + e), grad
</pre> <p>With this definition, the gradient at x=100 will be correctly evaluated as 1.0.</p> <p>Nesting custom gradients can lead to unintuitive results. The default behavior does not correspond to n-th order derivatives. For example</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def op(x):
  y = op1(x)
  @tf.custom_gradient
  def grad_fn(dy):
    gdy = op2(x, y, dy)
    def grad_grad_fn(ddy):  # Not the 2nd order gradient of op w.r.t. x.
      return op3(x, y, dy, ddy)
    return gdy, grad_grad_fn
  return y, grad_fn
</pre> <p>The function <code translate="no" dir="ltr">grad_grad_fn</code> will be calculating the first order gradient of <code translate="no" dir="ltr">grad_fn</code> with respect to <code translate="no" dir="ltr">dy</code>, which is used to generate forward-mode gradient graphs from backward-mode gradient graphs, but is not the same as the second order gradient of <code translate="no" dir="ltr">op</code> with respect to <code translate="no" dir="ltr">x</code>.</p> <p>Instead, wrap nested <code translate="no" dir="ltr">@tf.custom_gradients</code> in another function:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@tf.custom_gradient
def op_with_fused_backprop(x):
  y, x_grad = fused_op(x)
  def first_order_gradient(dy):
    @tf.custom_gradient
    def first_order_custom(unused_x):
      def second_order_and_transpose(ddy):
        return second_order_for_x(...), gradient_wrt_dy(...)
      return x_grad, second_order_and_transpose
    return dy * first_order_custom(x)
  return y, first_order_gradient
</pre> <p>Additional arguments to the inner <code translate="no" dir="ltr">@tf.custom_gradient</code>-decorated function control the expected return values of the innermost function.</p> <p>See also <a href="registergradient"><code translate="no" dir="ltr">tf.RegisterGradient</code></a> which registers a gradient function for a primitive TensorFlow operation. <a href="custom_gradient"><code translate="no" dir="ltr">tf.custom_gradient</code></a> on the other hand allows for fine grained control over the gradient computation of a sequence of operations.</p> <p>Note that if the decorated function uses <code translate="no" dir="ltr">Variable</code>s, the enclosing variable scope must be using <code translate="no" dir="ltr">ResourceVariable</code>s.</p> <h4 id="args">Args:</h4> <ul> <li>
<p><b><code translate="no" dir="ltr">f</code></b>: function <code translate="no" dir="ltr">f(*x)</code> that returns a tuple <code translate="no" dir="ltr">(y, grad_fn)</code> where:</p> <ul> <li>
<code translate="no" dir="ltr">x</code> is a sequence of <code translate="no" dir="ltr">Tensor</code> inputs to the function.</li> <li>
<code translate="no" dir="ltr">y</code> is a <code translate="no" dir="ltr">Tensor</code> or sequence of <code translate="no" dir="ltr">Tensor</code> outputs of applying TensorFlow operations in <code translate="no" dir="ltr">f</code> to <code translate="no" dir="ltr">x</code>.</li> <li>
<p><code translate="no" dir="ltr">grad_fn</code> is a function with the signature <code translate="no" dir="ltr">g(*grad_ys)</code> which returns a list of <code translate="no" dir="ltr">Tensor</code>s - the derivatives of <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">y</code> with respect to the <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">x</code>. <code translate="no" dir="ltr">grad_ys</code> is a <code translate="no" dir="ltr">Tensor</code> or sequence of <code translate="no" dir="ltr">Tensor</code>s the same size as <code translate="no" dir="ltr">y</code> holding the initial value gradients for each <code translate="no" dir="ltr">Tensor</code> in <code translate="no" dir="ltr">y</code>. In a pure mathematical sense, a vector-argument vector-valued function <code translate="no" dir="ltr">f</code>'s derivatives should be its Jacobian matrix <code translate="no" dir="ltr">J</code>. Here we are expressing the Jacobian <code translate="no" dir="ltr">J</code> as a function <code translate="no" dir="ltr">grad_fn</code> which defines how <code translate="no" dir="ltr">J</code> will transform a vector <code translate="no" dir="ltr">grad_ys</code> when left-multiplied with it (<code translate="no" dir="ltr">grad_ys * J</code>). This functional representation of a matrix is convenient to use for chain-rule calculation (in e.g. the back-propagation algorithm).</p> <p>If <code translate="no" dir="ltr">f</code> uses <code translate="no" dir="ltr">Variable</code>s (that are not part of the inputs), i.e. through <code translate="no" dir="ltr">get_variable</code>, then <code translate="no" dir="ltr">grad_fn</code> should have signature <code translate="no" dir="ltr">g(*grad_ys, variables=None)</code>, where <code translate="no" dir="ltr">variables</code> is a list of the <code translate="no" dir="ltr">Variable</code>s, and return a 2-tuple <code translate="no" dir="ltr">(grad_xs, grad_vars)</code>, where <code translate="no" dir="ltr">grad_xs</code> is the same as above, and <code translate="no" dir="ltr">grad_vars</code> is a <code translate="no" dir="ltr">list&lt;Tensor&gt;</code> with the derivatives of <code translate="no" dir="ltr">Tensor</code>s in <code translate="no" dir="ltr">y</code> with respect to the variables (that is, grad_vars has one Tensor per variable in variables).</p>
</li> </ul>
</li> </ul> <h4 id="returns">Returns:</h4> <p>A function <code translate="no" dir="ltr">h(x)</code> which returns the same value as <code translate="no" dir="ltr">f(x)[0]</code> and whose gradient (as calculated by <a href="gradients"><code translate="no" dir="ltr">tf.gradients</code></a>) is determined by <code translate="no" dir="ltr">f(x)[1]</code>.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="custom_gradient"><code translate="no" dir="ltr">tf.compat.v1.custom_gradient</code></a></li> <li><a href="custom_gradient"><code translate="no" dir="ltr">tf.compat.v2.custom_gradient</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/custom_gradient" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/custom_gradient</a>
  </p>
</div>
