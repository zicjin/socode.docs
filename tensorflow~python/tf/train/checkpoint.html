<h1 class="devsite-page-title">tf.train.Checkpoint</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.train.Checkpoint"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="save_counter"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="restore"> <meta itemprop="property" content="save"> <meta itemprop="property" content="write"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/Checkpoint">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/tracking/util.py#L1692-L1989">  View source on GitHub </a> </td>
</table>  <h2 id="class_checkpoint">Class <code translate="no" dir="ltr">Checkpoint</code>
</h2> <p>Groups trackable objects, saving and restoring them.</p> <h3 id="used_in_the_guide">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/checkpoint">Training checkpoints</a></li> <li><a href="https://www.tensorflow.org/guide/eager">Eager execution</a></li> </ul> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/distribute/custom_training">Custom training with tf.distribute.Strategy</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/cyclegan">CycleGAN</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/dcgan">Deep Convolutional Generative Adversarial Network</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/pix2pix">Pix2Pix</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/image_captioning">Image captioning with visual attention</a></li> </ul> <p><code translate="no" dir="ltr">Checkpoint</code>'s constructor accepts keyword arguments whose values are types that contain trackable state, such as <a href="../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> implementations, <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, <code translate="no" dir="ltr">tf.keras.Layer</code> implementations, or <a href="../keras/model"><code translate="no" dir="ltr">tf.keras.Model</code></a> implementations. It saves these values with a checkpoint, and maintains a <code translate="no" dir="ltr">save_counter</code> for numbering checkpoints.</p> <h4 id="example_usage">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">import tensorflow as tf
import os

checkpoint_directory = "/tmp/training_checkpoints"
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt")

checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))
for _ in range(num_training_steps):
  optimizer.minimize( ... )  # Variables will be restored on creation.
status.assert_consumed()  # Optional sanity checks.
checkpoint.save(file_prefix=checkpoint_prefix)
</pre> <p><a href="checkpoint#save"><code translate="no" dir="ltr">Checkpoint.save</code></a> and <a href="checkpoint#restore"><code translate="no" dir="ltr">Checkpoint.restore</code></a> write and read object-based checkpoints, in contrast to TensorFlow 1.x's <a href="../compat/v1/train/saver"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> which writes and reads <code translate="no" dir="ltr">variable.name</code> based checkpoints. Object-based checkpointing saves a graph of dependencies between Python objects (<code translate="no" dir="ltr">Layer</code>s, <code translate="no" dir="ltr">Optimizer</code>s, <code translate="no" dir="ltr">Variable</code>s, etc.) with named edges, and this graph is used to match variables when restoring a checkpoint. It can be more robust to changes in the Python program, and helps to support restore-on-create for variables.</p> <p><code translate="no" dir="ltr">Checkpoint</code> objects have dependencies on the objects passed as keyword arguments to their constructors, and each dependency is given a name that is identical to the name of the keyword argument for which it was created. TensorFlow classes like <code translate="no" dir="ltr">Layer</code>s and <code translate="no" dir="ltr">Optimizer</code>s will automatically add dependencies on their variables (e.g. "kernel" and "bias" for <a href="../keras/layers/dense"><code translate="no" dir="ltr">tf.keras.layers.Dense</code></a>). Inheriting from <a href="../keras/model"><code translate="no" dir="ltr">tf.keras.Model</code></a> makes managing dependencies easy in user-defined classes, since <code translate="no" dir="ltr">Model</code> hooks into attribute assignment. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">class Regress(tf.keras.Model):

  def __init__(self):
    super(Regress, self).__init__()
    self.input_transform = tf.keras.layers.Dense(10)
    # ...

  def call(self, inputs):
    x = self.input_transform(inputs)
    # ...
</pre> <p>This <code translate="no" dir="ltr">Model</code> has a dependency named "input_transform" on its <code translate="no" dir="ltr">Dense</code> layer, which in turn depends on its variables. As a result, saving an instance of <code translate="no" dir="ltr">Regress</code> using <a href="checkpoint"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> will also save all the variables created by the <code translate="no" dir="ltr">Dense</code> layer.</p> <p>When variables are assigned to multiple workers, each worker writes its own section of the checkpoint. These sections are then merged/re-indexed to behave as a single checkpoint. This avoids copying all variables to one worker, but does require that all workers see a common filesystem.</p> <p>While <a href="../keras/model#save_weights"><code translate="no" dir="ltr">tf.keras.Model.save_weights</code></a> and <a href="checkpoint#save"><code translate="no" dir="ltr">tf.train.Checkpoint.save</code></a> save in the same format, note that the root of the resulting checkpoint is the object the save method is attached to. This means saving a <a href="../keras/model"><code translate="no" dir="ltr">tf.keras.Model</code></a> using <code translate="no" dir="ltr">save_weights</code> and loading into a <a href="checkpoint"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> with a <code translate="no" dir="ltr">Model</code> attached (or vice versa) will not match the <code translate="no" dir="ltr">Model</code>'s variables. See the <a href="https://www.tensorflow.org/guide/checkpoint">guide to training checkpoints</a> for details. Prefer <a href="checkpoint"><code translate="no" dir="ltr">tf.train.Checkpoint</code></a> over <a href="../keras/model#save_weights"><code translate="no" dir="ltr">tf.keras.Model.save_weights</code></a> for training checkpoints.</p> <h4 id="attributes">Attributes:</h4> <ul> <li>
<b><code translate="no" dir="ltr">save_counter</code></b>: Incremented when <code translate="no" dir="ltr">save()</code> is called. Used to number checkpoints.</li> </ul> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/tracking/util.py#L1774-L1796">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(**kwargs)
</pre> <p>Group objects into a training checkpoint.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">**kwargs</code></b>: Keyword arguments are set as attributes of this object, and are saved with the checkpoint. Values must be trackable objects.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If objects in <code translate="no" dir="ltr">kwargs</code> are not trackable.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="save_counter"><code translate="no" dir="ltr">save_counter</code></h3> <p>An integer variable which starts at zero and is incremented on save.</p> <p>Used to number checkpoints.</p> <h4 id="returns">Returns:</h4> <p>The save counter variable.</p> <h2 id="methods">Methods</h2> <h3 id="restore"><code translate="no" dir="ltr">restore</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/tracking/util.py#L1910-L1989">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">restore(save_path)
</pre> <p>Restore a training checkpoint.</p> <p>Restores this <code translate="no" dir="ltr">Checkpoint</code> and any objects it depends on.</p> <p>Either assigns values immediately if variables to restore have been created already, or defers restoration until the variables are created. Dependencies added after this call will be matched if they have a corresponding object in the checkpoint (the restore request will queue in any trackable object waiting for the expected dependency to be added).</p> <p>To ensure that loading is complete and no more assignments will take place, use the <code translate="no" dir="ltr">assert_consumed()</code> method of the status object returned by <code translate="no" dir="ltr">restore</code>:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">checkpoint = tf.train.Checkpoint( ... )
checkpoint.restore(path).assert_consumed()
</pre> <p>An exception will be raised if any Python objects in the dependency graph were not found in the checkpoint, or if any checkpointed values do not have a matching Python object.</p> <p>Name-based <a href="../compat/v1/train/saver"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> checkpoints from TensorFlow 1.x can be loaded using this method. Names are used to match variables. Re-encode name-based checkpoints using <a href="checkpoint#save"><code translate="no" dir="ltr">tf.train.Checkpoint.save</code></a> as soon as possible.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">save_path</code></b>: The path to the checkpoint, as returned by <code translate="no" dir="ltr">save</code> or <a href="latest_checkpoint"><code translate="no" dir="ltr">tf.train.latest_checkpoint</code></a>. If None (as when there is no latest checkpoint for <a href="latest_checkpoint"><code translate="no" dir="ltr">tf.train.latest_checkpoint</code></a> to return), returns an object which may run initializers for objects in the dependency graph. If the checkpoint was written by the name-based <a href="../compat/v1/train/saver"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a>, names are used to match variables.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A load status object, which can be used to make assertions about the status of a checkpoint restoration.</p> <p>The returned status object has the following methods:</p> <ul> <li><p><code translate="no" dir="ltr">assert_consumed()</code>: Raises an exception if any variables are unmatched: either checkpointed values which don't have a matching Python object or Python objects in the dependency graph with no values in the checkpoint. This method returns the status object, and so may be chained with other assertions.</p></li> <li><p><code translate="no" dir="ltr">assert_existing_objects_matched()</code>: Raises an exception if any existing Python objects in the dependency graph are unmatched. Unlike <code translate="no" dir="ltr">assert_consumed</code>, this assertion will pass if values in the checkpoint have no corresponding Python objects. For example a <code translate="no" dir="ltr">tf.keras.Layer</code> object which has not yet been built, and so has not created any variables, will pass this assertion but fail <code translate="no" dir="ltr">assert_consumed</code>. Useful when loading part of a larger checkpoint into a new Python program, e.g. a training checkpoint with a <a href="../compat/v1/train/optimizer"><code translate="no" dir="ltr">tf.compat.v1.train.Optimizer</code></a> was saved but only the state required for inference is being loaded. This method returns the status object, and so may be chained with other assertions.</p></li> <li><p><code translate="no" dir="ltr">assert_nontrivial_match()</code>: Asserts that something aside from the root object was matched. This is a very weak assertion, but is useful for sanity checking in library code where objects may exist in the checkpoint which haven't been created in Python and some Python objects may not have a checkpointed value.</p></li> <li><p><code translate="no" dir="ltr">expect_partial()</code>: Silence warnings about incomplete checkpoint restores. Warnings are otherwise printed for unused parts of the checkpoint file or object when the <code translate="no" dir="ltr">Checkpoint</code> object is deleted (often at program shutdown).</p></li> </ul> <h3 id="save"><code translate="no" dir="ltr">save</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/tracking/util.py#L1855-L1908">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">save(file_prefix)
</pre> <p>Saves a training checkpoint and provides basic checkpoint management.</p> <p>The saved checkpoint includes variables created by this object and any trackable objects it depends on at the time <a href="checkpoint#save"><code translate="no" dir="ltr">Checkpoint.save()</code></a> is called.</p> <p><code translate="no" dir="ltr">save</code> is a basic convenience wrapper around the <code translate="no" dir="ltr">write</code> method, sequentially numbering checkpoints using <code translate="no" dir="ltr">save_counter</code> and updating the metadata used by <a href="latest_checkpoint"><code translate="no" dir="ltr">tf.train.latest_checkpoint</code></a>. More advanced checkpoint management, for example garbage collection and custom numbering, may be provided by other utilities which also wrap <code translate="no" dir="ltr">write</code> (<a href="checkpointmanager"><code translate="no" dir="ltr">tf.train.CheckpointManager</code></a> for example).</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">file_prefix</code></b>: A prefix to use for the checkpoint filenames (/path/to/directory/and_a_prefix). Names are generated based on this prefix and <a href="checkpoint#save_counter"><code translate="no" dir="ltr">Checkpoint.save_counter</code></a>.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>The full path to the checkpoint.</p> <h3 id="write"><code translate="no" dir="ltr">write</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/tracking/util.py#L1813-L1841">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">write(file_prefix)
</pre> <p>Writes a training checkpoint.</p> <p>The checkpoint includes variables created by this object and any trackable objects it depends on at the time <a href="checkpoint#write"><code translate="no" dir="ltr">Checkpoint.write()</code></a> is called.</p> <p><code translate="no" dir="ltr">write</code> does not number checkpoints, increment <code translate="no" dir="ltr">save_counter</code>, or update the metadata used by <a href="latest_checkpoint"><code translate="no" dir="ltr">tf.train.latest_checkpoint</code></a>. It is primarily intended for use by higher level checkpoint management utilities. <code translate="no" dir="ltr">save</code> provides a very basic implementation of these features.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">file_prefix</code></b>: A prefix to use for the checkpoint filenames (/path/to/directory/and_a_prefix).</li> </ul> <h4 id="returns_4">Returns:</h4> <p>The full path to the checkpoint (i.e. <code translate="no" dir="ltr">file_prefix</code>).</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="checkpoint"><code translate="no" dir="ltr">tf.compat.v2.train.Checkpoint</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint</a>
  </p>
</div>
