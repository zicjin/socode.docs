<h1 class="devsite-page-title">tf.train.ExponentialMovingAverage</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.train.ExponentialMovingAverage"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="name"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply"> <meta itemprop="property" content="average"> <meta itemprop="property" content="average_name"> <meta itemprop="property" content="variables_to_restore"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/train/ExponentialMovingAverage">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L260-L550">  View source on GitHub </a> </td>
</table>  <h2 id="class_exponentialmovingaverage">Class <code translate="no" dir="ltr">ExponentialMovingAverage</code>
</h2> <p>Maintains moving averages of variables by employing an exponential decay.</p>  <p>When training a model, it is often beneficial to maintain moving averages of the trained parameters. Evaluations that use averaged parameters sometimes produce significantly better results than the final trained values.</p> <p>The <code translate="no" dir="ltr">apply()</code> method adds shadow copies of trained variables and add ops that maintain a moving average of the trained variables in their shadow copies. It is used when building the training model. The ops that maintain moving averages are typically run after each training step. The <code translate="no" dir="ltr">average()</code> and <code translate="no" dir="ltr">average_name()</code> methods give access to the shadow variables and their names. They are useful when building an evaluation model, or when restoring a model from a checkpoint file. They help use the moving averages in place of the last trained values for evaluations.</p> <p>The moving averages are computed using exponential decay. You specify the decay value when creating the <code translate="no" dir="ltr">ExponentialMovingAverage</code> object. The shadow variables are initialized with the same initial values as the trained variables. When you run the ops to maintain the moving averages, each shadow variable is updated with the formula:</p> <p><code translate="no" dir="ltr">shadow_variable -= (1 - decay) * (shadow_variable - variable)</code></p> <p>This is mathematically equivalent to the classic formula below, but the use of an <code translate="no" dir="ltr">assign_sub</code> op (the <code translate="no" dir="ltr">"-="</code> in the formula) allows concurrent lockless updates to the variables:</p> <p><code translate="no" dir="ltr">shadow_variable = decay * shadow_variable + (1 - decay) * variable</code></p> <p>Reasonable values for <code translate="no" dir="ltr">decay</code> are close to 1.0, typically in the multiple-nines range: 0.999, 0.9999, etc.</p> <p>Example usage when creating a training model:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...
...
# Create an op that applies the optimizer.  This is what we usually
# would use as a training op.
opt_op = opt.minimize(my_loss, [var0, var1])

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

with tf.control_dependencies([opt_op]):
    # Create the shadow variables, and add ops to maintain moving averages
    # of var0 and var1. This also creates an op that will update the moving
    # averages after each training step.  This is what we will use in place
    # of the usual training op.
    training_op = ema.apply([var0, var1])

...train the model by running training_op...
</pre> <p>There are two ways to use the moving averages for evaluations:</p> <ul> <li>Build a model that uses the shadow variables instead of the variables. For this, use the <code translate="no" dir="ltr">average()</code> method which returns the shadow variable for a given variable.</li> <li>Build a model normally but load the checkpoint files to evaluate by using the shadow variable names. For this use the <code translate="no" dir="ltr">average_name()</code> method. See the <a href="../compat/v1/train/saver"><code translate="no" dir="ltr">tf.compat.v1.train.Saver</code></a> for more information on restoring saved variables.</li> </ul> <p>Example of restoring the shadow variable values:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Create a Saver that loads variables from their saved shadow values.
shadow_var0_name = ema.average_name(var0)
shadow_var1_name = ema.average_name(var1)
saver = tf.compat.v1.train.Saver({shadow_var0_name: var0, shadow_var1_name:
var1})
saver.restore(...checkpoint filename...)
# var0 and var1 now hold the moving average values
</pre> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L341-L371">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
</pre> <p>Creates a new ExponentialMovingAverage object.</p> <p>The <code translate="no" dir="ltr">apply()</code> method has to be called to create shadow variables and add ops to maintain moving averages.</p> <p>The optional <code translate="no" dir="ltr">num_updates</code> parameter allows one to tweak the decay rate dynamically. It is typical to pass the count of training steps, usually kept in a variable that is incremented at each step, in which case the decay rate is lower at the start of training. This makes moving averages move faster. If passed, the actual decay rate used is:</p> <p><code translate="no" dir="ltr">min(decay, (1 + num_updates) / (10 + num_updates))</code></p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">decay</code></b>: Float. The decay to use.</li> <li>
<b><code translate="no" dir="ltr">num_updates</code></b>: Optional count of number of updates applied to variables.</li> <li>
<b><code translate="no" dir="ltr">zero_debias</code></b>: If <code translate="no" dir="ltr">True</code>, zero debias moving-averages that are initialized with tensors.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: String. Optional prefix name to use for the name of ops added in <code translate="no" dir="ltr">apply()</code>.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="name"><code translate="no" dir="ltr">name</code></h3> <p>The name of this ExponentialMovingAverage object.</p> <h2 id="methods">Methods</h2> <h3 id="apply"><code translate="no" dir="ltr">apply</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L378-L462">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply(var_list=None)
</pre> <p>Maintains moving averages of variables.</p> <p><code translate="no" dir="ltr">var_list</code> must be a list of <code translate="no" dir="ltr">Variable</code> or <code translate="no" dir="ltr">Tensor</code> objects. This method creates shadow variables for all elements of <code translate="no" dir="ltr">var_list</code>. Shadow variables for <code translate="no" dir="ltr">Variable</code> objects are initialized to the variable's initial value. They will be added to the <code translate="no" dir="ltr">GraphKeys.MOVING_AVERAGE_VARIABLES</code> collection. For <code translate="no" dir="ltr">Tensor</code> objects, the shadow variables are initialized to 0 and zero debiased (see docstring in <code translate="no" dir="ltr">assign_moving_average</code> for more details).</p> <p>shadow variables are created with <code translate="no" dir="ltr">trainable=False</code> and added to the <code translate="no" dir="ltr">GraphKeys.ALL_VARIABLES</code> collection. They will be returned by calls to <a href="../compat/v1/global_variables"><code translate="no" dir="ltr">tf.compat.v1.global_variables()</code></a>.</p> <p>Returns an op that updates all shadow variables from the current value of their associated variables.</p> <p>Note that <code translate="no" dir="ltr">apply()</code> can be called multiple times. When eager execution is enabled each call to apply will update the variables once, so this needs to be called in a loop.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: A list of Variable or Tensor objects. The variables and Tensors must be of types bfloat16, float16, float32, or float64.</li> </ul> <h4 id="returns">Returns:</h4> <p>An Operation that updates the moving averages.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If the arguments are not an allowed type.</li> </ul> <h3 id="average"><code translate="no" dir="ltr">average</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L464-L474">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">average(var)
</pre> <p>Returns the <code translate="no" dir="ltr">Variable</code> holding the average of <code translate="no" dir="ltr">var</code>.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">var</code></b>: A <code translate="no" dir="ltr">Variable</code> object.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A <code translate="no" dir="ltr">Variable</code> object or <code translate="no" dir="ltr">None</code> if the moving average of <code translate="no" dir="ltr">var</code> is not maintained.</p> <h3 id="average_name"><code translate="no" dir="ltr">average_name</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L476-L501">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">average_name(var)
</pre> <p>Returns the name of the <code translate="no" dir="ltr">Variable</code> holding the average for <code translate="no" dir="ltr">var</code>.</p> <p>The typical scenario for <code translate="no" dir="ltr">ExponentialMovingAverage</code> is to compute moving averages of variables during training, and restore the variables from the computed moving averages during evaluations.</p> <p>To restore variables, you have to know the name of the shadow variables. That name and the original variable can then be passed to a <code translate="no" dir="ltr">Saver()</code> object to restore the variable from the moving average value with: <code translate="no" dir="ltr">saver = tf.compat.v1.train.Saver({ema.average_name(var): var})</code></p> <p><code translate="no" dir="ltr">average_name()</code> can be called whether or not <code translate="no" dir="ltr">apply()</code> has been called.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">var</code></b>: A <code translate="no" dir="ltr">Variable</code> object.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>A string: The name of the variable that will be used or was used by the <code translate="no" dir="ltr">ExponentialMovingAverage class</code> to hold the moving average of <code translate="no" dir="ltr">var</code>.</p> <h3 id="variables_to_restore"><code translate="no" dir="ltr">variables_to_restore</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/moving_averages.py#L503-L550">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables_to_restore(moving_avg_variables=None)
</pre> <p>Returns a map of names to <code translate="no" dir="ltr">Variables</code> to restore.</p> <p>If a variable has a moving average, use the moving average variable name as the restore name; otherwise, use the variable name.</p> <p>For example,</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables_to_restore = ema.variables_to_restore()
saver = tf.compat.v1.train.Saver(variables_to_restore)
</pre> <p>Below is an example of such mapping:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">conv/batchnorm/gamma/ExponentialMovingAverage: conv/batchnorm/gamma,
conv_4/conv2d_params/ExponentialMovingAverage: conv_4/conv2d_params,
global_step: global_step
</pre> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">moving_avg_variables</code></b>: a list of variables that require to use of the moving average variable name to be restored. If None, it will default to variables.moving_average_variables() + variables.trainable_variables()</li> </ul> <h4 id="returns_4">Returns:</h4> <p>A map from restore_names to variables. The restore_name is either the original or the moving average version of the variable name, depending on whether the variable name is in the <code translate="no" dir="ltr">moving_avg_variables</code>.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="exponentialmovingaverage"><code translate="no" dir="ltr">tf.compat.v1.train.ExponentialMovingAverage</code></a></li> <li><a href="exponentialmovingaverage"><code translate="no" dir="ltr">tf.compat.v2.train.ExponentialMovingAverage</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage</a>
  </p>
</div>
