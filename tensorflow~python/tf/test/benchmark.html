<h1 class="devsite-page-title">tf.test.Benchmark</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.test.Benchmark"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="evaluate"> <meta itemprop="property" content="is_abstract"> <meta itemprop="property" content="report_benchmark"> <meta itemprop="property" content="run_op_benchmark"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/test/Benchmark">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L287-L405">  View source on GitHub </a> </td>
</table>  <h2 id="class_benchmark">Class <code translate="no" dir="ltr">Benchmark</code>
</h2> <p>Abstract class that provides helpers for TensorFlow benchmarks.</p>  <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L290-L294">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__()
</pre> <p>Initialize self. See help(type(self)) for accurate signature.</p> <h2 id="methods">Methods</h2> <h3 id="evaluate"><code translate="no" dir="ltr">evaluate</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L395-L405">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">evaluate(tensors)
</pre> <p>Evaluates tensors and returns numpy values.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">tensors</code></b>: A Tensor or a nested list/tuple of Tensors.</li> </ul> <h4 id="returns">Returns:</h4> <p>tensors numpy values.</p> <h3 id="is_abstract"><code translate="no" dir="ltr">is_abstract</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L296-L300">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@classmethod
is_abstract(cls)
</pre> <h3 id="report_benchmark"><code translate="no" dir="ltr">report_benchmark</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L241-L270">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">report_benchmark(
    iters=None,
    cpu_time=None,
    wall_time=None,
    throughput=None,
    extras=None,
    name=None,
    metrics=None
)
</pre> <p>Report a benchmark.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">iters</code></b>: (optional) How many iterations were run</li> <li>
<b><code translate="no" dir="ltr">cpu_time</code></b>: (optional) Median or mean cpu time in seconds.</li> <li>
<b><code translate="no" dir="ltr">wall_time</code></b>: (optional) Median or mean wall time in seconds.</li> <li>
<b><code translate="no" dir="ltr">throughput</code></b>: (optional) Throughput (in MB/s)</li> <li>
<b><code translate="no" dir="ltr">extras</code></b>: (optional) Dict mapping string keys to additional benchmark info. Values may be either floats or values that are convertible to strings.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: (optional) Override the BenchmarkEntry name with <code translate="no" dir="ltr">name</code>. Otherwise it is inferred from the top-level method name.</li> <li>
<b><code translate="no" dir="ltr">metrics</code></b>: (optional) A list of dict, where each dict has the keys below name (required), string, metric name value (required), double, metric value min_value (optional), double, minimum acceptable metric value max_value (optional), double, maximum acceptable metric value</li> </ul> <h3 id="run_op_benchmark"><code translate="no" dir="ltr">run_op_benchmark</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/platform/benchmark.py#L302-L393">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">run_op_benchmark(
    sess,
    op_or_tensor,
    feed_dict=None,
    burn_iters=2,
    min_iters=10,
    store_trace=False,
    store_memory_usage=True,
    name=None,
    extras=None,
    mbs=0
)
</pre> <p>Run an op or tensor in the given session. Report the results.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">sess</code></b>: <code translate="no" dir="ltr">Session</code> object to use for timing.</li> <li>
<b><code translate="no" dir="ltr">op_or_tensor</code></b>: <code translate="no" dir="ltr">Operation</code> or <code translate="no" dir="ltr">Tensor</code> to benchmark.</li> <li>
<b><code translate="no" dir="ltr">feed_dict</code></b>: A <code translate="no" dir="ltr">dict</code> of values to feed for each op iteration (see the <code translate="no" dir="ltr">feed_dict</code> parameter of <code translate="no" dir="ltr">Session.run</code>).</li> <li>
<b><code translate="no" dir="ltr">burn_iters</code></b>: Number of burn-in iterations to run.</li> <li>
<b><code translate="no" dir="ltr">min_iters</code></b>: Minimum number of iterations to use for timing.</li> <li>
<b><code translate="no" dir="ltr">store_trace</code></b>: Boolean, whether to run an extra untimed iteration and store the trace of iteration in returned extras. The trace will be stored as a string in Google Chrome trace format in the extras field "full_trace_chrome_format". Note that trace will not be stored in test_log_pb2.TestResults proto.</li> <li>
<b><code translate="no" dir="ltr">store_memory_usage</code></b>: Boolean, whether to run an extra untimed iteration, calculate memory usage, and store that in extras fields.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: (optional) Override the BenchmarkEntry name with <code translate="no" dir="ltr">name</code>. Otherwise it is inferred from the top-level method name.</li> <li>
<b><code translate="no" dir="ltr">extras</code></b>: (optional) Dict mapping string keys to additional benchmark info. Values may be either floats or values that are convertible to strings.</li> <li>
<b><code translate="no" dir="ltr">mbs</code></b>: (optional) The number of megabytes moved by this op, used to calculate the ops throughput.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A <code translate="no" dir="ltr">dict</code> containing the key-value pairs that were passed to <code translate="no" dir="ltr">report_benchmark</code>. If <code translate="no" dir="ltr">store_trace</code> option is used, then <code translate="no" dir="ltr">full_chrome_trace_format</code> will be included in return dictionary even though it is not passed to <code translate="no" dir="ltr">report_benchmark</code> with <code translate="no" dir="ltr">extras</code>.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="benchmark"><code translate="no" dir="ltr">tf.compat.v1.test.Benchmark</code></a></li> <li><a href="benchmark"><code translate="no" dir="ltr">tf.compat.v2.test.Benchmark</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/test/Benchmark" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/test/Benchmark</a>
  </p>
</div>
