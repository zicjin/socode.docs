<h1 class="devsite-page-title">tf.py_function</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.py_function"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/py_function">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/script_ops.py#L348-L428">  View source on GitHub </a> </td>
</table>  <p>Wraps a python function into a TensorFlow op that executes it eagerly.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.py_function(
    func,
    inp,
    Tout,
    name=None
)
</pre> <h3 id="used_in_the_guide">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/data_performance">Better performance with the tf.data API</a></li> <li><a href="https://www.tensorflow.org/guide/data">tf.data: Build TensorFlow input pipelines</a></li> </ul> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/customization/performance">Better performance with tf.function</a></li> <li><a href="https://www.tensorflow.org/tutorials/load_data/text">Load text</a></li> <li><a href="https://www.tensorflow.org/tutorials/load_data/tfrecord">TFRecord and tf.Example</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding</a></li> </ul> <p>This function allows expressing computations in a TensorFlow graph as Python functions. In particular, it wraps a Python function <code translate="no" dir="ltr">func</code> in a once-differentiable TensorFlow operation that executes it with eager execution enabled. As a consequence, <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> makes it possible to express control flow using Python constructs (<code translate="no" dir="ltr">if</code>, <code translate="no" dir="ltr">while</code>, <code translate="no" dir="ltr">for</code>, etc.), instead of TensorFlow control flow constructs (<a href="cond"><code translate="no" dir="ltr">tf.cond</code></a>, <a href="while_loop"><code translate="no" dir="ltr">tf.while_loop</code></a>). For example, you might use <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> to implement the log huber function:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def log_huber(x, m):
  if tf.abs(x) &lt;= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.compat.v1.placeholder(tf.float32)
m = tf.compat.v1.placeholder(tf.float32)

y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)
dy_dx = tf.gradients(y, x)[0]

with tf.compat.v1.Session() as sess:
  # The session executes `log_huber` eagerly. Given the feed values below,
  # it will take the first branch, so `y` evaluates to 1.0 and
  # `dy_dx` evaluates to 2.0.
  y, dy_dx = sess.run([y, dy_dx], feed_dict={x: 1.0, m: 2.0})
</pre> <p>You can also use <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> to debug your models at runtime using Python tools, i.e., you can isolate portions of your code that you want to debug, wrap them in Python functions and insert <code translate="no" dir="ltr">pdb</code> tracepoints or print statements as desired, and wrap those functions in <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a>.</p> <p>For more information on eager execution, see the <a href="https://tensorflow.org/guide/eager">Eager guide</a>.</p> <p><a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> is similar in spirit to <a href="compat/v1/py_func"><code translate="no" dir="ltr">tf.compat.v1.py_func</code></a>, but unlike the latter, the former lets you use TensorFlow operations in the wrapped Python function. In particular, while <a href="compat/v1/py_func"><code translate="no" dir="ltr">tf.compat.v1.py_func</code></a> only runs on CPUs and wraps functions that take NumPy arrays as inputs and return NumPy arrays as outputs, <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> can be placed on GPUs and wraps functions that take Tensors as inputs, execute TensorFlow operations in their bodies, and return Tensors as outputs.</p> <p>Like <a href="compat/v1/py_func"><code translate="no" dir="ltr">tf.compat.v1.py_func</code></a>, <a href="py_function"><code translate="no" dir="ltr">tf.py_function</code></a> has the following limitations with respect to serialization and distribution:</p> <ul> <li><p>The body of the function (i.e. <code translate="no" dir="ltr">func</code>) will not be serialized in a <code translate="no" dir="ltr">GraphDef</code>. Therefore, you should not use this function if you need to serialize your model and restore it in a different environment.</p></li> <li><p>The operation must run in the same address space as the Python program that calls <a href="py_function"><code translate="no" dir="ltr">tf.py_function()</code></a>. If you are using distributed TensorFlow, you must run a <a href="distribute/server"><code translate="no" dir="ltr">tf.distribute.Server</code></a> in the same process as the program that calls <a href="py_function"><code translate="no" dir="ltr">tf.py_function()</code></a> and you must pin the created operation to a device in that server (e.g. using <code translate="no" dir="ltr">with tf.device():</code>).</p></li> </ul> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">func</code></b>: A Python function which accepts a list of <code translate="no" dir="ltr">Tensor</code> objects having element types that match the corresponding <a href="tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> objects in <code translate="no" dir="ltr">inp</code> and returns a list of <code translate="no" dir="ltr">Tensor</code> objects (or a single <code translate="no" dir="ltr">Tensor</code>, or <code translate="no" dir="ltr">None</code>) having element types that match the corresponding values in <code translate="no" dir="ltr">Tout</code>.</li> <li>
<b><code translate="no" dir="ltr">inp</code></b>: A list of <code translate="no" dir="ltr">Tensor</code> objects.</li> <li>
<b><code translate="no" dir="ltr">Tout</code></b>: A list or tuple of tensorflow data types or a single tensorflow data type if there is only one, indicating what <code translate="no" dir="ltr">func</code> returns; an empty list if no value is returned (i.e., if the return value is <code translate="no" dir="ltr">None</code>).</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns">Returns:</h4> <p>A list of <code translate="no" dir="ltr">Tensor</code> or a single <code translate="no" dir="ltr">Tensor</code> which <code translate="no" dir="ltr">func</code> computes; an empty list if <code translate="no" dir="ltr">func</code> returns None.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="py_function"><code translate="no" dir="ltr">tf.compat.v1.py_function</code></a></li> <li><a href="py_function"><code translate="no" dir="ltr">tf.compat.v2.py_function</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/py_function" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/py_function</a>
  </p>
</div>
