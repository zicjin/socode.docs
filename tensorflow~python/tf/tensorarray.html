<h1 class="devsite-page-title">tf.TensorArray</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.TensorArray"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="dynamic_size"> <meta itemprop="property" content="element_shape"> <meta itemprop="property" content="flow"> <meta itemprop="property" content="handle"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="close"> <meta itemprop="property" content="concat"> <meta itemprop="property" content="gather"> <meta itemprop="property" content="grad"> <meta itemprop="property" content="identity"> <meta itemprop="property" content="read"> <meta itemprop="property" content="scatter"> <meta itemprop="property" content="size"> <meta itemprop="property" content="split"> <meta itemprop="property" content="stack"> <meta itemprop="property" content="unstack"> <meta itemprop="property" content="write"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/TensorArray">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L945-L1267">  View source on GitHub </a> </td>
</table>  <h2 id="class_tensorarray">Class <code translate="no" dir="ltr">TensorArray</code>
</h2> <p>Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <h3 id="used_in_the_guide">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/function">Better performance with tf.function and AutoGraph</a></li> </ul> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/customization/performance">Better performance with tf.function</a></li> </ul> <p>This class is meant to be used with dynamic iteration primitives such as <code translate="no" dir="ltr">while_loop</code> and <code translate="no" dir="ltr">map_fn</code>. It supports gradient back-propagation via special "flow" control flow dependencies.</p> <p>Example 1: plain reading and writing.</p> <blockquote>   <p>ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False) ta = ta.write(0, 10) ta = ta.write(1, 20) ta = ta.write(2, 30)</p> <p>ta.read(0) <tf.tensor: shape="()," dtype="float32," numpy="10.0"> ta.read(1) <tf.tensor: shape="()," dtype="float32," numpy="20.0"> ta.read(2) <tf.tensor: shape="()," dtype="float32," numpy="30.0"> ta.stack() <tf.tensor: shape="(3,)," dtype="float32," numpy="array([10.,"></tf.tensor:></tf.tensor:></tf.tensor:></tf.tensor:></p>   </blockquote> <p>Example 2: Fibonacci sequence algorithm that writes in a loop then returns.</p> <blockquote>   <p>@tf.function ... def fibonacci(n): ... ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True) ... ta = ta.unstack([0., 1.]) ... ... for i in range(2, n): ... ta = ta.write(i, ta.read(i - 1) + ta.read(i - 2)) ... ... return ta.stack()</p> <p>fibonacci(7) <tf.tensor: shape="(7,)," dtype="float32," numpy="array([0.,"></tf.tensor:></p>   </blockquote> <p>Example 3: A simple loop interacting with a tf.Variable.</p> <blockquote>   <p>v = tf.Variable(1)</p> <p>@tf.function ... def f(x): ... ta = tf.TensorArray(tf.int32, size=0, dynamic_size=True) ... ... for i in tf.range(x): ... v.assign_add(i) ... ta = ta.write(i, v) ... ... return ta.stack()</p> <p>f(5) <tf.tensor: shape="(5,)," dtype="int32," numpy="array(["></tf.tensor:></p>   </blockquote> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1001-L1080">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    dtype,
    size=None,
    dynamic_size=None,
    clear_after_read=None,
    tensor_array_name=None,
    handle=None,
    flow=None,
    infer_shape=True,
    element_shape=None,
    colocate_with_first_write_call=True,
    name=None
)
</pre> <p>Construct a new TensorArray or wrap an existing TensorArray handle.</p> <p>A note about the parameter <code translate="no" dir="ltr">name</code>:</p> <p>The name of the <code translate="no" dir="ltr">TensorArray</code> (even if passed in) is uniquified: each time a new <code translate="no" dir="ltr">TensorArray</code> is created at runtime it is assigned its own name for the duration of the run. This avoids name collisions if a <code translate="no" dir="ltr">TensorArray</code> is created within a <code translate="no" dir="ltr">while_loop</code>.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">dtype</code></b>: (required) data type of the TensorArray.</li> <li>
<b><code translate="no" dir="ltr">size</code></b>: (optional) int32 scalar <code translate="no" dir="ltr">Tensor</code>: the size of the TensorArray. Required if handle is not provided.</li> <li>
<b><code translate="no" dir="ltr">dynamic_size</code></b>: (optional) Python bool: If true, writes to the TensorArray can grow the TensorArray past its initial size. Default: False.</li> <li>
<b><code translate="no" dir="ltr">clear_after_read</code></b>: Boolean (optional, default: True). If True, clear TensorArray values after reading them. This disables read-many semantics, but allows early release of memory.</li> <li>
<b><code translate="no" dir="ltr">tensor_array_name</code></b>: (optional) Python string: the name of the TensorArray. This is used when creating the TensorArray handle. If this value is set, handle should be None.</li> <li>
<b><code translate="no" dir="ltr">handle</code></b>: (optional) A <code translate="no" dir="ltr">Tensor</code> handle to an existing TensorArray. If this is set, tensor_array_name should be None. Only supported in graph mode.</li> <li>
<b><code translate="no" dir="ltr">flow</code></b>: (optional) A float <code translate="no" dir="ltr">Tensor</code> scalar coming from an existing <a href="tensorarray#flow"><code translate="no" dir="ltr">TensorArray.flow</code></a>. Only supported in graph mode.</li> <li>
<b><code translate="no" dir="ltr">infer_shape</code></b>: (optional, default: True) If True, shape inference is enabled. In this case, all elements must have the same shape.</li> <li>
<b><code translate="no" dir="ltr">element_shape</code></b>: (optional, default: None) A <code translate="no" dir="ltr">TensorShape</code> object specifying the shape constraints of each of the elements of the TensorArray. Need not be fully defined.</li> <li>
<b><code translate="no" dir="ltr">colocate_with_first_write_call</code></b>: If <code translate="no" dir="ltr">True</code>, the TensorArray will be colocated on the same device as the Tensor used on its first write (write operations include <code translate="no" dir="ltr">write</code>, <code translate="no" dir="ltr">unstack</code>, and <code translate="no" dir="ltr">split</code>). If <code translate="no" dir="ltr">False</code>, the TensorArray will be placed on the device determined by the device context available during its initialization.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if both handle and tensor_array_name are provided.</li> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: if handle is provided but is not a Tensor.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="dtype"><code translate="no" dir="ltr">dtype</code></h3> <p>The data type of this TensorArray.</p> <h3 id="dynamic_size"><code translate="no" dir="ltr">dynamic_size</code></h3> <p>Python bool; if <code translate="no" dir="ltr">True</code> the TensorArray can grow dynamically.</p> <h3 id="element_shape"><code translate="no" dir="ltr">element_shape</code></h3> <p>The <a href="tensorshape"><code translate="no" dir="ltr">tf.TensorShape</code></a> of elements in this TensorArray.</p> <h3 id="flow"><code translate="no" dir="ltr">flow</code></h3> <p>The flow <code translate="no" dir="ltr">Tensor</code> forcing ops leading to this TensorArray state.</p> <h3 id="handle"><code translate="no" dir="ltr">handle</code></h3> <p>The reference to the TensorArray.</p> <h2 id="methods">Methods</h2> <h3 id="close"><code translate="no" dir="ltr">close</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1264-L1267">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">close(name=None)
</pre> <p>Close the current TensorArray.</p> <p><strong>NOTE</strong> The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.</p> <h3 id="concat"><code translate="no" dir="ltr">concat</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1188-L1200">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">concat(name=None)
</pre> <p>Return the values in the TensorArray as a concatenated <code translate="no" dir="ltr">Tensor</code>.</p> <p>All of the values must have been written, their ranks must match, and and their shapes must all match for all dimensions except the first.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns">Returns:</h4> <p>All the tensors in the TensorArray concatenated into one tensor.</p> <h3 id="gather"><code translate="no" dir="ltr">gather</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1171-L1186">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">gather(
    indices,
    name=None
)
</pre> <p>Return selected values in the TensorArray as a packed <code translate="no" dir="ltr">Tensor</code>.</p> <p>All of selected values must have been written and their shapes must all match.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">indices</code></b>: A <code translate="no" dir="ltr">1-D</code> <code translate="no" dir="ltr">Tensor</code> taking values in <code translate="no" dir="ltr">[0, max_value)</code>. If the <code translate="no" dir="ltr">TensorArray</code> is not dynamic, <code translate="no" dir="ltr">max_value=size()</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_2">Returns:</h4> <p>The tensors in the <code translate="no" dir="ltr">TensorArray</code> selected by <code translate="no" dir="ltr">indices</code>, packed into one tensor.</p> <h3 id="grad"><code translate="no" dir="ltr">grad</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1124-L1125">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">grad(
    source,
    flow=None,
    name=None
)
</pre> <h3 id="identity"><code translate="no" dir="ltr">identity</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1114-L1122">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">identity()
</pre> <p>Returns a TensorArray with the same content and properties.</p> <h4 id="returns_3">Returns:</h4> <p>A new TensorArray object with flow that ensures the control dependencies from the contexts will become control dependencies for writes, reads, etc. Use this object all for subsequent operations.</p> <h3 id="read"><code translate="no" dir="ltr">read</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1127-L1137">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">read(
    index,
    name=None
)
</pre> <p>Read the value at location <code translate="no" dir="ltr">index</code> in the TensorArray.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">index</code></b>: 0-D. int32 tensor with the index to read from.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_4">Returns:</h4> <p>The tensor at index <code translate="no" dir="ltr">index</code>.</p> <h3 id="scatter"><code translate="no" dir="ltr">scatter</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1222-L1239">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">scatter(
    indices,
    value,
    name=None
)
</pre> <p>Scatter the values of a <code translate="no" dir="ltr">Tensor</code> in specific indices of a <code translate="no" dir="ltr">TensorArray</code>.</p> <p>Args: indices: A <code translate="no" dir="ltr">1-D</code> <code translate="no" dir="ltr">Tensor</code> taking values in <code translate="no" dir="ltr">[0, max_value)</code>. If the <code translate="no" dir="ltr">TensorArray</code> is not dynamic, <code translate="no" dir="ltr">max_value=size()</code>. value: (N+1)-D. Tensor of type <code translate="no" dir="ltr">dtype</code>. The Tensor to unpack. name: A name for the operation (optional).</p> <p>Returns: A new TensorArray object with flow that ensures the scatter occurs. Use this object all for subsequent operations.</p> <p>Raises: ValueError: if the shape inference fails.</p> <p><strong>NOTE</strong> The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.</p> <h3 id="size"><code translate="no" dir="ltr">size</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1260-L1262">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">size(name=None)
</pre> <p>Return the size of the TensorArray.</p> <h3 id="split"><code translate="no" dir="ltr">split</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1241-L1258">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">split(
    value,
    lengths,
    name=None
)
</pre> <p>Split the values of a <code translate="no" dir="ltr">Tensor</code> into the TensorArray.</p> <p>Args: value: (N+1)-D. Tensor of type <code translate="no" dir="ltr">dtype</code>. The Tensor to split. lengths: 1-D. int32 vector with the lengths to use when splitting <code translate="no" dir="ltr">value</code> along its first dimension. name: A name for the operation (optional).</p> <p>Returns: A new TensorArray object with flow that ensures the split occurs. Use this object all for subsequent operations.</p> <p>Raises: ValueError: if the shape inference fails.</p> <p><strong>NOTE</strong> The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.</p> <h3 id="stack"><code translate="no" dir="ltr">stack</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1157-L1169">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">stack(name=None)
</pre> <p>Return the values in the TensorArray as a stacked <code translate="no" dir="ltr">Tensor</code>.</p> <p>All of the values must have been written and their shapes must all match. If input shapes have rank-<code translate="no" dir="ltr">R</code>, then output shape will have rank-<code translate="no" dir="ltr">(R+1)</code>.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">name</code></b>: A name for the operation (optional).</li> </ul> <h4 id="returns_5">Returns:</h4> <p>All the tensors in the TensorArray stacked into one tensor.</p> <h3 id="unstack"><code translate="no" dir="ltr">unstack</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1202-L1220">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">unstack(
    value,
    name=None
)
</pre> <p>Unstack the values of a <code translate="no" dir="ltr">Tensor</code> in the TensorArray.</p> <p>If input value shapes have rank-<code translate="no" dir="ltr">R</code>, then the output TensorArray will contain elements whose shapes are rank-<code translate="no" dir="ltr">(R-1)</code>.</p> <p>Args: value: (N+1)-D. Tensor of type <code translate="no" dir="ltr">dtype</code>. The Tensor to unstack. name: A name for the operation (optional).</p> <p>Returns: A new TensorArray object with flow that ensures the unstack occurs. Use this object all for subsequent operations.</p> <p>Raises: ValueError: if the shape inference fails.</p> <p><strong>NOTE</strong> The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.</p> <h3 id="write"><code translate="no" dir="ltr">write</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/tensor_array_ops.py#L1139-L1155">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">write(
    index,
    value,
    name=None
)
</pre> <p>Write <code translate="no" dir="ltr">value</code> into index <code translate="no" dir="ltr">index</code> of the TensorArray.</p> <p>Args: index: 0-D. int32 scalar with the index to write to. value: N-D. Tensor of type <code translate="no" dir="ltr">dtype</code>. The Tensor to write to this index. name: A name for the operation (optional).</p> <p>Returns: A new TensorArray object with flow that ensures the write occurs. Use this object all for subsequent operations.</p> <p>Raises: ValueError: if there are more writers than specified.</p> <p><strong>NOTE</strong> The output of this function should be used. If it is not, a warning will be logged or an error may be raised. To mark the output as used, call its .mark_used() method.</p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="tensorarray"><code translate="no" dir="ltr">tf.compat.v1.TensorArray</code></a></li> <li><a href="tensorarray"><code translate="no" dir="ltr">tf.compat.v2.TensorArray</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/TensorArray" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/TensorArray</a>
  </p>
</div>
