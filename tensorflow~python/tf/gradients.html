<h1 class="devsite-page-title">tf.gradients</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.gradients"> <meta itemprop="path" content="Stable"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/gradients">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/gradients_impl.py#L162-L274">  View source on GitHub </a> </td>
</table>  <p>Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
</pre>  <p><code translate="no" dir="ltr">ys</code> and <code translate="no" dir="ltr">xs</code> are each a <code translate="no" dir="ltr">Tensor</code> or a list of tensors. <code translate="no" dir="ltr">grad_ys</code> is a list of <code translate="no" dir="ltr">Tensor</code>, holding the gradients received by the <code translate="no" dir="ltr">ys</code>. The list must be the same length as <code translate="no" dir="ltr">ys</code>.</p> <p><code translate="no" dir="ltr">gradients()</code> adds ops to the graph to output the derivatives of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">xs</code>. It returns a list of <code translate="no" dir="ltr">Tensor</code> of length <code translate="no" dir="ltr">len(xs)</code> where each tensor is the <code translate="no" dir="ltr">sum(dy/dx)</code> for y in <code translate="no" dir="ltr">ys</code>.</p> <p><code translate="no" dir="ltr">grad_ys</code> is a list of tensors of the same length as <code translate="no" dir="ltr">ys</code> that holds the initial gradients for each y in <code translate="no" dir="ltr">ys</code>. When <code translate="no" dir="ltr">grad_ys</code> is None, we fill in a tensor of '1's of the shape of y for each y in <code translate="no" dir="ltr">ys</code>. A user can provide their own initial <code translate="no" dir="ltr">grad_ys</code> to compute the derivatives using a different initial gradient for each y (e.g., if one wanted to weight the gradient differently for each value in each y).</p> <p><code translate="no" dir="ltr">stop_gradients</code> is a <code translate="no" dir="ltr">Tensor</code> or a list of tensors to be considered constant with respect to all <code translate="no" dir="ltr">xs</code>. These tensors will not be backpropagated through, as though they had been explicitly disconnected using <code translate="no" dir="ltr">stop_gradient</code>. Among other things, this allows computation of partial derivatives as opposed to total derivatives. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.constant(0.)
b = 2 * a
g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])
</pre> <p>Here the partial derivatives <code translate="no" dir="ltr">g</code> evaluate to <code translate="no" dir="ltr">[1.0, 1.0]</code>, compared to the total derivatives <code translate="no" dir="ltr">tf.gradients(a + b, [a, b])</code>, which take into account the influence of <code translate="no" dir="ltr">a</code> on <code translate="no" dir="ltr">b</code> and evaluate to <code translate="no" dir="ltr">[3.0, 1.0]</code>. Note that the above is equivalent to:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.stop_gradient(tf.constant(0.))
b = tf.stop_gradient(2 * a)
g = tf.gradients(a + b, [a, b])
</pre> <p><code translate="no" dir="ltr">stop_gradients</code> provides a way of stopping gradient after the graph has already been constructed, as compared to <a href="stop_gradient"><code translate="no" dir="ltr">tf.stop_gradient</code></a> which is used during graph construction. When the two approaches are combined, backpropagation stops at both <a href="stop_gradient"><code translate="no" dir="ltr">tf.stop_gradient</code></a> nodes and nodes in <code translate="no" dir="ltr">stop_gradients</code>, whichever is encountered first.</p> <p>All integer tensors are considered constant with respect to all <code translate="no" dir="ltr">xs</code>, as if they were included in <code translate="no" dir="ltr">stop_gradients</code>.</p> <p><code translate="no" dir="ltr">unconnected_gradients</code> determines the value returned for each x in xs if it is unconnected in the graph to ys. By default this is None to safeguard against errors. Mathematically these gradients are zero which can be requested using the <code translate="no" dir="ltr">'zero'</code> option. <code translate="no" dir="ltr">tf.UnconnectedGradients</code> provides the following options and behaviors:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">a = tf.ones([1, 2])
b = tf.ones([3, 1])
g1 = tf.gradients([b], [a], unnconnected_gradients='none')
sess.run(g1)  # [None]

g2 = tf.gradients([b], [a], unconnected_gradients='zero')
sess.run(g2)  # [array([[0., 0.]], dtype=float32)]
</pre> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ys</code></b>: A <code translate="no" dir="ltr">Tensor</code> or list of tensors to be differentiated.</li> <li>
<b><code translate="no" dir="ltr">xs</code></b>: A <code translate="no" dir="ltr">Tensor</code> or list of tensors to be used for differentiation.</li> <li>
<b><code translate="no" dir="ltr">grad_ys</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> or list of tensors the same size as <code translate="no" dir="ltr">ys</code> and holding the gradients computed for each y in <code translate="no" dir="ltr">ys</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name to use for grouping all the gradient ops together. defaults to 'gradients'.</li> <li>
<b><code translate="no" dir="ltr">gate_gradients</code></b>: If True, add a tuple around the gradients returned for an operations. This avoids some race conditions.</li> <li>
<b><code translate="no" dir="ltr">aggregation_method</code></b>: Specifies the method used to combine gradient terms. Accepted values are constants defined in the class <code translate="no" dir="ltr">AggregationMethod</code>.</li> <li>
<b><code translate="no" dir="ltr">stop_gradients</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> or list of tensors not to differentiate through.</li> <li>
<b><code translate="no" dir="ltr">unconnected_gradients</code></b>: Optional. Specifies the gradient value returned when the given input tensors are unconnected. Accepted values are constants defined in the class <a href="unconnectedgradients"><code translate="no" dir="ltr">tf.UnconnectedGradients</code></a> and the default value is <code translate="no" dir="ltr">none</code>.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A list of <code translate="no" dir="ltr">sum(dy/dx)</code> for each x in <code translate="no" dir="ltr">xs</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">LookupError</code></b>: if one of the operations between <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> does not have a registered gradient function.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: if the arguments are invalid.</li> <li>
<b><code translate="no" dir="ltr">RuntimeError</code></b>: if called in Eager mode.</li> </ul> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="gradients"><code translate="no" dir="ltr">tf.compat.v2.gradients</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/gradients" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/gradients</a>
  </p>
</div>
