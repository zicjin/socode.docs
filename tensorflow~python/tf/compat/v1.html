<h1 class="devsite-page-title">Module: tf.compat.v1</h1>    <devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>  <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="AUTO_REUSE"> <meta itemprop="property" content="COMPILER_VERSION"> <meta itemprop="property" content="CXX11_ABI_FLAG"> <meta itemprop="property" content="GIT_VERSION"> <meta itemprop="property" content="GRAPH_DEF_VERSION"> <meta itemprop="property" content="GRAPH_DEF_VERSION_MIN_CONSUMER"> <meta itemprop="property" content="GRAPH_DEF_VERSION_MIN_PRODUCER"> <meta itemprop="property" content="MONOLITHIC_BUILD"> <meta itemprop="property" content="QUANTIZED_DTYPES"> <meta itemprop="property" content="VERSION"> <meta itemprop="property" content="__version__"> <meta itemprop="property" content="bfloat16"> <meta itemprop="property" content="bool"> <meta itemprop="property" content="complex128"> <meta itemprop="property" content="complex64"> <meta itemprop="property" content="double"> <meta itemprop="property" content="float16"> <meta itemprop="property" content="float32"> <meta itemprop="property" content="float64"> <meta itemprop="property" content="half"> <meta itemprop="property" content="int16"> <meta itemprop="property" content="int32"> <meta itemprop="property" content="int64"> <meta itemprop="property" content="int8"> <meta itemprop="property" content="qint16"> <meta itemprop="property" content="qint32"> <meta itemprop="property" content="qint8"> <meta itemprop="property" content="quint16"> <meta itemprop="property" content="quint8"> <meta itemprop="property" content="resource"> <meta itemprop="property" content="string"> <meta itemprop="property" content="uint16"> <meta itemprop="property" content="uint32"> <meta itemprop="property" content="uint64"> <meta itemprop="property" content="uint8"> <meta itemprop="property" content="variant"> </div> <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/compat/v1">  TensorFlow 1 version</a> </td> </table> <p>Bring in all of the public TensorFlow interface into this module.</p> <h2 id="modules">Modules</h2> <p><a href="v1/app"><code translate="no" dir="ltr">app</code></a> module: Generic entry point script.</p> <p><a href="v1/audio"><code translate="no" dir="ltr">audio</code></a> module: Public API for tf.audio namespace.</p> <p><a href="v1/autograph"><code translate="no" dir="ltr">autograph</code></a> module: Conversion of plain Python into TensorFlow graph code.</p> <p><a href="v1/bitwise"><code translate="no" dir="ltr">bitwise</code></a> module: Operations for manipulating the binary representations of integers.</p> <p><a href="v1/compat"><code translate="no" dir="ltr">compat</code></a> module: Compatibility functions.</p> <p><a href="v1/config"><code translate="no" dir="ltr">config</code></a> module: Public API for tf.config namespace.</p> <p><a href="v1/data"><code translate="no" dir="ltr">data</code></a> module: <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> API for input pipelines.</p> <p><a href="v1/debugging"><code translate="no" dir="ltr">debugging</code></a> module: Public API for tf.debugging namespace.</p> <p><a href="v1/distribute"><code translate="no" dir="ltr">distribute</code></a> module: Library for running a computation across multiple devices.</p> <p><a href="v1/distributions"><code translate="no" dir="ltr">distributions</code></a> module: Core module for TensorFlow distribution objects and helpers.</p> <p><a href="v1/dtypes"><code translate="no" dir="ltr">dtypes</code></a> module: Public API for tf.dtypes namespace.</p> <p><a href="v1/errors"><code translate="no" dir="ltr">errors</code></a> module: Exception types for TensorFlow errors.</p> <p><a href="v1/estimator"><code translate="no" dir="ltr">estimator</code></a> module: Estimator: High level tools for working with models.</p> <p><a href="v1/experimental"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.experimental namespace.</p> <p><a href="v1/feature_column"><code translate="no" dir="ltr">feature_column</code></a> module: Public API for tf.feature_column namespace.</p> <p><a href="v1/flags"><code translate="no" dir="ltr">flags</code></a> module: Import router for absl.flags. See https://github.com/abseil/abseil-py.</p> <p><a href="v1/gfile"><code translate="no" dir="ltr">gfile</code></a> module: Import router for file_io.</p> <p><a href="v1/graph_util"><code translate="no" dir="ltr">graph_util</code></a> module: Helpers to manipulate a tensor graph in python.</p> <p><a href="v1/image"><code translate="no" dir="ltr">image</code></a> module: Image processing and decoding ops.</p> <p><a href="v1/initializers"><code translate="no" dir="ltr">initializers</code></a> module: Public API for tf.initializers namespace.</p> <p><a href="v1/io"><code translate="no" dir="ltr">io</code></a> module: Public API for tf.io namespace.</p> <p><a href="v1/keras"><code translate="no" dir="ltr">keras</code></a> module: Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p><a href="v1/layers"><code translate="no" dir="ltr">layers</code></a> module: Public API for tf.layers namespace.</p> <p><a href="v1/linalg"><code translate="no" dir="ltr">linalg</code></a> module: Operations for linear algebra.</p> <p><a href="v1/lite"><code translate="no" dir="ltr">lite</code></a> module: Public API for tf.lite namespace.</p> <p><a href="v1/logging"><code translate="no" dir="ltr">logging</code></a> module: Logging and Summary Operations.</p> <p><a href="v1/lookup"><code translate="no" dir="ltr">lookup</code></a> module: Public API for tf.lookup namespace.</p> <p><a href="v1/losses"><code translate="no" dir="ltr">losses</code></a> module: Loss operations for use in neural networks.</p> <p><a href="v1/manip"><code translate="no" dir="ltr">manip</code></a> module: Operators for manipulating tensors.</p> <p><a href="v1/math"><code translate="no" dir="ltr">math</code></a> module: Math Operations.</p> <p><a href="v1/metrics"><code translate="no" dir="ltr">metrics</code></a> module: Evaluation-related metrics.</p> <p><a href="v1/mixed_precision"><code translate="no" dir="ltr">mixed_precision</code></a> module: Public API for tf.mixed_precision namespace.</p> <p><a href="v1/mlir"><code translate="no" dir="ltr">mlir</code></a> module: Public API for tf.mlir namespace.</p> <p><a href="v1/nest"><code translate="no" dir="ltr">nest</code></a> module: Public API for tf.nest namespace.</p> <p><a href="v1/nn"><code translate="no" dir="ltr">nn</code></a> module: Wrappers for primitive Neural Net (NN) Operations.</p> <p><a href="v1/profiler"><code translate="no" dir="ltr">profiler</code></a> module: Public API for tf.profiler namespace.</p> <p><a href="v1/python_io"><code translate="no" dir="ltr">python_io</code></a> module: Python functions for directly manipulating TFRecord-formatted files.</p> <p><a href="v1/quantization"><code translate="no" dir="ltr">quantization</code></a> module: Public API for tf.quantization namespace.</p> <p><a href="v1/queue"><code translate="no" dir="ltr">queue</code></a> module: Public API for tf.queue namespace.</p> <p><a href="v1/ragged"><code translate="no" dir="ltr">ragged</code></a> module: Ragged Tensors.</p> <p><a href="v1/random"><code translate="no" dir="ltr">random</code></a> module: Public API for tf.random namespace.</p> <p><a href="v1/raw_ops"><code translate="no" dir="ltr">raw_ops</code></a> module: Public API for tf.raw_ops namespace.</p> <p><a href="v1/resource_loader"><code translate="no" dir="ltr">resource_loader</code></a> module: Resource management library.</p> <p><a href="v1/saved_model"><code translate="no" dir="ltr">saved_model</code></a> module: Public API for tf.saved_model namespace.</p> <p><a href="v1/sets"><code translate="no" dir="ltr">sets</code></a> module: Tensorflow set operations.</p> <p><a href="v1/signal"><code translate="no" dir="ltr">signal</code></a> module: Signal processing operations.</p> <p><a href="v1/sparse"><code translate="no" dir="ltr">sparse</code></a> module: Sparse Tensor Representation.</p> <p><a href="v1/spectral"><code translate="no" dir="ltr">spectral</code></a> module: Public API for tf.spectral namespace.</p> <p><a href="v1/strings"><code translate="no" dir="ltr">strings</code></a> module: Operations for working with string Tensors.</p> <p><a href="v1/summary"><code translate="no" dir="ltr">summary</code></a> module: Operations for writing summary data, for use in analysis and visualization.</p> <p><a href="v1/sysconfig"><code translate="no" dir="ltr">sysconfig</code></a> module: System configuration library.</p> <p><a href="v1/test"><code translate="no" dir="ltr">test</code></a> module: Testing.</p> <p><a href="v1/tpu"><code translate="no" dir="ltr">tpu</code></a> module: Ops related to Tensor Processing Units.</p> <p><a href="v1/train"><code translate="no" dir="ltr">train</code></a> module: Support for training models.</p> <p><a href="v1/user_ops"><code translate="no" dir="ltr">user_ops</code></a> module: Public API for tf.user_ops namespace.</p> <p><a href="v1/version"><code translate="no" dir="ltr">version</code></a> module: Public API for tf.version namespace.</p> <p><a href="v1/xla"><code translate="no" dir="ltr">xla</code></a> module: Public API for tf.xla namespace.</p> <h2 id="classes">Classes</h2> <p><a href="../aggregationmethod"><code translate="no" dir="ltr">class AggregationMethod</code></a>: A class listing aggregation methods used to combine gradients.</p> <p><a href="v1/attrvalue"><code translate="no" dir="ltr">class AttrValue</code></a>: A ProtocolMessage</p> <p><a href="v1/conditionalaccumulator"><code translate="no" dir="ltr">class ConditionalAccumulator</code></a>: A conditional accumulator for aggregating gradients.</p> <p><a href="v1/conditionalaccumulatorbase"><code translate="no" dir="ltr">class ConditionalAccumulatorBase</code></a>: A conditional accumulator for aggregating gradients.</p> <p><a href="v1/configproto"><code translate="no" dir="ltr">class ConfigProto</code></a>: A ProtocolMessage</p> <p><a href="../criticalsection"><code translate="no" dir="ltr">class CriticalSection</code></a>: Critical section.</p> <p><a href="../dtypes/dtype"><code translate="no" dir="ltr">class DType</code></a>: Represents the type of the elements in a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="v1/devicespec"><code translate="no" dir="ltr">class DeviceSpec</code></a>: Represents a (possibly partial) specification for a TensorFlow device.</p> <p><a href="v1/dimension"><code translate="no" dir="ltr">class Dimension</code></a>: Represents the value of one dimension in a TensorShape.</p> <p><a href="v1/event"><code translate="no" dir="ltr">class Event</code></a>: A ProtocolMessage</p> <p><a href="../queue/fifoqueue"><code translate="no" dir="ltr">class FIFOQueue</code></a>: A queue implementation that dequeues elements in first-in first-out order.</p> <p><a href="../io/fixedlenfeature"><code translate="no" dir="ltr">class FixedLenFeature</code></a>: Configuration for parsing a fixed-length input feature.</p> <p><a href="../io/fixedlensequencefeature"><code translate="no" dir="ltr">class FixedLenSequenceFeature</code></a>: Configuration for parsing a variable-length input feature into a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="v1/fixedlengthrecordreader"><code translate="no" dir="ltr">class FixedLengthRecordReader</code></a>: A Reader that outputs fixed-length records from a file.</p> <p><a href="v1/gpuoptions"><code translate="no" dir="ltr">class GPUOptions</code></a>: A ProtocolMessage</p> <p><a href="../gradienttape"><code translate="no" dir="ltr">class GradientTape</code></a>: Record operations for automatic differentiation.</p> <p><a href="../graph"><code translate="no" dir="ltr">class Graph</code></a>: A TensorFlow computation, represented as a dataflow graph.</p> <p><a href="v1/graphdef"><code translate="no" dir="ltr">class GraphDef</code></a>: A ProtocolMessage</p> <p><a href="v1/graphkeys"><code translate="no" dir="ltr">class GraphKeys</code></a>: Standard names to use for graph collections.</p> <p><a href="v1/graphoptions"><code translate="no" dir="ltr">class GraphOptions</code></a>: A ProtocolMessage</p> <p><a href="v1/histogramproto"><code translate="no" dir="ltr">class HistogramProto</code></a>: A ProtocolMessage</p> <p><a href="v1/identityreader"><code translate="no" dir="ltr">class IdentityReader</code></a>: A Reader that outputs the queued work as both the key and value.</p> <p><a href="../indexedslices"><code translate="no" dir="ltr">class IndexedSlices</code></a>: A sparse representation of a set of tensor slices at given indices.</p> <p><a href="../indexedslicesspec"><code translate="no" dir="ltr">class IndexedSlicesSpec</code></a>: Type specification for a <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p><a href="v1/interactivesession"><code translate="no" dir="ltr">class InteractiveSession</code></a>: A TensorFlow <code translate="no" dir="ltr">Session</code> for use in interactive contexts, such as a shell.</p> <p><a href="v1/lmdbreader"><code translate="no" dir="ltr">class LMDBReader</code></a>: A Reader that outputs the records from a LMDB file.</p> <p><a href="v1/logmessage"><code translate="no" dir="ltr">class LogMessage</code></a>: A ProtocolMessage</p> <p><a href="v1/metagraphdef"><code translate="no" dir="ltr">class MetaGraphDef</code></a>: A ProtocolMessage</p> <p><a href="../module"><code translate="no" dir="ltr">class Module</code></a>: Base neural network module class.</p> <p><a href="v1/nameattrlist"><code translate="no" dir="ltr">class NameAttrList</code></a>: A ProtocolMessage</p> <p><a href="v1/nodedef"><code translate="no" dir="ltr">class NodeDef</code></a>: A ProtocolMessage</p> <p><a href="../errors/operror"><code translate="no" dir="ltr">class OpError</code></a>: A generic error that is raised when TensorFlow execution fails.</p> <p><a href="../operation"><code translate="no" dir="ltr">class Operation</code></a>: Represents a graph node that performs computation on tensors.</p> <p><a href="v1/optimizeroptions"><code translate="no" dir="ltr">class OptimizerOptions</code></a>: A ProtocolMessage</p> <p><a href="../optionalspec"><code translate="no" dir="ltr">class OptionalSpec</code></a>: Represents an optional potentially containing a structured value.</p> <p><a href="../queue/paddingfifoqueue"><code translate="no" dir="ltr">class PaddingFIFOQueue</code></a>: A FIFOQueue that supports batching variable-sized tensors by padding.</p> <p><a href="../queue/priorityqueue"><code translate="no" dir="ltr">class PriorityQueue</code></a>: A queue implementation that dequeues elements in prioritized order.</p> <p><a href="../queue/queuebase"><code translate="no" dir="ltr">class QueueBase</code></a>: Base class for queue implementations.</p> <p><a href="../raggedtensor"><code translate="no" dir="ltr">class RaggedTensor</code></a>: Represents a ragged tensor.</p> <p><a href="../raggedtensorspec"><code translate="no" dir="ltr">class RaggedTensorSpec</code></a>: Type specification for a <a href="../raggedtensor"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>.</p> <p><a href="../queue/randomshufflequeue"><code translate="no" dir="ltr">class RandomShuffleQueue</code></a>: A queue implementation that dequeues elements in a random order.</p> <p><a href="v1/readerbase"><code translate="no" dir="ltr">class ReaderBase</code></a>: Base class for different Reader types, that produce a record every step.</p> <p><a href="../registergradient"><code translate="no" dir="ltr">class RegisterGradient</code></a>: A decorator for registering the gradient function for an op type.</p> <p><a href="v1/runmetadata"><code translate="no" dir="ltr">class RunMetadata</code></a>: A ProtocolMessage</p> <p><a href="v1/runoptions"><code translate="no" dir="ltr">class RunOptions</code></a>: A ProtocolMessage</p> <p><a href="v1/session"><code translate="no" dir="ltr">class Session</code></a>: A class for running TensorFlow operations.</p> <p><a href="v1/sessionlog"><code translate="no" dir="ltr">class SessionLog</code></a>: A ProtocolMessage</p> <p><a href="v1/sparseconditionalaccumulator"><code translate="no" dir="ltr">class SparseConditionalAccumulator</code></a>: A conditional accumulator for aggregating sparse gradients.</p> <p><a href="../io/sparsefeature"><code translate="no" dir="ltr">class SparseFeature</code></a>: Configuration for parsing a sparse input feature from an <code translate="no" dir="ltr">Example</code>.</p> <p><a href="../sparse/sparsetensor"><code translate="no" dir="ltr">class SparseTensor</code></a>: Represents a sparse tensor.</p> <p><a href="../sparsetensorspec"><code translate="no" dir="ltr">class SparseTensorSpec</code></a>: Type specification for a <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.SparseTensor</code></a>.</p> <p><a href="v1/sparsetensorvalue"><code translate="no" dir="ltr">class SparseTensorValue</code></a>: SparseTensorValue(indices, values, dense_shape)</p> <p><a href="v1/summary"><code translate="no" dir="ltr">class Summary</code></a>: A ProtocolMessage</p> <p><a href="v1/summarymetadata"><code translate="no" dir="ltr">class SummaryMetadata</code></a>: A ProtocolMessage</p> <p><a href="v1/tfrecordreader"><code translate="no" dir="ltr">class TFRecordReader</code></a>: A Reader that outputs the records from a TFRecords file.</p> <p><a href="../tensor"><code translate="no" dir="ltr">class Tensor</code></a>: Represents one of the outputs of an <code translate="no" dir="ltr">Operation</code>.</p> <p><a href="../tensorarray"><code translate="no" dir="ltr">class TensorArray</code></a>: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p><a href="../tensorarrayspec"><code translate="no" dir="ltr">class TensorArraySpec</code></a>: Type specification for a <a href="../tensorarray"><code translate="no" dir="ltr">tf.TensorArray</code></a>.</p> <p><a href="v1/tensorinfo"><code translate="no" dir="ltr">class TensorInfo</code></a>: A ProtocolMessage</p> <p><a href="../tensorshape"><code translate="no" dir="ltr">class TensorShape</code></a>: Represents the shape of a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../tensorspec"><code translate="no" dir="ltr">class TensorSpec</code></a>: Describes a tf.Tensor.</p> <p><a href="v1/textlinereader"><code translate="no" dir="ltr">class TextLineReader</code></a>: A Reader that outputs the lines of a file delimited by newlines.</p> <p><a href="../typespec"><code translate="no" dir="ltr">class TypeSpec</code></a>: Specifies a TensorFlow value type.</p> <p><a href="../unconnectedgradients"><code translate="no" dir="ltr">class UnconnectedGradients</code></a>: Controls how gradient computation behaves when y does not depend on x.</p> <p><a href="../io/varlenfeature"><code translate="no" dir="ltr">class VarLenFeature</code></a>: Configuration for parsing a variable-length input feature.</p> <p><a href="v1/variable"><code translate="no" dir="ltr">class Variable</code></a>: See the <a href="https://tensorflow.org/guide/variables">Variables Guide</a>.</p> <p><a href="v1/variableaggregation"><code translate="no" dir="ltr">class VariableAggregation</code></a>: Indicates how a distributed variable will be aggregated.</p> <p><a href="v1/variablescope"><code translate="no" dir="ltr">class VariableScope</code></a>: Variable scope object to carry defaults to provide to <code translate="no" dir="ltr">get_variable</code>.</p> <p><a href="../variablesynchronization"><code translate="no" dir="ltr">class VariableSynchronization</code></a>: Indicates when a distributed variable will be synced.</p> <p><a href="v1/wholefilereader"><code translate="no" dir="ltr">class WholeFileReader</code></a>: A Reader that outputs the entire contents of a file as a value.</p> <p><a href="v1/keras/initializers/constant"><code translate="no" dir="ltr">class constant_initializer</code></a>: Initializer that generates tensors with constant values.</p> <p><a href="v1/keras/initializers/glorot_normal"><code translate="no" dir="ltr">class glorot_normal_initializer</code></a>: The Glorot normal initializer, also called Xavier normal initializer.</p> <p><a href="v1/keras/initializers/glorot_uniform"><code translate="no" dir="ltr">class glorot_uniform_initializer</code></a>: The Glorot uniform initializer, also called Xavier uniform initializer.</p> <p><a href="v1/keras/backend/name_scope"><code translate="no" dir="ltr">class name_scope</code></a>: A context manager for use when defining a Python op.</p> <p><a href="v1/keras/initializers/ones"><code translate="no" dir="ltr">class ones_initializer</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href="v1/keras/initializers/orthogonal"><code translate="no" dir="ltr">class orthogonal_initializer</code></a>: Initializer that generates an orthogonal matrix.</p> <p><a href="v1/random_normal_initializer"><code translate="no" dir="ltr">class random_normal_initializer</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href="v1/random_uniform_initializer"><code translate="no" dir="ltr">class random_uniform_initializer</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href="v1/truncated_normal_initializer"><code translate="no" dir="ltr">class truncated_normal_initializer</code></a>: Initializer that generates a truncated normal distribution.</p> <p><a href="v1/uniform_unit_scaling_initializer"><code translate="no" dir="ltr">class uniform_unit_scaling_initializer</code></a>: Initializer that generates tensors without scaling variance.</p> <p><a href="v1/variable_scope"><code translate="no" dir="ltr">class variable_scope</code></a>: A context manager for defining ops that creates variables (layers).</p> <p><a href="v1/keras/initializers/variancescaling"><code translate="no" dir="ltr">class variance_scaling_initializer</code></a>: Initializer capable of adapting its scale to the shape of weights tensors.</p> <p><a href="v1/keras/initializers/zeros"><code translate="no" dir="ltr">class zeros_initializer</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id="functions">Functions</h2> <p><a href="../debugging/assert"><code translate="no" dir="ltr">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href="../no_gradient"><code translate="no" dir="ltr">NoGradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="../no_gradient"><code translate="no" dir="ltr">NotDifferentiable(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="v1/print"><code translate="no" dir="ltr">Print(...)</code></a>: Prints a list of tensors. (deprecated)</p> <p><a href="../math/abs"><code translate="no" dir="ltr">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href="../math/accumulate_n"><code translate="no" dir="ltr">accumulate_n(...)</code></a>: Returns the element-wise sum of a list of tensors.</p> <p><a href="../math/acos"><code translate="no" dir="ltr">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href="../math/acosh"><code translate="no" dir="ltr">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href="../math/add"><code translate="no" dir="ltr">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href="v1/add_check_numerics_ops"><code translate="no" dir="ltr">add_check_numerics_ops(...)</code></a>: Connect a <a href="../debugging/check_numerics"><code translate="no" dir="ltr">tf.debugging.check_numerics</code></a> to every floating point tensor.</p> <p><a href="../math/add_n"><code translate="no" dir="ltr">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href="v1/add_to_collection"><code translate="no" dir="ltr">add_to_collection(...)</code></a>: Wrapper for <a href="../graph#add_to_collection"><code translate="no" dir="ltr">Graph.add_to_collection()</code></a> using the default graph.</p> <p><a href="v1/add_to_collections"><code translate="no" dir="ltr">add_to_collections(...)</code></a>: Wrapper for <a href="../graph#add_to_collections"><code translate="no" dir="ltr">Graph.add_to_collections()</code></a> using the default graph.</p> <p><a href="v1/all_variables"><code translate="no" dir="ltr">all_variables(...)</code></a>: Use <a href="v1/global_variables"><code translate="no" dir="ltr">tf.compat.v1.global_variables</code></a> instead. (deprecated)</p> <p><a href="../math/angle"><code translate="no" dir="ltr">angle(...)</code></a>: Returns the element-wise argument of a complex (or real) tensor.</p> <p><a href="v1/arg_max"><code translate="no" dir="ltr">arg_max(...)</code></a>: Returns the index with the largest value across dimensions of a tensor.</p> <p><a href="v1/arg_min"><code translate="no" dir="ltr">arg_min(...)</code></a>: Returns the index with the smallest value across dimensions of a tensor.</p> <p><a href="v1/argmax"><code translate="no" dir="ltr">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor. (deprecated arguments)</p> <p><a href="v1/argmin"><code translate="no" dir="ltr">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor. (deprecated arguments)</p> <p><a href="../argsort"><code translate="no" dir="ltr">argsort(...)</code></a>: Returns the indices of a tensor that give its sorted order along an axis.</p> <p><a href="../dtypes/as_dtype"><code translate="no" dir="ltr">as_dtype(...)</code></a>: Converts the given <code translate="no" dir="ltr">type_value</code> to a <code translate="no" dir="ltr">DType</code>.</p> <p><a href="../strings/as_string"><code translate="no" dir="ltr">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href="../math/asin"><code translate="no" dir="ltr">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href="../math/asinh"><code translate="no" dir="ltr">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href="v1/assert_equal"><code translate="no" dir="ltr">assert_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x == y</code> holds element-wise.</p> <p><a href="v1/assert_greater"><code translate="no" dir="ltr">assert_greater(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; y</code> holds element-wise.</p> <p><a href="v1/assert_greater_equal"><code translate="no" dir="ltr">assert_greater_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt;= y</code> holds element-wise.</p> <p><a href="v1/assert_integer"><code translate="no" dir="ltr">assert_integer(...)</code></a>: Assert that <code translate="no" dir="ltr">x</code> is of integer dtype.</p> <p><a href="v1/assert_less"><code translate="no" dir="ltr">assert_less(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; y</code> holds element-wise.</p> <p><a href="v1/assert_less_equal"><code translate="no" dir="ltr">assert_less_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt;= y</code> holds element-wise.</p> <p><a href="v1/assert_near"><code translate="no" dir="ltr">assert_near(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> are close element-wise.</p> <p><a href="v1/assert_negative"><code translate="no" dir="ltr">assert_negative(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; 0</code> holds element-wise.</p> <p><a href="v1/assert_non_negative"><code translate="no" dir="ltr">assert_non_negative(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt;= 0</code> holds element-wise.</p> <p><a href="v1/assert_non_positive"><code translate="no" dir="ltr">assert_non_positive(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt;= 0</code> holds element-wise.</p> <p><a href="v1/assert_none_equal"><code translate="no" dir="ltr">assert_none_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x != y</code> holds element-wise.</p> <p><a href="v1/assert_positive"><code translate="no" dir="ltr">assert_positive(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; 0</code> holds element-wise.</p> <p><a href="../debugging/assert_proper_iterable"><code translate="no" dir="ltr">assert_proper_iterable(...)</code></a>: Static assert that values is a "proper" iterable.</p> <p><a href="v1/assert_rank"><code translate="no" dir="ltr">assert_rank(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code>.</p> <p><a href="v1/assert_rank_at_least"><code translate="no" dir="ltr">assert_rank_at_least(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code> or higher.</p> <p><a href="v1/assert_rank_in"><code translate="no" dir="ltr">assert_rank_in(...)</code></a>: Assert <code translate="no" dir="ltr">x</code> has rank in <code translate="no" dir="ltr">ranks</code>.</p> <p><a href="../debugging/assert_same_float_dtype"><code translate="no" dir="ltr">assert_same_float_dtype(...)</code></a>: Validate and return float type based on <code translate="no" dir="ltr">tensors</code> and <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="v1/assert_scalar"><code translate="no" dir="ltr">assert_scalar(...)</code></a>: Asserts that the given <code translate="no" dir="ltr">tensor</code> is a scalar (i.e. zero-dimensional).</p> <p><a href="v1/assert_type"><code translate="no" dir="ltr">assert_type(...)</code></a>: Statically asserts that the given <code translate="no" dir="ltr">Tensor</code> is of the specified type.</p> <p><a href="v1/assert_variables_initialized"><code translate="no" dir="ltr">assert_variables_initialized(...)</code></a>: Returns an Op to check if variables are initialized.</p> <p><a href="v1/assign"><code translate="no" dir="ltr">assign(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by assigning <code translate="no" dir="ltr">value</code> to it.</p> <p><a href="v1/assign_add"><code translate="no" dir="ltr">assign_add(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by adding <code translate="no" dir="ltr">value</code> to it.</p> <p><a href="v1/assign_sub"><code translate="no" dir="ltr">assign_sub(...)</code></a>: Update <code translate="no" dir="ltr">ref</code> by subtracting <code translate="no" dir="ltr">value</code> from it.</p> <p><a href="../math/atan"><code translate="no" dir="ltr">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href="../math/atan2"><code translate="no" dir="ltr">atan2(...)</code></a>: Computes arctangent of <code translate="no" dir="ltr">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href="../math/atanh"><code translate="no" dir="ltr">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href="v1/batch_gather"><code translate="no" dir="ltr">batch_gather(...)</code></a>: Gather slices from params according to indices with leading batch dims. (deprecated)</p> <p><a href="v1/batch_scatter_update"><code translate="no" dir="ltr">batch_scatter_update(...)</code></a>: Generalization of <a href="v1/scatter_update"><code translate="no" dir="ltr">tf.compat.v1.scatter_update</code></a> to axis different than 0. (deprecated)</p> <p><a href="v1/batch_to_space"><code translate="no" dir="ltr">batch_to_space(...)</code></a>: BatchToSpace for 4-D tensors of type T.</p> <p><a href="v1/batch_to_space_nd"><code translate="no" dir="ltr">batch_to_space_nd(...)</code></a>: BatchToSpace for N-D tensors of type T.</p> <p><a href="../math/betainc"><code translate="no" dir="ltr">betainc(...)</code></a>: Compute the regularized incomplete beta integral \(I_x(a, b)\).</p> <p><a href="v1/bincount"><code translate="no" dir="ltr">bincount(...)</code></a>: Counts the number of occurrences of each value in an integer array.</p> <p><a href="../bitcast"><code translate="no" dir="ltr">bitcast(...)</code></a>: Bitcasts a tensor from one type to another without copying data.</p> <p><a href="v1/boolean_mask"><code translate="no" dir="ltr">boolean_mask(...)</code></a>: Apply boolean mask to tensor.</p> <p><a href="../broadcast_dynamic_shape"><code translate="no" dir="ltr">broadcast_dynamic_shape(...)</code></a>: Computes the shape of a broadcast given symbolic shapes.</p> <p><a href="../broadcast_static_shape"><code translate="no" dir="ltr">broadcast_static_shape(...)</code></a>: Computes the shape of a broadcast given known shapes.</p> <p><a href="../broadcast_to"><code translate="no" dir="ltr">broadcast_to(...)</code></a>: Broadcast an array for a compatible shape.</p> <p><a href="v1/case"><code translate="no" dir="ltr">case(...)</code></a>: Create a case operation.</p> <p><a href="../cast"><code translate="no" dir="ltr">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href="../math/ceil"><code translate="no" dir="ltr">ceil(...)</code></a>: Returns element-wise smallest integer not less than x.</p> <p><a href="../debugging/check_numerics"><code translate="no" dir="ltr">check_numerics(...)</code></a>: Checks a tensor for NaN and Inf values.</p> <p><a href="../linalg/cholesky"><code translate="no" dir="ltr">cholesky(...)</code></a>: Computes the Cholesky decomposition of one or more square matrices.</p> <p><a href="../linalg/cholesky_solve"><code translate="no" dir="ltr">cholesky_solve(...)</code></a>: Solves systems of linear eqns <code translate="no" dir="ltr">A X = RHS</code>, given Cholesky factorizations.</p> <p><a href="v1/clip_by_average_norm"><code translate="no" dir="ltr">clip_by_average_norm(...)</code></a>: Clips tensor values to a maximum average L2-norm. (deprecated)</p> <p><a href="../clip_by_global_norm"><code translate="no" dir="ltr">clip_by_global_norm(...)</code></a>: Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p><a href="../clip_by_norm"><code translate="no" dir="ltr">clip_by_norm(...)</code></a>: Clips tensor values to a maximum L2-norm.</p> <p><a href="../clip_by_value"><code translate="no" dir="ltr">clip_by_value(...)</code></a>: Clips tensor values to a specified min and max.</p> <p><a href="v1/colocate_with"><code translate="no" dir="ltr">colocate_with(...)</code></a>: DEPRECATED FUNCTION</p> <p><a href="../dtypes/complex"><code translate="no" dir="ltr">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href="../concat"><code translate="no" dir="ltr">concat(...)</code></a>: Concatenates tensors along one dimension.</p> <p><a href="v1/cond"><code translate="no" dir="ltr">cond(...)</code></a>: Return <code translate="no" dir="ltr">true_fn()</code> if the predicate <code translate="no" dir="ltr">pred</code> is true else <code translate="no" dir="ltr">false_fn()</code>. (deprecated arguments)</p> <p><a href="v1/confusion_matrix"><code translate="no" dir="ltr">confusion_matrix(...)</code></a>: Computes the confusion matrix from predictions and labels.</p> <p><a href="../math/conj"><code translate="no" dir="ltr">conj(...)</code></a>: Returns the complex conjugate of a complex number.</p> <p><a href="v1/constant"><code translate="no" dir="ltr">constant(...)</code></a>: Creates a constant tensor.</p> <p><a href="v1/container"><code translate="no" dir="ltr">container(...)</code></a>: Wrapper for <a href="../graph#container"><code translate="no" dir="ltr">Graph.container()</code></a> using the default graph.</p> <p><a href="../control_dependencies"><code translate="no" dir="ltr">control_dependencies(...)</code></a>: Wrapper for <a href="../graph#control_dependencies"><code translate="no" dir="ltr">Graph.control_dependencies()</code></a> using the default graph.</p> <p><a href="v1/control_flow_v2_enabled"><code translate="no" dir="ltr">control_flow_v2_enabled(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if v2 control flow is enabled.</p> <p><a href="v1/convert_to_tensor"><code translate="no" dir="ltr">convert_to_tensor(...)</code></a>: Converts the given <code translate="no" dir="ltr">value</code> to a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="v1/convert_to_tensor_or_indexed_slices"><code translate="no" dir="ltr">convert_to_tensor_or_indexed_slices(...)</code></a>: Converts the given object to a <code translate="no" dir="ltr">Tensor</code> or an <code translate="no" dir="ltr">IndexedSlices</code>.</p> <p><a href="v1/convert_to_tensor_or_sparse_tensor"><code translate="no" dir="ltr">convert_to_tensor_or_sparse_tensor(...)</code></a>: Converts value to a <code translate="no" dir="ltr">SparseTensor</code> or <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../math/cos"><code translate="no" dir="ltr">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href="../math/cosh"><code translate="no" dir="ltr">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href="v1/count_nonzero"><code translate="no" dir="ltr">count_nonzero(...)</code></a>: Computes number of nonzero elements across dimensions of a tensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="v1/count_up_to"><code translate="no" dir="ltr">count_up_to(...)</code></a>: Increments 'ref' until it reaches 'limit'. (deprecated)</p> <p><a href="v1/create_partitioned_variables"><code translate="no" dir="ltr">create_partitioned_variables(...)</code></a>: Create a list of partitioned variables according to the given <code translate="no" dir="ltr">slicing</code>. (deprecated)</p> <p><a href="../linalg/cross"><code translate="no" dir="ltr">cross(...)</code></a>: Compute the pairwise cross product.</p> <p><a href="../math/cumprod"><code translate="no" dir="ltr">cumprod(...)</code></a>: Compute the cumulative product of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="../math/cumsum"><code translate="no" dir="ltr">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="../custom_gradient"><code translate="no" dir="ltr">custom_gradient(...)</code></a>: Decorator to define a function with a custom gradient.</p> <p><a href="../io/decode_base64"><code translate="no" dir="ltr">decode_base64(...)</code></a>: Decode web-safe base64-encoded strings.</p> <p><a href="../io/decode_compressed"><code translate="no" dir="ltr">decode_compressed(...)</code></a>: Decompress strings.</p> <p><a href="v1/decode_csv"><code translate="no" dir="ltr">decode_csv(...)</code></a>: Convert CSV records to tensors. Each column maps to one tensor.</p> <p><a href="../io/decode_json_example"><code translate="no" dir="ltr">decode_json_example(...)</code></a>: Convert JSON-encoded Example records to binary protocol buffer strings.</p> <p><a href="v1/decode_raw"><code translate="no" dir="ltr">decode_raw(...)</code></a>: Convert raw byte strings into tensors. (deprecated arguments)</p> <p><a href="v1/delete_session_tensor"><code translate="no" dir="ltr">delete_session_tensor(...)</code></a>: Delete the tensor for the given tensor handle.</p> <p><a href="v1/depth_to_space"><code translate="no" dir="ltr">depth_to_space(...)</code></a>: DepthToSpace for tensors of type T.</p> <p><a href="../quantization/dequantize"><code translate="no" dir="ltr">dequantize(...)</code></a>: Dequantize the 'input' tensor into a float Tensor.</p> <p><a href="../io/deserialize_many_sparse"><code translate="no" dir="ltr">deserialize_many_sparse(...)</code></a>: Deserialize and concatenate <code translate="no" dir="ltr">SparseTensors</code> from a serialized minibatch.</p> <p><a href="v1/device"><code translate="no" dir="ltr">device(...)</code></a>: Wrapper for <a href="../graph#device"><code translate="no" dir="ltr">Graph.device()</code></a> using the default graph.</p> <p><a href="../linalg/tensor_diag"><code translate="no" dir="ltr">diag(...)</code></a>: Returns a diagonal tensor with a given diagonal values.</p> <p><a href="../linalg/tensor_diag_part"><code translate="no" dir="ltr">diag_part(...)</code></a>: Returns the diagonal part of the tensor.</p> <p><a href="../math/digamma"><code translate="no" dir="ltr">digamma(...)</code></a>: Computes Psi, the derivative of Lgamma (the log of the absolute value of</p> <p><a href="dimension_at_index"><code translate="no" dir="ltr">dimension_at_index(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href="dimension_value"><code translate="no" dir="ltr">dimension_value(...)</code></a>: Compatibility utility required to allow for both V1 and V2 behavior in TF.</p> <p><a href="v1/disable_control_flow_v2"><code translate="no" dir="ltr">disable_control_flow_v2(...)</code></a>: Opts out of control flow v2.</p> <p><a href="v1/disable_eager_execution"><code translate="no" dir="ltr">disable_eager_execution(...)</code></a>: Disables eager execution.</p> <p><a href="v1/disable_resource_variables"><code translate="no" dir="ltr">disable_resource_variables(...)</code></a>: Opts out of resource variables. (deprecated)</p> <p><a href="v1/disable_tensor_equality"><code translate="no" dir="ltr">disable_tensor_equality(...)</code></a>: Compare Tensors by their id and be hashable.</p> <p><a href="v1/disable_v2_behavior"><code translate="no" dir="ltr">disable_v2_behavior(...)</code></a>: Disables TensorFlow 2.x behaviors.</p> <p><a href="v1/disable_v2_tensorshape"><code translate="no" dir="ltr">disable_v2_tensorshape(...)</code></a>: Disables the V2 TensorShape behavior and reverts to V1 behavior.</p> <p><a href="../raggedtensor#__div__"><code translate="no" dir="ltr">div(...)</code></a>: Divides x / y elementwise (using Python 2 division operator semantics). (deprecated)</p> <p><a href="../math/divide_no_nan"><code translate="no" dir="ltr">div_no_nan(...)</code></a>: Computes a safe divide which returns 0 if the y is zero.</p> <p><a href="../math/divide"><code translate="no" dir="ltr">divide(...)</code></a>: Computes Python style division of <code translate="no" dir="ltr">x</code> by <code translate="no" dir="ltr">y</code>.</p> <p><a href="../dynamic_partition"><code translate="no" dir="ltr">dynamic_partition(...)</code></a>: Partitions <code translate="no" dir="ltr">data</code> into <code translate="no" dir="ltr">num_partitions</code> tensors using indices from <code translate="no" dir="ltr">partitions</code>.</p> <p><a href="../dynamic_stitch"><code translate="no" dir="ltr">dynamic_stitch(...)</code></a>: Interleave the values from the <code translate="no" dir="ltr">data</code> tensors into a single tensor.</p> <p><a href="../edit_distance"><code translate="no" dir="ltr">edit_distance(...)</code></a>: Computes the Levenshtein distance between sequences.</p> <p><a href="../einsum"><code translate="no" dir="ltr">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href="v1/enable_control_flow_v2"><code translate="no" dir="ltr">enable_control_flow_v2(...)</code></a>: Use control flow v2.</p> <p><a href="v1/enable_eager_execution"><code translate="no" dir="ltr">enable_eager_execution(...)</code></a>: Enables eager execution for the lifetime of this program.</p> <p><a href="v1/enable_resource_variables"><code translate="no" dir="ltr">enable_resource_variables(...)</code></a>: Creates resource variables by default.</p> <p><a href="v1/enable_tensor_equality"><code translate="no" dir="ltr">enable_tensor_equality(...)</code></a>: Compare Tensors with element-wise comparison and thus be unhashable.</p> <p><a href="v1/enable_v2_behavior"><code translate="no" dir="ltr">enable_v2_behavior(...)</code></a>: Enables TensorFlow 2.x behaviors.</p> <p><a href="v1/enable_v2_tensorshape"><code translate="no" dir="ltr">enable_v2_tensorshape(...)</code></a>: In TensorFlow 2.0, iterating over a TensorShape instance returns values.</p> <p><a href="../io/encode_base64"><code translate="no" dir="ltr">encode_base64(...)</code></a>: Encode strings into web-safe base64 format.</p> <p><a href="../ensure_shape"><code translate="no" dir="ltr">ensure_shape(...)</code></a>: Updates the shape of a tensor and checks at runtime that the shape holds.</p> <p><a href="../math/equal"><code translate="no" dir="ltr">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href="../math/erf"><code translate="no" dir="ltr">erf(...)</code></a>: Computes the Gauss error function of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../math/erfc"><code translate="no" dir="ltr">erfc(...)</code></a>: Computes the complementary error function of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="v1/executing_eagerly"><code translate="no" dir="ltr">executing_eagerly(...)</code></a>: Checks whether the current thread has eager execution enabled.</p> <p><a href="../math/exp"><code translate="no" dir="ltr">exp(...)</code></a>: Computes exponential of x element-wise. \(y = e^x\).</p> <p><a href="v1/expand_dims"><code translate="no" dir="ltr">expand_dims(...)</code></a>: Inserts a dimension of 1 into a tensor's shape. (deprecated arguments)</p> <p><a href="../math/expm1"><code translate="no" dir="ltr">expm1(...)</code></a>: Computes <code translate="no" dir="ltr">exp(x) - 1</code> element-wise.</p> <p><a href="v1/extract_image_patches"><code translate="no" dir="ltr">extract_image_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">images</code> and put them in the "depth" output dimension.</p> <p><a href="../extract_volume_patches"><code translate="no" dir="ltr">extract_volume_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">input</code> and put them in the "depth" output dimension. 3D extension of <code translate="no" dir="ltr">extract_image_patches</code>.</p> <p><a href="../eye"><code translate="no" dir="ltr">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href="../quantization/fake_quant_with_min_max_args"><code translate="no" dir="ltr">fake_quant_with_min_max_args(...)</code></a>: Fake-quantize the 'inputs' tensor, type float to 'outputs' tensor of same type.</p> <p><a href="../quantization/fake_quant_with_min_max_args_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_args_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxArgs operation.</p> <p><a href="../quantization/fake_quant_with_min_max_vars"><code translate="no" dir="ltr">fake_quant_with_min_max_vars(...)</code></a>: Fake-quantize the 'inputs' tensor of type float via global float scalars <code translate="no" dir="ltr">min</code></p> <p><a href="../quantization/fake_quant_with_min_max_vars_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVars operation.</p> <p><a href="../quantization/fake_quant_with_min_max_vars_per_channel"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_per_channel(...)</code></a>: Fake-quantize the 'inputs' tensor of type float and one of the shapes: <code translate="no" dir="ltr">[d]</code>,</p> <p><a href="../quantization/fake_quant_with_min_max_vars_per_channel_gradient"><code translate="no" dir="ltr">fake_quant_with_min_max_vars_per_channel_gradient(...)</code></a>: Compute gradients for a FakeQuantWithMinMaxVarsPerChannel operation.</p> <p><a href="../signal/fft"><code translate="no" dir="ltr">fft(...)</code></a>: Fast Fourier transform.</p> <p><a href="../signal/fft2d"><code translate="no" dir="ltr">fft2d(...)</code></a>: 2D fast Fourier transform.</p> <p><a href="../signal/fft3d"><code translate="no" dir="ltr">fft3d(...)</code></a>: 3D fast Fourier transform.</p> <p><a href="../fill"><code translate="no" dir="ltr">fill(...)</code></a>: Creates a tensor filled with a scalar value.</p> <p><a href="../fingerprint"><code translate="no" dir="ltr">fingerprint(...)</code></a>: Generates fingerprint values.</p> <p><a href="v1/fixed_size_partitioner"><code translate="no" dir="ltr">fixed_size_partitioner(...)</code></a>: Partitioner to specify a fixed number of shards along given axis.</p> <p><a href="../math/floor"><code translate="no" dir="ltr">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href="v1/floor_div"><code translate="no" dir="ltr">floor_div(...)</code></a>: Returns x // y element-wise.</p> <p><a href="../math/floordiv"><code translate="no" dir="ltr">floordiv(...)</code></a>: Divides <code translate="no" dir="ltr">x / y</code> elementwise, rounding toward the most negative integer.</p> <p><a href="../math/floormod"><code translate="no" dir="ltr">floormod(...)</code></a>: Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p><a href="../foldl"><code translate="no" dir="ltr">foldl(...)</code></a>: foldl on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../foldr"><code translate="no" dir="ltr">foldr(...)</code></a>: foldr on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../function"><code translate="no" dir="ltr">function(...)</code></a>: Compiles a function into a callable TensorFlow graph.</p> <p><a href="v1/gather"><code translate="no" dir="ltr">gather(...)</code></a>: Gather slices from params axis <code translate="no" dir="ltr">axis</code> according to indices.</p> <p><a href="v1/gather_nd"><code translate="no" dir="ltr">gather_nd(...)</code></a>: Gather slices from <code translate="no" dir="ltr">params</code> into a Tensor with shape specified by <code translate="no" dir="ltr">indices</code>.</p> <p><a href="v1/get_collection"><code translate="no" dir="ltr">get_collection(...)</code></a>: Wrapper for <a href="../graph#get_collection"><code translate="no" dir="ltr">Graph.get_collection()</code></a> using the default graph.</p> <p><a href="v1/get_collection_ref"><code translate="no" dir="ltr">get_collection_ref(...)</code></a>: Wrapper for <a href="../graph#get_collection_ref"><code translate="no" dir="ltr">Graph.get_collection_ref()</code></a> using the default graph.</p> <p><a href="v1/get_default_graph"><code translate="no" dir="ltr">get_default_graph(...)</code></a>: Returns the default graph for the current thread.</p> <p><a href="v1/get_default_session"><code translate="no" dir="ltr">get_default_session(...)</code></a>: Returns the default session for the current thread.</p> <p><a href="v1/get_local_variable"><code translate="no" dir="ltr">get_local_variable(...)</code></a>: Gets an existing <em>local</em> variable or creates a new one.</p> <p><a href="../get_logger"><code translate="no" dir="ltr">get_logger(...)</code></a>: Return TF logger instance.</p> <p><a href="v1/get_seed"><code translate="no" dir="ltr">get_seed(...)</code></a>: Returns the local seeds an operation should use given an op-specific seed.</p> <p><a href="v1/get_session_handle"><code translate="no" dir="ltr">get_session_handle(...)</code></a>: Return the handle of <code translate="no" dir="ltr">data</code>.</p> <p><a href="v1/get_session_tensor"><code translate="no" dir="ltr">get_session_tensor(...)</code></a>: Get the tensor of type <code translate="no" dir="ltr">dtype</code> by feeding a tensor handle.</p> <p><a href="../get_static_value"><code translate="no" dir="ltr">get_static_value(...)</code></a>: Returns the constant value of the given tensor, if efficiently calculable.</p> <p><a href="v1/get_variable"><code translate="no" dir="ltr">get_variable(...)</code></a>: Gets an existing variable with these parameters or create a new one.</p> <p><a href="v1/get_variable_scope"><code translate="no" dir="ltr">get_variable_scope(...)</code></a>: Returns the current variable scope.</p> <p><a href="../linalg/global_norm"><code translate="no" dir="ltr">global_norm(...)</code></a>: Computes the global norm of multiple tensors.</p> <p><a href="v1/global_variables"><code translate="no" dir="ltr">global_variables(...)</code></a>: Returns global variables.</p> <p><a href="v1/global_variables_initializer"><code translate="no" dir="ltr">global_variables_initializer(...)</code></a>: Returns an Op that initializes global variables.</p> <p><a href="../grad_pass_through"><code translate="no" dir="ltr">grad_pass_through(...)</code></a>: Creates a grad-pass-through op with the forward behavior provided in f.</p> <p><a href="v1/gradients"><code translate="no" dir="ltr">gradients(...)</code></a>: Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="../math/greater"><code translate="no" dir="ltr">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href="../math/greater_equal"><code translate="no" dir="ltr">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href="../group"><code translate="no" dir="ltr">group(...)</code></a>: Create an op that groups multiple operations.</p> <p><a href="../guarantee_const"><code translate="no" dir="ltr">guarantee_const(...)</code></a>: Gives a guarantee to the TF runtime that the input tensor is a constant.</p> <p><a href="v1/hessians"><code translate="no" dir="ltr">hessians(...)</code></a>: Constructs the Hessian of sum of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">x</code> in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="../histogram_fixed_width"><code translate="no" dir="ltr">histogram_fixed_width(...)</code></a>: Return histogram of values.</p> <p><a href="../histogram_fixed_width_bins"><code translate="no" dir="ltr">histogram_fixed_width_bins(...)</code></a>: Bins the given values for use in a histogram.</p> <p><a href="../identity"><code translate="no" dir="ltr">identity(...)</code></a>: Return a tensor with the same shape and contents as input.</p> <p><a href="../identity_n"><code translate="no" dir="ltr">identity_n(...)</code></a>: Returns a list of tensors with the same shapes and contents as the input</p> <p><a href="../signal/ifft"><code translate="no" dir="ltr">ifft(...)</code></a>: Inverse fast Fourier transform.</p> <p><a href="../signal/ifft2d"><code translate="no" dir="ltr">ifft2d(...)</code></a>: Inverse 2D fast Fourier transform.</p> <p><a href="../signal/ifft3d"><code translate="no" dir="ltr">ifft3d(...)</code></a>: Inverse 3D fast Fourier transform.</p> <p><a href="../math/igamma"><code translate="no" dir="ltr">igamma(...)</code></a>: Compute the lower regularized incomplete Gamma function <code translate="no" dir="ltr">P(a, x)</code>.</p> <p><a href="../math/igammac"><code translate="no" dir="ltr">igammac(...)</code></a>: Compute the upper regularized incomplete Gamma function <code translate="no" dir="ltr">Q(a, x)</code>.</p> <p><a href="../math/imag"><code translate="no" dir="ltr">imag(...)</code></a>: Returns the imaginary part of a complex (or real) tensor.</p> <p><a href="../graph_util/import_graph_def"><code translate="no" dir="ltr">import_graph_def(...)</code></a>: Imports the graph from <code translate="no" dir="ltr">graph_def</code> into the current default <code translate="no" dir="ltr">Graph</code>. (deprecated arguments)</p> <p><a href="../init_scope"><code translate="no" dir="ltr">init_scope(...)</code></a>: A context manager that lifts ops out of control-flow scopes and function-building graphs.</p> <p><a href="v1/initialize_all_tables"><code translate="no" dir="ltr">initialize_all_tables(...)</code></a>: Returns an Op that initializes all tables of the default graph. (deprecated)</p> <p><a href="v1/initialize_all_variables"><code translate="no" dir="ltr">initialize_all_variables(...)</code></a>: See <a href="v1/global_variables_initializer"><code translate="no" dir="ltr">tf.compat.v1.global_variables_initializer</code></a>. (deprecated)</p> <p><a href="v1/initialize_local_variables"><code translate="no" dir="ltr">initialize_local_variables(...)</code></a>: See <a href="v1/local_variables_initializer"><code translate="no" dir="ltr">tf.compat.v1.local_variables_initializer</code></a>. (deprecated)</p> <p><a href="v1/initialize_variables"><code translate="no" dir="ltr">initialize_variables(...)</code></a>: See <a href="v1/variables_initializer"><code translate="no" dir="ltr">tf.compat.v1.variables_initializer</code></a>. (deprecated)</p> <p><a href="../math/invert_permutation"><code translate="no" dir="ltr">invert_permutation(...)</code></a>: Computes the inverse permutation of a tensor.</p> <p><a href="../math/is_finite"><code translate="no" dir="ltr">is_finite(...)</code></a>: Returns which elements of x are finite.</p> <p><a href="../math/is_inf"><code translate="no" dir="ltr">is_inf(...)</code></a>: Returns which elements of x are Inf.</p> <p><a href="../math/is_nan"><code translate="no" dir="ltr">is_nan(...)</code></a>: Returns which elements of x are NaN.</p> <p><a href="../math/is_non_decreasing"><code translate="no" dir="ltr">is_non_decreasing(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if <code translate="no" dir="ltr">x</code> is non-decreasing.</p> <p><a href="../debugging/is_numeric_tensor"><code translate="no" dir="ltr">is_numeric_tensor(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if the elements of <code translate="no" dir="ltr">tensor</code> are numbers.</p> <p><a href="../math/is_strictly_increasing"><code translate="no" dir="ltr">is_strictly_increasing(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if <code translate="no" dir="ltr">x</code> is strictly increasing.</p> <p><a href="../is_tensor"><code translate="no" dir="ltr">is_tensor(...)</code></a>: Checks whether <code translate="no" dir="ltr">x</code> is a tensor or "tensor-like".</p> <p><a href="v1/is_variable_initialized"><code translate="no" dir="ltr">is_variable_initialized(...)</code></a>: Tests if a variable has been initialized.</p> <p><a href="../math/lbeta"><code translate="no" dir="ltr">lbeta(...)</code></a>: Computes \(ln(|Beta(x)|)\), reducing along the last dimension.</p> <p><a href="../math/less"><code translate="no" dir="ltr">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href="../math/less_equal"><code translate="no" dir="ltr">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href="../math/lgamma"><code translate="no" dir="ltr">lgamma(...)</code></a>: Computes the log of the absolute value of <code translate="no" dir="ltr">Gamma(x)</code> element-wise.</p> <p><a href="../linspace"><code translate="no" dir="ltr">lin_space(...)</code></a>: Generates values in an interval.</p> <p><a href="../linspace"><code translate="no" dir="ltr">linspace(...)</code></a>: Generates values in an interval.</p> <p><a href="v1/load_file_system_library"><code translate="no" dir="ltr">load_file_system_library(...)</code></a>: Loads a TensorFlow plugin, containing file system implementation. (deprecated)</p> <p><a href="../load_library"><code translate="no" dir="ltr">load_library(...)</code></a>: Loads a TensorFlow plugin.</p> <p><a href="../load_op_library"><code translate="no" dir="ltr">load_op_library(...)</code></a>: Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p><a href="v1/local_variables"><code translate="no" dir="ltr">local_variables(...)</code></a>: Returns local variables.</p> <p><a href="v1/local_variables_initializer"><code translate="no" dir="ltr">local_variables_initializer(...)</code></a>: Returns an Op that initializes all local variables.</p> <p><a href="../math/log"><code translate="no" dir="ltr">log(...)</code></a>: Computes natural logarithm of x element-wise.</p> <p><a href="../math/log1p"><code translate="no" dir="ltr">log1p(...)</code></a>: Computes natural logarithm of (1 + x) element-wise.</p> <p><a href="../math/log_sigmoid"><code translate="no" dir="ltr">log_sigmoid(...)</code></a>: Computes log sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../math/logical_and"><code translate="no" dir="ltr">logical_and(...)</code></a>: Returns the truth value of x AND y element-wise.</p> <p><a href="../math/logical_not"><code translate="no" dir="ltr">logical_not(...)</code></a>: Returns the truth value of NOT x element-wise.</p> <p><a href="../math/logical_or"><code translate="no" dir="ltr">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href="../math/logical_xor"><code translate="no" dir="ltr">logical_xor(...)</code></a>: Logical XOR function.</p> <p><a href="../make_ndarray"><code translate="no" dir="ltr">make_ndarray(...)</code></a>: Create a numpy ndarray from a tensor.</p> <p><a href="v1/make_template"><code translate="no" dir="ltr">make_template(...)</code></a>: Given an arbitrary function, wrap it so that it does variable sharing.</p> <p><a href="../make_tensor_proto"><code translate="no" dir="ltr">make_tensor_proto(...)</code></a>: Create a TensorProto.</p> <p><a href="../map_fn"><code translate="no" dir="ltr">map_fn(...)</code></a>: map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../io/matching_files"><code translate="no" dir="ltr">matching_files(...)</code></a>: Returns the set of files matching one or more glob patterns.</p> <p><a href="../linalg/matmul"><code translate="no" dir="ltr">matmul(...)</code></a>: Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p><a href="../linalg/band_part"><code translate="no" dir="ltr">matrix_band_part(...)</code></a>: Copy a tensor setting everything outside a central band in each innermost matrix</p> <p><a href="../linalg/det"><code translate="no" dir="ltr">matrix_determinant(...)</code></a>: Computes the determinant of one or more square matrices.</p> <p><a href="../linalg/diag"><code translate="no" dir="ltr">matrix_diag(...)</code></a>: Returns a batched diagonal tensor with given batched diagonal values.</p> <p><a href="../linalg/diag_part"><code translate="no" dir="ltr">matrix_diag_part(...)</code></a>: Returns the batched diagonal part of a batched tensor.</p> <p><a href="../linalg/inv"><code translate="no" dir="ltr">matrix_inverse(...)</code></a>: Computes the inverse of one or more square invertible matrices or their</p> <p><a href="../linalg/set_diag"><code translate="no" dir="ltr">matrix_set_diag(...)</code></a>: Returns a batched matrix tensor with new batched diagonal values.</p> <p><a href="../linalg/solve"><code translate="no" dir="ltr">matrix_solve(...)</code></a>: Solves systems of linear equations.</p> <p><a href="../linalg/lstsq"><code translate="no" dir="ltr">matrix_solve_ls(...)</code></a>: Solves one or more linear least-squares problems.</p> <p><a href="../linalg/sqrtm"><code translate="no" dir="ltr">matrix_square_root(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href="../linalg/matrix_transpose"><code translate="no" dir="ltr">matrix_transpose(...)</code></a>: Transposes last two dimensions of tensor <code translate="no" dir="ltr">a</code>.</p> <p><a href="../linalg/triangular_solve"><code translate="no" dir="ltr">matrix_triangular_solve(...)</code></a>: Solves systems of linear equations with upper or lower triangular matrices by backsubstitution.</p> <p><a href="../math/maximum"><code translate="no" dir="ltr">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href="../meshgrid"><code translate="no" dir="ltr">meshgrid(...)</code></a>: Broadcasts parameters for evaluation on an N-D grid.</p> <p><a href="v1/min_max_variable_partitioner"><code translate="no" dir="ltr">min_max_variable_partitioner(...)</code></a>: Partitioner to allocate minimum size per slice.</p> <p><a href="../math/minimum"><code translate="no" dir="ltr">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href="../math/floormod"><code translate="no" dir="ltr">mod(...)</code></a>: Returns element-wise remainder of division. When <code translate="no" dir="ltr">x &lt; 0</code> xor <code translate="no" dir="ltr">y &lt; 0</code> is</p> <p><a href="v1/model_variables"><code translate="no" dir="ltr">model_variables(...)</code></a>: Returns all variables in the MODEL_VARIABLES collection.</p> <p><a href="v1/moving_average_variables"><code translate="no" dir="ltr">moving_average_variables(...)</code></a>: Returns all variables that maintain their moving averages.</p> <p><a href="v1/multinomial"><code translate="no" dir="ltr">multinomial(...)</code></a>: Draws samples from a multinomial distribution. (deprecated)</p> <p><a href="../math/multiply"><code translate="no" dir="ltr">multiply(...)</code></a>: Returns x * y element-wise.</p> <p><a href="../math/negative"><code translate="no" dir="ltr">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href="../no_gradient"><code translate="no" dir="ltr">no_gradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="../no_op"><code translate="no" dir="ltr">no_op(...)</code></a>: Does nothing. Only useful as a placeholder for control edges.</p> <p><a href="v1/no_regularizer"><code translate="no" dir="ltr">no_regularizer(...)</code></a>: Use this function to prevent regularization of variables.</p> <p><a href="../nondifferentiable_batch_function"><code translate="no" dir="ltr">nondifferentiable_batch_function(...)</code></a>: Batches the computation done by the decorated function.</p> <p><a href="v1/norm"><code translate="no" dir="ltr">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors. (deprecated arguments)</p> <p><a href="../math/not_equal"><code translate="no" dir="ltr">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href="../numpy_function"><code translate="no" dir="ltr">numpy_function(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="../one_hot"><code translate="no" dir="ltr">one_hot(...)</code></a>: Returns a one-hot tensor.</p> <p><a href="../ones"><code translate="no" dir="ltr">ones(...)</code></a>: Creates a tensor with all elements set to one (1).</p> <p><a href="v1/ones_like"><code translate="no" dir="ltr">ones_like(...)</code></a>: Creates a tensor with all elements set to 1.</p> <p><a href="v1/op_scope"><code translate="no" dir="ltr">op_scope(...)</code></a>: DEPRECATED. Same as name_scope above, just different argument order.</p> <p><a href="v1/pad"><code translate="no" dir="ltr">pad(...)</code></a>: Pads a tensor.</p> <p><a href="../parallel_stack"><code translate="no" dir="ltr">parallel_stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor in parallel.</p> <p><a href="v1/parse_example"><code translate="no" dir="ltr">parse_example(...)</code></a>: Parses <code translate="no" dir="ltr">Example</code> protos into a <code translate="no" dir="ltr">dict</code> of tensors.</p> <p><a href="v1/parse_single_example"><code translate="no" dir="ltr">parse_single_example(...)</code></a>: Parses a single <code translate="no" dir="ltr">Example</code> proto.</p> <p><a href="../io/parse_single_sequence_example"><code translate="no" dir="ltr">parse_single_sequence_example(...)</code></a>: Parses a single <code translate="no" dir="ltr">SequenceExample</code> proto.</p> <p><a href="../io/parse_tensor"><code translate="no" dir="ltr">parse_tensor(...)</code></a>: Transforms a serialized tensorflow.TensorProto proto into a Tensor.</p> <p><a href="v1/placeholder"><code translate="no" dir="ltr">placeholder(...)</code></a>: Inserts a placeholder for a tensor that will be always fed.</p> <p><a href="v1/placeholder_with_default"><code translate="no" dir="ltr">placeholder_with_default(...)</code></a>: A placeholder op that passes through <code translate="no" dir="ltr">input</code> when its output is not fed.</p> <p><a href="../math/polygamma"><code translate="no" dir="ltr">polygamma(...)</code></a>: Compute the polygamma function \(\psi^{(n)}(x)\).</p> <p><a href="../math/pow"><code translate="no" dir="ltr">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href="../print"><code translate="no" dir="ltr">print(...)</code></a>: Print the specified inputs.</p> <p><a href="v1/py_func"><code translate="no" dir="ltr">py_func(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="../py_function"><code translate="no" dir="ltr">py_function(...)</code></a>: Wraps a python function into a TensorFlow op that executes it eagerly.</p> <p><a href="../linalg/qr"><code translate="no" dir="ltr">qr(...)</code></a>: Computes the QR decompositions of one or more matrices.</p> <p><a href="../quantization/quantize"><code translate="no" dir="ltr">quantize(...)</code></a>: Quantize the 'input' tensor of type float to 'output' tensor of type 'T'.</p> <p><a href="v1/quantize_v2"><code translate="no" dir="ltr">quantize_v2(...)</code></a>: Please use <a href="../quantization/quantize"><code translate="no" dir="ltr">tf.quantization.quantize</code></a> instead.</p> <p><a href="../quantization/quantized_concat"><code translate="no" dir="ltr">quantized_concat(...)</code></a>: Concatenates quantized tensors along one dimension.</p> <p><a href="../image/random_crop"><code translate="no" dir="ltr">random_crop(...)</code></a>: Randomly crops a tensor to a given size.</p> <p><a href="../random/gamma"><code translate="no" dir="ltr">random_gamma(...)</code></a>: Draws <code translate="no" dir="ltr">shape</code> samples from each of the given Gamma distribution(s).</p> <p><a href="../random/normal"><code translate="no" dir="ltr">random_normal(...)</code></a>: Outputs random values from a normal distribution.</p> <p><a href="v1/random_poisson"><code translate="no" dir="ltr">random_poisson(...)</code></a>: Draws <code translate="no" dir="ltr">shape</code> samples from each of the given Poisson distribution(s).</p> <p><a href="../random/shuffle"><code translate="no" dir="ltr">random_shuffle(...)</code></a>: Randomly shuffles a tensor along its first dimension.</p> <p><a href="../random/uniform"><code translate="no" dir="ltr">random_uniform(...)</code></a>: Outputs random values from a uniform distribution.</p> <p><a href="../range"><code translate="no" dir="ltr">range(...)</code></a>: Creates a sequence of numbers.</p> <p><a href="../rank"><code translate="no" dir="ltr">rank(...)</code></a>: Returns the rank of a tensor.</p> <p><a href="../io/read_file"><code translate="no" dir="ltr">read_file(...)</code></a>: Reads and outputs the entire contents of the input filename.</p> <p><a href="../math/real"><code translate="no" dir="ltr">real(...)</code></a>: Returns the real part of a complex (or real) tensor.</p> <p><a href="../realdiv"><code translate="no" dir="ltr">realdiv(...)</code></a>: Returns x / y element-wise for real types.</p> <p><a href="../math/reciprocal"><code translate="no" dir="ltr">reciprocal(...)</code></a>: Computes the reciprocal of x element-wise.</p> <p><a href="../recompute_grad"><code translate="no" dir="ltr">recompute_grad(...)</code></a>: An eager-compatible version of recompute_grad.</p> <p><a href="v1/reduce_all"><code translate="no" dir="ltr">reduce_all(...)</code></a>: Computes the "logical and" of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="v1/reduce_any"><code translate="no" dir="ltr">reduce_any(...)</code></a>: Computes the "logical or" of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="v1/reduce_join"><code translate="no" dir="ltr">reduce_join(...)</code></a>: Joins all strings into a single string, or joins along an axis.</p> <p><a href="v1/reduce_logsumexp"><code translate="no" dir="ltr">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))). (deprecated arguments)</p> <p><a href="v1/reduce_max"><code translate="no" dir="ltr">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="v1/reduce_mean"><code translate="no" dir="ltr">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href="v1/reduce_min"><code translate="no" dir="ltr">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="v1/reduce_prod"><code translate="no" dir="ltr">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="v1/reduce_sum"><code translate="no" dir="ltr">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor. (deprecated arguments)</p> <p><a href="../strings/regex_replace"><code translate="no" dir="ltr">regex_replace(...)</code></a>: Replace elements of <code translate="no" dir="ltr">input</code> matching regex <code translate="no" dir="ltr">pattern</code> with <code translate="no" dir="ltr">rewrite</code>.</p> <p><a href="../register_tensor_conversion_function"><code translate="no" dir="ltr">register_tensor_conversion_function(...)</code></a>: Registers a function for converting objects of <code translate="no" dir="ltr">base_type</code> to <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../repeat"><code translate="no" dir="ltr">repeat(...)</code></a>: Repeat elements of <code translate="no" dir="ltr">input</code>.</p> <p><a href="v1/report_uninitialized_variables"><code translate="no" dir="ltr">report_uninitialized_variables(...)</code></a>: Adds ops to list the names of uninitialized variables.</p> <p><a href="../required_space_to_batch_paddings"><code translate="no" dir="ltr">required_space_to_batch_paddings(...)</code></a>: Calculate padding required to make block_shape divide input_shape.</p> <p><a href="v1/reset_default_graph"><code translate="no" dir="ltr">reset_default_graph(...)</code></a>: Clears the default graph stack and resets the global default graph.</p> <p><a href="../reshape"><code translate="no" dir="ltr">reshape(...)</code></a>: Reshapes a tensor.</p> <p><a href="v1/resource_variables_enabled"><code translate="no" dir="ltr">resource_variables_enabled(...)</code></a>: Returns <code translate="no" dir="ltr">True</code> if resource variables are enabled.</p> <p><a href="../reverse"><code translate="no" dir="ltr">reverse(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="v1/reverse_sequence"><code translate="no" dir="ltr">reverse_sequence(...)</code></a>: Reverses variable length slices. (deprecated arguments) (deprecated arguments)</p> <p><a href="../reverse"><code translate="no" dir="ltr">reverse_v2(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="../math/rint"><code translate="no" dir="ltr">rint(...)</code></a>: Returns element-wise integer closest to x.</p> <p><a href="../roll"><code translate="no" dir="ltr">roll(...)</code></a>: Rolls the elements of a tensor along an axis.</p> <p><a href="../math/round"><code translate="no" dir="ltr">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href="../math/rsqrt"><code translate="no" dir="ltr">rsqrt(...)</code></a>: Computes reciprocal of square root of x element-wise.</p> <p><a href="../dtypes/saturate_cast"><code translate="no" dir="ltr">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate="no" dir="ltr">value</code> to <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="v1/scalar_mul"><code translate="no" dir="ltr">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code> object.</p> <p><a href="../scan"><code translate="no" dir="ltr">scan(...)</code></a>: scan on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="v1/scatter_add"><code translate="no" dir="ltr">scatter_add(...)</code></a>: Adds sparse updates to the variable referenced by <code translate="no" dir="ltr">resource</code>.</p> <p><a href="v1/scatter_div"><code translate="no" dir="ltr">scatter_div(...)</code></a>: Divides a variable reference by sparse updates.</p> <p><a href="v1/scatter_max"><code translate="no" dir="ltr">scatter_max(...)</code></a>: Reduces sparse updates into a variable reference using the <code translate="no" dir="ltr">max</code> operation.</p> <p><a href="v1/scatter_min"><code translate="no" dir="ltr">scatter_min(...)</code></a>: Reduces sparse updates into a variable reference using the <code translate="no" dir="ltr">min</code> operation.</p> <p><a href="v1/scatter_mul"><code translate="no" dir="ltr">scatter_mul(...)</code></a>: Multiplies sparse updates into a variable reference.</p> <p><a href="../scatter_nd"><code translate="no" dir="ltr">scatter_nd(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into a new tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="v1/scatter_nd_add"><code translate="no" dir="ltr">scatter_nd_add(...)</code></a>: Applies sparse addition to individual values or slices in a Variable.</p> <p><a href="v1/scatter_nd_sub"><code translate="no" dir="ltr">scatter_nd_sub(...)</code></a>: Applies sparse subtraction to individual values or slices in a Variable.</p> <p><a href="v1/scatter_nd_update"><code translate="no" dir="ltr">scatter_nd_update(...)</code></a>: Applies sparse <code translate="no" dir="ltr">updates</code> to individual values or slices in a Variable.</p> <p><a href="v1/scatter_sub"><code translate="no" dir="ltr">scatter_sub(...)</code></a>: Subtracts sparse updates to a variable reference.</p> <p><a href="v1/scatter_update"><code translate="no" dir="ltr">scatter_update(...)</code></a>: Applies sparse updates to a variable reference.</p> <p><a href="../searchsorted"><code translate="no" dir="ltr">searchsorted(...)</code></a>: Searches input tensor for values on the innermost dimension.</p> <p><a href="../math/segment_max"><code translate="no" dir="ltr">segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href="../math/segment_mean"><code translate="no" dir="ltr">segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href="../math/segment_min"><code translate="no" dir="ltr">segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href="../math/segment_prod"><code translate="no" dir="ltr">segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href="../math/segment_sum"><code translate="no" dir="ltr">segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href="../linalg/eigh"><code translate="no" dir="ltr">self_adjoint_eig(...)</code></a>: Computes the eigen decomposition of a batch of self-adjoint matrices.</p> <p><a href="../linalg/eigvalsh"><code translate="no" dir="ltr">self_adjoint_eigvals(...)</code></a>: Computes the eigenvalues of one or more self-adjoint matrices.</p> <p><a href="../sequence_mask"><code translate="no" dir="ltr">sequence_mask(...)</code></a>: Returns a mask tensor representing the first N positions of each cell.</p> <p><a href="v1/serialize_many_sparse"><code translate="no" dir="ltr">serialize_many_sparse(...)</code></a>: Serialize <code translate="no" dir="ltr">N</code>-minibatch <code translate="no" dir="ltr">SparseTensor</code> into an <code translate="no" dir="ltr">[N, 3]</code> <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="v1/serialize_sparse"><code translate="no" dir="ltr">serialize_sparse(...)</code></a>: Serialize a <code translate="no" dir="ltr">SparseTensor</code> into a 3-vector (1-D <code translate="no" dir="ltr">Tensor</code>) object.</p> <p><a href="../io/serialize_tensor"><code translate="no" dir="ltr">serialize_tensor(...)</code></a>: Transforms a Tensor into a serialized TensorProto proto.</p> <p><a href="v1/set_random_seed"><code translate="no" dir="ltr">set_random_seed(...)</code></a>: Sets the graph-level random seed for the default graph.</p> <p><a href="v1/setdiff1d"><code translate="no" dir="ltr">setdiff1d(...)</code></a>: Computes the difference between two lists of numbers or strings.</p> <p><a href="v1/shape"><code translate="no" dir="ltr">shape(...)</code></a>: Returns the shape of a tensor.</p> <p><a href="../shape_n"><code translate="no" dir="ltr">shape_n(...)</code></a>: Returns shape of tensors.</p> <p><a href="../math/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Computes sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../math/sign"><code translate="no" dir="ltr">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href="../math/sin"><code translate="no" dir="ltr">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href="../math/sinh"><code translate="no" dir="ltr">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href="v1/size"><code translate="no" dir="ltr">size(...)</code></a>: Returns the size of a tensor.</p> <p><a href="../slice"><code translate="no" dir="ltr">slice(...)</code></a>: Extracts a slice from a tensor.</p> <p><a href="../sort"><code translate="no" dir="ltr">sort(...)</code></a>: Sorts a tensor.</p> <p><a href="v1/space_to_batch"><code translate="no" dir="ltr">space_to_batch(...)</code></a>: SpaceToBatch for 4-D tensors of type T.</p> <p><a href="../space_to_batch_nd"><code translate="no" dir="ltr">space_to_batch_nd(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="v1/space_to_depth"><code translate="no" dir="ltr">space_to_depth(...)</code></a>: SpaceToDepth for tensors of type T.</p> <p><a href="v1/sparse_add"><code translate="no" dir="ltr">sparse_add(...)</code></a>: Adds two tensors, at least one of each is a <code translate="no" dir="ltr">SparseTensor</code>. (deprecated arguments)</p> <p><a href="v1/sparse_concat"><code translate="no" dir="ltr">sparse_concat(...)</code></a>: Concatenates a list of <code translate="no" dir="ltr">SparseTensor</code> along the specified dimension. (deprecated arguments)</p> <p><a href="../sparse/fill_empty_rows"><code translate="no" dir="ltr">sparse_fill_empty_rows(...)</code></a>: Fills empty rows in the input 2-D <code translate="no" dir="ltr">SparseTensor</code> with a default value.</p> <p><a href="../sparse/mask"><code translate="no" dir="ltr">sparse_mask(...)</code></a>: Masks elements of <code translate="no" dir="ltr">IndexedSlices</code>.</p> <p><a href="v1/sparse_matmul"><code translate="no" dir="ltr">sparse_matmul(...)</code></a>: Multiply matrix "a" by matrix "b".</p> <p><a href="../sparse/maximum"><code translate="no" dir="ltr">sparse_maximum(...)</code></a>: Returns the element-wise max of two SparseTensors.</p> <p><a href="v1/sparse_merge"><code translate="no" dir="ltr">sparse_merge(...)</code></a>: Combines a batch of feature ids and values into a single <code translate="no" dir="ltr">SparseTensor</code>. (deprecated)</p> <p><a href="../sparse/minimum"><code translate="no" dir="ltr">sparse_minimum(...)</code></a>: Returns the element-wise min of two SparseTensors.</p> <p><a href="v1/sparse_placeholder"><code translate="no" dir="ltr">sparse_placeholder(...)</code></a>: Inserts a placeholder for a sparse tensor that will be always fed.</p> <p><a href="v1/sparse_reduce_max"><code translate="no" dir="ltr">sparse_reduce_max(...)</code></a>: Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="v1/sparse_reduce_max_sparse"><code translate="no" dir="ltr">sparse_reduce_max_sparse(...)</code></a>: Computes the max of elements across dimensions of a SparseTensor. (deprecated arguments)</p> <p><a href="v1/sparse_reduce_sum"><code translate="no" dir="ltr">sparse_reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)</p> <p><a href="v1/sparse_reduce_sum_sparse"><code translate="no" dir="ltr">sparse_reduce_sum_sparse(...)</code></a>: Computes the sum of elements across dimensions of a SparseTensor. (deprecated arguments)</p> <p><a href="../sparse/reorder"><code translate="no" dir="ltr">sparse_reorder(...)</code></a>: Reorders a <code translate="no" dir="ltr">SparseTensor</code> into the canonical, row-major ordering.</p> <p><a href="../sparse/reset_shape"><code translate="no" dir="ltr">sparse_reset_shape(...)</code></a>: Resets the shape of a <code translate="no" dir="ltr">SparseTensor</code> with indices and values unchanged.</p> <p><a href="../sparse/reshape"><code translate="no" dir="ltr">sparse_reshape(...)</code></a>: Reshapes a <code translate="no" dir="ltr">SparseTensor</code> to represent values in a new dense shape.</p> <p><a href="../sparse/retain"><code translate="no" dir="ltr">sparse_retain(...)</code></a>: Retains specified non-empty values within a <code translate="no" dir="ltr">SparseTensor</code>.</p> <p><a href="v1/sparse_segment_mean"><code translate="no" dir="ltr">sparse_segment_mean(...)</code></a>: Computes the mean along sparse segments of a tensor.</p> <p><a href="v1/sparse_segment_sqrt_n"><code translate="no" dir="ltr">sparse_segment_sqrt_n(...)</code></a>: Computes the sum along sparse segments of a tensor divided by the sqrt(N).</p> <p><a href="v1/sparse_segment_sum"><code translate="no" dir="ltr">sparse_segment_sum(...)</code></a>: Computes the sum along sparse segments of a tensor.</p> <p><a href="../sparse/slice"><code translate="no" dir="ltr">sparse_slice(...)</code></a>: Slice a <code translate="no" dir="ltr">SparseTensor</code> based on the <code translate="no" dir="ltr">start</code> and `size.</p> <p><a href="../sparse/softmax"><code translate="no" dir="ltr">sparse_softmax(...)</code></a>: Applies softmax to a batched N-D <code translate="no" dir="ltr">SparseTensor</code>.</p> <p><a href="v1/sparse_split"><code translate="no" dir="ltr">sparse_split(...)</code></a>: Split a <code translate="no" dir="ltr">SparseTensor</code> into <code translate="no" dir="ltr">num_split</code> tensors along <code translate="no" dir="ltr">axis</code>. (deprecated arguments)</p> <p><a href="../sparse/sparse_dense_matmul"><code translate="no" dir="ltr">sparse_tensor_dense_matmul(...)</code></a>: Multiply SparseTensor (of rank 2) "A" by dense matrix "B".</p> <p><a href="../sparse/to_dense"><code translate="no" dir="ltr">sparse_tensor_to_dense(...)</code></a>: Converts a <code translate="no" dir="ltr">SparseTensor</code> into a dense tensor.</p> <p><a href="v1/sparse_to_dense"><code translate="no" dir="ltr">sparse_to_dense(...)</code></a>: Converts a sparse representation into a dense tensor. (deprecated)</p> <p><a href="../sparse/to_indicator"><code translate="no" dir="ltr">sparse_to_indicator(...)</code></a>: Converts a <code translate="no" dir="ltr">SparseTensor</code> of ids into a dense bool indicator tensor.</p> <p><a href="../sparse/transpose"><code translate="no" dir="ltr">sparse_transpose(...)</code></a>: Transposes a <code translate="no" dir="ltr">SparseTensor</code></p> <p><a href="../split"><code translate="no" dir="ltr">split(...)</code></a>: Splits a tensor into sub tensors.</p> <p><a href="../math/sqrt"><code translate="no" dir="ltr">sqrt(...)</code></a>: Computes square root of x element-wise.</p> <p><a href="../math/square"><code translate="no" dir="ltr">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href="../math/squared_difference"><code translate="no" dir="ltr">squared_difference(...)</code></a>: Returns (x - y)(x - y) element-wise.</p> <p><a href="v1/squeeze"><code translate="no" dir="ltr">squeeze(...)</code></a>: Removes dimensions of size 1 from the shape of a tensor. (deprecated arguments)</p> <p><a href="../stack"><code translate="no" dir="ltr">stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor.</p> <p><a href="../stop_gradient"><code translate="no" dir="ltr">stop_gradient(...)</code></a>: Stops gradient computation.</p> <p><a href="../strided_slice"><code translate="no" dir="ltr">strided_slice(...)</code></a>: Extracts a strided slice of a tensor (generalized python array indexing).</p> <p><a href="../strings/join"><code translate="no" dir="ltr">string_join(...)</code></a>: Joins the strings in the given list of string tensors into one tensor;</p> <p><a href="v1/string_split"><code translate="no" dir="ltr">string_split(...)</code></a>: Split elements of <code translate="no" dir="ltr">source</code> based on <code translate="no" dir="ltr">delimiter</code>. (deprecated arguments)</p> <p><a href="../strings/strip"><code translate="no" dir="ltr">string_strip(...)</code></a>: Strip leading and trailing whitespaces from the Tensor.</p> <p><a href="v1/string_to_hash_bucket"><code translate="no" dir="ltr">string_to_hash_bucket(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="../strings/to_hash_bucket_fast"><code translate="no" dir="ltr">string_to_hash_bucket_fast(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="../strings/to_hash_bucket_strong"><code translate="no" dir="ltr">string_to_hash_bucket_strong(...)</code></a>: Converts each string in the input Tensor to its hash mod by a number of buckets.</p> <p><a href="v1/string_to_number"><code translate="no" dir="ltr">string_to_number(...)</code></a>: Converts each string in the input Tensor to the specified numeric type.</p> <p><a href="v1/substr"><code translate="no" dir="ltr">substr(...)</code></a>: Return substrings from <code translate="no" dir="ltr">Tensor</code> of strings.</p> <p><a href="../math/subtract"><code translate="no" dir="ltr">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href="../linalg/svd"><code translate="no" dir="ltr">svd(...)</code></a>: Computes the singular value decompositions of one or more matrices.</p> <p><a href="../switch_case"><code translate="no" dir="ltr">switch_case(...)</code></a>: Create a switch/case operation, i.e. an integer-indexed conditional.</p> <p><a href="v1/tables_initializer"><code translate="no" dir="ltr">tables_initializer(...)</code></a>: Returns an Op that initializes all tables of the default graph.</p> <p><a href="../math/tan"><code translate="no" dir="ltr">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href="../math/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_nd_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_nd_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_nd_update(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_update(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensordot"><code translate="no" dir="ltr">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href="../tile"><code translate="no" dir="ltr">tile(...)</code></a>: Constructs a tensor by tiling a given tensor.</p> <p><a href="../timestamp"><code translate="no" dir="ltr">timestamp(...)</code></a>: Provides the time since epoch in seconds.</p> <p><a href="v1/to_bfloat16"><code translate="no" dir="ltr">to_bfloat16(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">bfloat16</code>. (deprecated)</p> <p><a href="v1/to_complex128"><code translate="no" dir="ltr">to_complex128(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">complex128</code>. (deprecated)</p> <p><a href="v1/to_complex64"><code translate="no" dir="ltr">to_complex64(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">complex64</code>. (deprecated)</p> <p><a href="v1/to_double"><code translate="no" dir="ltr">to_double(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">float64</code>. (deprecated)</p> <p><a href="v1/to_float"><code translate="no" dir="ltr">to_float(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">float32</code>. (deprecated)</p> <p><a href="v1/to_int32"><code translate="no" dir="ltr">to_int32(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">int32</code>. (deprecated)</p> <p><a href="v1/to_int64"><code translate="no" dir="ltr">to_int64(...)</code></a>: Casts a tensor to type <code translate="no" dir="ltr">int64</code>. (deprecated)</p> <p><a href="../linalg/trace"><code translate="no" dir="ltr">trace(...)</code></a>: Compute the trace of a tensor <code translate="no" dir="ltr">x</code>.</p> <p><a href="v1/trainable_variables"><code translate="no" dir="ltr">trainable_variables(...)</code></a>: Returns all variables created with <code translate="no" dir="ltr">trainable=True</code>.</p> <p><a href="v1/transpose"><code translate="no" dir="ltr">transpose(...)</code></a>: Transposes <code translate="no" dir="ltr">a</code>.</p> <p><a href="../math/truediv"><code translate="no" dir="ltr">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href="../random/truncated_normal"><code translate="no" dir="ltr">truncated_normal(...)</code></a>: Outputs random values from a truncated normal distribution.</p> <p><a href="../truncatediv"><code translate="no" dir="ltr">truncatediv(...)</code></a>: Returns x / y element-wise for integer types.</p> <p><a href="../truncatemod"><code translate="no" dir="ltr">truncatemod(...)</code></a>: Returns element-wise remainder of division. This emulates C semantics in that</p> <p><a href="v1/tuple"><code translate="no" dir="ltr">tuple(...)</code></a>: Group tensors together.</p> <p><a href="../unique"><code translate="no" dir="ltr">unique(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="../unique_with_counts"><code translate="no" dir="ltr">unique_with_counts(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="../unravel_index"><code translate="no" dir="ltr">unravel_index(...)</code></a>: Converts an array of flat indices into a tuple of coordinate arrays.</p> <p><a href="../math/unsorted_segment_max"><code translate="no" dir="ltr">unsorted_segment_max(...)</code></a>: Computes the maximum along segments of a tensor.</p> <p><a href="../math/unsorted_segment_mean"><code translate="no" dir="ltr">unsorted_segment_mean(...)</code></a>: Computes the mean along segments of a tensor.</p> <p><a href="../math/unsorted_segment_min"><code translate="no" dir="ltr">unsorted_segment_min(...)</code></a>: Computes the minimum along segments of a tensor.</p> <p><a href="../math/unsorted_segment_prod"><code translate="no" dir="ltr">unsorted_segment_prod(...)</code></a>: Computes the product along segments of a tensor.</p> <p><a href="../math/unsorted_segment_sqrt_n"><code translate="no" dir="ltr">unsorted_segment_sqrt_n(...)</code></a>: Computes the sum along segments of a tensor divided by the sqrt(N).</p> <p><a href="../math/unsorted_segment_sum"><code translate="no" dir="ltr">unsorted_segment_sum(...)</code></a>: Computes the sum along segments of a tensor.</p> <p><a href="../unstack"><code translate="no" dir="ltr">unstack(...)</code></a>: Unpacks the given dimension of a rank-<code translate="no" dir="ltr">R</code> tensor into rank-<code translate="no" dir="ltr">(R-1)</code> tensors.</p> <p><a href="v1/variable_axis_size_partitioner"><code translate="no" dir="ltr">variable_axis_size_partitioner(...)</code></a>: Get a partitioner for VariableScope to keep shards below <code translate="no" dir="ltr">max_shard_bytes</code>.</p> <p><a href="v1/variable_creator_scope"><code translate="no" dir="ltr">variable_creator_scope(...)</code></a>: Scope which defines a variable creation function to be used by variable().</p> <p><a href="v1/variable_op_scope"><code translate="no" dir="ltr">variable_op_scope(...)</code></a>: Deprecated: context manager for defining an op that creates variables.</p> <p><a href="v1/variables_initializer"><code translate="no" dir="ltr">variables_initializer(...)</code></a>: Returns an Op that initializes a list of variables.</p> <p><a href="../vectorized_map"><code translate="no" dir="ltr">vectorized_map(...)</code></a>: Parallel map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="v1/verify_tensor_all_finite"><code translate="no" dir="ltr">verify_tensor_all_finite(...)</code></a>: Assert that the tensor does not contain any NaN's or Inf's.</p> <p><a href="v1/where"><code translate="no" dir="ltr">where(...)</code></a>: Return the elements, either from <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code>, depending on the <code translate="no" dir="ltr">condition</code>.</p> <p><a href="../where"><code translate="no" dir="ltr">where_v2(...)</code></a>: Return the elements, either from <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code>, depending on the <code translate="no" dir="ltr">condition</code>.</p> <p><a href="v1/while_loop"><code translate="no" dir="ltr">while_loop(...)</code></a>: Repeat <code translate="no" dir="ltr">body</code> while the condition <code translate="no" dir="ltr">cond</code> is true.</p> <p><a href="v1/wrap_function"><code translate="no" dir="ltr">wrap_function(...)</code></a>: Wraps the TF 1.x function fn into a graph function.</p> <p><a href="../io/write_file"><code translate="no" dir="ltr">write_file(...)</code></a>: Writes contents to the file at input filename. Creates file and recursively</p> <p><a href="../zeros"><code translate="no" dir="ltr">zeros(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="v1/zeros_like"><code translate="no" dir="ltr">zeros_like(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="../math/zeta"><code translate="no" dir="ltr">zeta(...)</code></a>: Compute the Hurwitz zeta function \(\zeta(x, q)\).</p> <h2 id="other_members">Other Members</h2> <ul> <li>
<code translate="no" dir="ltr">AUTO_REUSE</code> 
</li> <li>
<code translate="no" dir="ltr">COMPILER_VERSION = '7.3.1 20180303'</code> 
</li> <li>
<code translate="no" dir="ltr">CXX11_ABI_FLAG = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GIT_VERSION = 'v2.1.0-rc2-17-ge5bf8de'</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION = 175</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION_MIN_CONSUMER = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GRAPH_DEF_VERSION_MIN_PRODUCER = 0</code> 
</li> <li>
<code translate="no" dir="ltr">MONOLITHIC_BUILD = 0</code> 
</li> <li>
<code translate="no" dir="ltr">QUANTIZED_DTYPES</code> 
</li> <li>
<code translate="no" dir="ltr">VERSION = '2.1.0'</code> 
</li> <li>
<code translate="no" dir="ltr">__version__ = '2.1.0'</code> 
</li> <li>
<code translate="no" dir="ltr">bfloat16</code> 
</li> <li>
<code translate="no" dir="ltr">bool</code> 
</li> <li>
<code translate="no" dir="ltr">complex128</code> 
</li> <li>
<code translate="no" dir="ltr">complex64</code> 
</li> <li>
<code translate="no" dir="ltr">double</code> 
</li> <li>
<code translate="no" dir="ltr">float16</code> 
</li> <li>
<code translate="no" dir="ltr">float32</code> 
</li> <li>
<code translate="no" dir="ltr">float64</code> 
</li> <li>
<code translate="no" dir="ltr">half</code> 
</li> <li>
<code translate="no" dir="ltr">int16</code> 
</li> <li>
<code translate="no" dir="ltr">int32</code> 
</li> <li>
<code translate="no" dir="ltr">int64</code> 
</li> <li>
<code translate="no" dir="ltr">int8</code> 
</li> <li>
<code translate="no" dir="ltr">qint16</code> 
</li> <li>
<code translate="no" dir="ltr">qint32</code> 
</li> <li>
<code translate="no" dir="ltr">qint8</code> 
</li> <li>
<code translate="no" dir="ltr">quint16</code> 
</li> <li>
<code translate="no" dir="ltr">quint8</code> 
</li> <li>
<code translate="no" dir="ltr">resource</code> 
</li> <li>
<code translate="no" dir="ltr">string</code> 
</li> <li>
<code translate="no" dir="ltr">uint16</code> 
</li> <li>
<code translate="no" dir="ltr">uint32</code> 
</li> <li>
<code translate="no" dir="ltr">uint64</code> 
</li> <li>
<code translate="no" dir="ltr">uint8</code> 
</li> <li>
<code translate="no" dir="ltr">variant</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1</a>
  </p>
</div>
