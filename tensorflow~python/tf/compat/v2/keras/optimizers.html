<h1 class="devsite-page-title">Module: tf.compat.v2.keras.optimizers</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v2.keras.optimizers"> <meta itemprop="path" content="Stable"> </div> <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/compat/v2/keras/optimizers">  TensorFlow 1 version</a> </td> </table> <p>Built-in optimizer classes.</p> <h2 id="modules">Modules</h2> <p><a href="optimizers/schedules"><code translate="no" dir="ltr">schedules</code></a> module: Public API for tf.keras.optimizers.schedules namespace.</p> <h2 id="classes">Classes</h2> <p><a href="../../../keras/optimizers/adadelta"><code translate="no" dir="ltr">class Adadelta</code></a>: Optimizer that implements the Adadelta algorithm.</p> <p><a href="../../../keras/optimizers/adagrad"><code translate="no" dir="ltr">class Adagrad</code></a>: Optimizer that implements the Adagrad algorithm.</p> <p><a href="../../../keras/optimizers/adam"><code translate="no" dir="ltr">class Adam</code></a>: Optimizer that implements the Adam algorithm.</p> <p><a href="../../../keras/optimizers/adamax"><code translate="no" dir="ltr">class Adamax</code></a>: Optimizer that implements the Adamax algorithm.</p> <p><a href="../../../keras/optimizers/ftrl"><code translate="no" dir="ltr">class Ftrl</code></a>: Optimizer that implements the FTRL algorithm.</p> <p><a href="../../../keras/optimizers/nadam"><code translate="no" dir="ltr">class Nadam</code></a>: Optimizer that implements the NAdam algorithm.</p> <p><a href="../../../keras/optimizers/optimizer"><code translate="no" dir="ltr">class Optimizer</code></a>: Updated base class for optimizers.</p> <p><a href="../../../keras/optimizers/rmsprop"><code translate="no" dir="ltr">class RMSprop</code></a>: Optimizer that implements the RMSprop algorithm.</p> <p><a href="../../../keras/optimizers/sgd"><code translate="no" dir="ltr">class SGD</code></a>: Stochastic gradient descent and momentum optimizer.</p> <h2 id="functions">Functions</h2> <p><a href="../../../keras/optimizers/deserialize"><code translate="no" dir="ltr">deserialize(...)</code></a>: Inverse of the <code translate="no" dir="ltr">serialize</code> function.</p> <p><a href="../../../keras/optimizers/get"><code translate="no" dir="ltr">get(...)</code></a>: Retrieves a Keras Optimizer instance.</p> <p><a href="../../../keras/optimizers/serialize"><code translate="no" dir="ltr">serialize(...)</code></a></p> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="optimizers"><code translate="no" dir="ltr">tf.compat.v2.optimizers</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v2/keras/optimizers" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v2/keras/optimizers</a>
  </p>
</div>
