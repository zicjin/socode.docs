<h1 class="devsite-page-title">Module: tf.compat.v2</h1>    <devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>  <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v2"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__version__"> <meta itemprop="property" content="bfloat16"> <meta itemprop="property" content="bool"> <meta itemprop="property" content="complex128"> <meta itemprop="property" content="complex64"> <meta itemprop="property" content="double"> <meta itemprop="property" content="float16"> <meta itemprop="property" content="float32"> <meta itemprop="property" content="float64"> <meta itemprop="property" content="half"> <meta itemprop="property" content="int16"> <meta itemprop="property" content="int32"> <meta itemprop="property" content="int64"> <meta itemprop="property" content="int8"> <meta itemprop="property" content="qint16"> <meta itemprop="property" content="qint32"> <meta itemprop="property" content="qint8"> <meta itemprop="property" content="quint16"> <meta itemprop="property" content="quint8"> <meta itemprop="property" content="resource"> <meta itemprop="property" content="string"> <meta itemprop="property" content="uint16"> <meta itemprop="property" content="uint32"> <meta itemprop="property" content="uint64"> <meta itemprop="property" content="uint8"> <meta itemprop="property" content="variant"> </div> <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/compat/v2">  TensorFlow 1 version</a> </td> </table> <p>Bring in all of the public TensorFlow interface into this module.</p> <h2 id="modules">Modules</h2> <p><a href="v2/audio"><code translate="no" dir="ltr">audio</code></a> module: Public API for tf.audio namespace.</p> <p><a href="v2/autodiff"><code translate="no" dir="ltr">autodiff</code></a> module: Public API for tf.autodiff namespace.</p> <p><a href="v2/autograph"><code translate="no" dir="ltr">autograph</code></a> module: Conversion of plain Python into TensorFlow graph code.</p> <p><a href="v2/bitwise"><code translate="no" dir="ltr">bitwise</code></a> module: Operations for manipulating the binary representations of integers.</p> <p><a href="v2/compat"><code translate="no" dir="ltr">compat</code></a> module: Compatibility functions.</p> <p><a href="v2/config"><code translate="no" dir="ltr">config</code></a> module: Public API for tf.config namespace.</p> <p><a href="v2/data"><code translate="no" dir="ltr">data</code></a> module: <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> API for input pipelines.</p> <p><a href="v2/debugging"><code translate="no" dir="ltr">debugging</code></a> module: Public API for tf.debugging namespace.</p> <p><a href="v2/distribute"><code translate="no" dir="ltr">distribute</code></a> module: Library for running a computation across multiple devices.</p> <p><a href="v2/dtypes"><code translate="no" dir="ltr">dtypes</code></a> module: Public API for tf.dtypes namespace.</p> <p><a href="v2/errors"><code translate="no" dir="ltr">errors</code></a> module: Exception types for TensorFlow errors.</p> <p><a href="../estimator"><code translate="no" dir="ltr">estimator</code></a> module: Estimator: High level tools for working with models.</p> <p><a href="v2/experimental"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.experimental namespace.</p> <p><a href="v2/feature_column"><code translate="no" dir="ltr">feature_column</code></a> module: Public API for tf.feature_column namespace.</p> <p><a href="v2/graph_util"><code translate="no" dir="ltr">graph_util</code></a> module: Helpers to manipulate a tensor graph in python.</p> <p><a href="v2/image"><code translate="no" dir="ltr">image</code></a> module: Image processing and decoding ops.</p> <p><a href="v2/keras/initializers"><code translate="no" dir="ltr">initializers</code></a> module: Keras initializer serialization / deserialization.</p> <p><a href="v2/io"><code translate="no" dir="ltr">io</code></a> module: Public API for tf.io namespace.</p> <p><a href="v2/keras"><code translate="no" dir="ltr">keras</code></a> module: Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p><a href="v2/linalg"><code translate="no" dir="ltr">linalg</code></a> module: Operations for linear algebra.</p> <p><a href="v2/lite"><code translate="no" dir="ltr">lite</code></a> module: Public API for tf.lite namespace.</p> <p><a href="v2/lookup"><code translate="no" dir="ltr">lookup</code></a> module: Public API for tf.lookup namespace.</p> <p><a href="v2/keras/losses"><code translate="no" dir="ltr">losses</code></a> module: Built-in loss functions.</p> <p><a href="v2/math"><code translate="no" dir="ltr">math</code></a> module: Math Operations.</p> <p><a href="v2/keras/metrics"><code translate="no" dir="ltr">metrics</code></a> module: Built-in metrics.</p> <p><a href="v2/mixed_precision"><code translate="no" dir="ltr">mixed_precision</code></a> module: Public API for tf.mixed_precision namespace.</p> <p><a href="v2/mlir"><code translate="no" dir="ltr">mlir</code></a> module: Public API for tf.mlir namespace.</p> <p><a href="v2/nest"><code translate="no" dir="ltr">nest</code></a> module: Public API for tf.nest namespace.</p> <p><a href="v2/nn"><code translate="no" dir="ltr">nn</code></a> module: Wrappers for primitive Neural Net (NN) Operations.</p> <p><a href="v2/keras/optimizers"><code translate="no" dir="ltr">optimizers</code></a> module: Built-in optimizer classes.</p> <p><a href="v2/quantization"><code translate="no" dir="ltr">quantization</code></a> module: Public API for tf.quantization namespace.</p> <p><a href="v2/queue"><code translate="no" dir="ltr">queue</code></a> module: Public API for tf.queue namespace.</p> <p><a href="v2/ragged"><code translate="no" dir="ltr">ragged</code></a> module: Ragged Tensors.</p> <p><a href="v2/random"><code translate="no" dir="ltr">random</code></a> module: Public API for tf.random namespace.</p> <p><a href="v2/raw_ops"><code translate="no" dir="ltr">raw_ops</code></a> module: Public API for tf.raw_ops namespace.</p> <p><a href="v2/saved_model"><code translate="no" dir="ltr">saved_model</code></a> module: Public API for tf.saved_model namespace.</p> <p><a href="v2/sets"><code translate="no" dir="ltr">sets</code></a> module: Tensorflow set operations.</p> <p><a href="v2/signal"><code translate="no" dir="ltr">signal</code></a> module: Signal processing operations.</p> <p><a href="v2/sparse"><code translate="no" dir="ltr">sparse</code></a> module: Sparse Tensor Representation.</p> <p><a href="v2/strings"><code translate="no" dir="ltr">strings</code></a> module: Operations for working with string Tensors.</p> <p><a href="../summary"><code translate="no" dir="ltr">summary</code></a> module: Operations for writing summary data, for use in analysis and visualization.</p> <p><a href="v2/sysconfig"><code translate="no" dir="ltr">sysconfig</code></a> module: System configuration library.</p> <p><a href="v2/test"><code translate="no" dir="ltr">test</code></a> module: Testing.</p> <p><a href="v2/tpu"><code translate="no" dir="ltr">tpu</code></a> module: Ops related to Tensor Processing Units.</p> <p><a href="v2/train"><code translate="no" dir="ltr">train</code></a> module: Support for training models.</p> <p><a href="v2/version"><code translate="no" dir="ltr">version</code></a> module: Public API for tf.version namespace.</p> <p><a href="v2/xla"><code translate="no" dir="ltr">xla</code></a> module: Public API for tf.xla namespace.</p> <h2 id="classes">Classes</h2> <p><a href="../aggregationmethod"><code translate="no" dir="ltr">class AggregationMethod</code></a>: A class listing aggregation methods used to combine gradients.</p> <p><a href="../criticalsection"><code translate="no" dir="ltr">class CriticalSection</code></a>: Critical section.</p> <p><a href="../dtypes/dtype"><code translate="no" dir="ltr">class DType</code></a>: Represents the type of the elements in a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../devicespec"><code translate="no" dir="ltr">class DeviceSpec</code></a>: Represents a (possibly partial) specification for a TensorFlow device.</p> <p><a href="../gradienttape"><code translate="no" dir="ltr">class GradientTape</code></a>: Record operations for automatic differentiation.</p> <p><a href="../graph"><code translate="no" dir="ltr">class Graph</code></a>: A TensorFlow computation, represented as a dataflow graph.</p> <p><a href="../indexedslices"><code translate="no" dir="ltr">class IndexedSlices</code></a>: A sparse representation of a set of tensor slices at given indices.</p> <p><a href="../indexedslicesspec"><code translate="no" dir="ltr">class IndexedSlicesSpec</code></a>: Type specification for a <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p><a href="../module"><code translate="no" dir="ltr">class Module</code></a>: Base neural network module class.</p> <p><a href="../operation"><code translate="no" dir="ltr">class Operation</code></a>: Represents a graph node that performs computation on tensors.</p> <p><a href="../optionalspec"><code translate="no" dir="ltr">class OptionalSpec</code></a>: Represents an optional potentially containing a structured value.</p> <p><a href="../raggedtensor"><code translate="no" dir="ltr">class RaggedTensor</code></a>: Represents a ragged tensor.</p> <p><a href="../raggedtensorspec"><code translate="no" dir="ltr">class RaggedTensorSpec</code></a>: Type specification for a <a href="../raggedtensor"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>.</p> <p><a href="../registergradient"><code translate="no" dir="ltr">class RegisterGradient</code></a>: A decorator for registering the gradient function for an op type.</p> <p><a href="../sparse/sparsetensor"><code translate="no" dir="ltr">class SparseTensor</code></a>: Represents a sparse tensor.</p> <p><a href="../sparsetensorspec"><code translate="no" dir="ltr">class SparseTensorSpec</code></a>: Type specification for a <a href="../sparse/sparsetensor"><code translate="no" dir="ltr">tf.SparseTensor</code></a>.</p> <p><a href="../tensor"><code translate="no" dir="ltr">class Tensor</code></a>: Represents one of the outputs of an <code translate="no" dir="ltr">Operation</code>.</p> <p><a href="../tensorarray"><code translate="no" dir="ltr">class TensorArray</code></a>: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p><a href="../tensorarrayspec"><code translate="no" dir="ltr">class TensorArraySpec</code></a>: Type specification for a <a href="../tensorarray"><code translate="no" dir="ltr">tf.TensorArray</code></a>.</p> <p><a href="../tensorshape"><code translate="no" dir="ltr">class TensorShape</code></a>: Represents the shape of a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../tensorspec"><code translate="no" dir="ltr">class TensorSpec</code></a>: Describes a tf.Tensor.</p> <p><a href="../typespec"><code translate="no" dir="ltr">class TypeSpec</code></a>: Specifies a TensorFlow value type.</p> <p><a href="../unconnectedgradients"><code translate="no" dir="ltr">class UnconnectedGradients</code></a>: Controls how gradient computation behaves when y does not depend on x.</p> <p><a href="../variable"><code translate="no" dir="ltr">class Variable</code></a>: See the <a href="https://tensorflow.org/guide/variable">variable guide</a>.</p> <p><a href="../variableaggregation"><code translate="no" dir="ltr">class VariableAggregation</code></a>: Indicates how a distributed variable will be aggregated.</p> <p><a href="../variablesynchronization"><code translate="no" dir="ltr">class VariableSynchronization</code></a>: Indicates when a distributed variable will be synced.</p> <p><a href="../constant_initializer"><code translate="no" dir="ltr">class constant_initializer</code></a>: Initializer that generates tensors with constant values.</p> <p><a href="../name_scope"><code translate="no" dir="ltr">class name_scope</code></a>: A context manager for use when defining a Python op.</p> <p><a href="../ones_initializer"><code translate="no" dir="ltr">class ones_initializer</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href="../random_normal_initializer"><code translate="no" dir="ltr">class random_normal_initializer</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href="../random_uniform_initializer"><code translate="no" dir="ltr">class random_uniform_initializer</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href="../zeros_initializer"><code translate="no" dir="ltr">class zeros_initializer</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id="functions">Functions</h2> <p><a href="../debugging/assert"><code translate="no" dir="ltr">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href="../math/abs"><code translate="no" dir="ltr">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href="../math/acos"><code translate="no" dir="ltr">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href="../math/acosh"><code translate="no" dir="ltr">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href="../math/add"><code translate="no" dir="ltr">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href="../math/add_n"><code translate="no" dir="ltr">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href="../math/argmax"><code translate="no" dir="ltr">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor.</p> <p><a href="../math/argmin"><code translate="no" dir="ltr">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor.</p> <p><a href="../argsort"><code translate="no" dir="ltr">argsort(...)</code></a>: Returns the indices of a tensor that give its sorted order along an axis.</p> <p><a href="../dtypes/as_dtype"><code translate="no" dir="ltr">as_dtype(...)</code></a>: Converts the given <code translate="no" dir="ltr">type_value</code> to a <code translate="no" dir="ltr">DType</code>.</p> <p><a href="../strings/as_string"><code translate="no" dir="ltr">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href="../math/asin"><code translate="no" dir="ltr">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href="../math/asinh"><code translate="no" dir="ltr">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href="../debugging/assert_equal"><code translate="no" dir="ltr">assert_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x == y</code> holds element-wise.</p> <p><a href="../debugging/assert_greater"><code translate="no" dir="ltr">assert_greater(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; y</code> holds element-wise.</p> <p><a href="../debugging/assert_less"><code translate="no" dir="ltr">assert_less(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; y</code> holds element-wise.</p> <p><a href="../debugging/assert_rank"><code translate="no" dir="ltr">assert_rank(...)</code></a>: Assert that <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code>.</p> <p><a href="../math/atan"><code translate="no" dir="ltr">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href="../math/atan2"><code translate="no" dir="ltr">atan2(...)</code></a>: Computes arctangent of <code translate="no" dir="ltr">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href="../math/atanh"><code translate="no" dir="ltr">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href="../batch_to_space"><code translate="no" dir="ltr">batch_to_space(...)</code></a>: BatchToSpace for N-D tensors of type T.</p> <p><a href="../bitcast"><code translate="no" dir="ltr">bitcast(...)</code></a>: Bitcasts a tensor from one type to another without copying data.</p> <p><a href="../boolean_mask"><code translate="no" dir="ltr">boolean_mask(...)</code></a>: Apply boolean mask to tensor.</p> <p><a href="../broadcast_dynamic_shape"><code translate="no" dir="ltr">broadcast_dynamic_shape(...)</code></a>: Computes the shape of a broadcast given symbolic shapes.</p> <p><a href="../broadcast_static_shape"><code translate="no" dir="ltr">broadcast_static_shape(...)</code></a>: Computes the shape of a broadcast given known shapes.</p> <p><a href="../broadcast_to"><code translate="no" dir="ltr">broadcast_to(...)</code></a>: Broadcast an array for a compatible shape.</p> <p><a href="../case"><code translate="no" dir="ltr">case(...)</code></a>: Create a case operation.</p> <p><a href="../cast"><code translate="no" dir="ltr">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href="../clip_by_global_norm"><code translate="no" dir="ltr">clip_by_global_norm(...)</code></a>: Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p><a href="../clip_by_norm"><code translate="no" dir="ltr">clip_by_norm(...)</code></a>: Clips tensor values to a maximum L2-norm.</p> <p><a href="../clip_by_value"><code translate="no" dir="ltr">clip_by_value(...)</code></a>: Clips tensor values to a specified min and max.</p> <p><a href="../dtypes/complex"><code translate="no" dir="ltr">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href="../concat"><code translate="no" dir="ltr">concat(...)</code></a>: Concatenates tensors along one dimension.</p> <p><a href="../cond"><code translate="no" dir="ltr">cond(...)</code></a>: Return <code translate="no" dir="ltr">true_fn()</code> if the predicate <code translate="no" dir="ltr">pred</code> is true else <code translate="no" dir="ltr">false_fn()</code>.</p> <p><a href="../constant"><code translate="no" dir="ltr">constant(...)</code></a>: Creates a constant tensor from a tensor-like object.</p> <p><a href="../control_dependencies"><code translate="no" dir="ltr">control_dependencies(...)</code></a>: Wrapper for <a href="../graph#control_dependencies"><code translate="no" dir="ltr">Graph.control_dependencies()</code></a> using the default graph.</p> <p><a href="../convert_to_tensor"><code translate="no" dir="ltr">convert_to_tensor(...)</code></a>: Converts the given <code translate="no" dir="ltr">value</code> to a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../math/cos"><code translate="no" dir="ltr">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href="../math/cosh"><code translate="no" dir="ltr">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href="../math/cumsum"><code translate="no" dir="ltr">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="../custom_gradient"><code translate="no" dir="ltr">custom_gradient(...)</code></a>: Decorator to define a function with a custom gradient.</p> <p><a href="../device"><code translate="no" dir="ltr">device(...)</code></a>: Specifies the device for ops created/executed in this context.</p> <p><a href="../math/divide"><code translate="no" dir="ltr">divide(...)</code></a>: Computes Python style division of <code translate="no" dir="ltr">x</code> by <code translate="no" dir="ltr">y</code>.</p> <p><a href="../dynamic_partition"><code translate="no" dir="ltr">dynamic_partition(...)</code></a>: Partitions <code translate="no" dir="ltr">data</code> into <code translate="no" dir="ltr">num_partitions</code> tensors using indices from <code translate="no" dir="ltr">partitions</code>.</p> <p><a href="../dynamic_stitch"><code translate="no" dir="ltr">dynamic_stitch(...)</code></a>: Interleave the values from the <code translate="no" dir="ltr">data</code> tensors into a single tensor.</p> <p><a href="../edit_distance"><code translate="no" dir="ltr">edit_distance(...)</code></a>: Computes the Levenshtein distance between sequences.</p> <p><a href="../eig"><code translate="no" dir="ltr">eig(...)</code></a>: Computes the eigen decomposition of a batch of matrices.</p> <p><a href="../eigvals"><code translate="no" dir="ltr">eigvals(...)</code></a>: Computes the eigenvalues of one or more matrices.</p> <p><a href="../einsum"><code translate="no" dir="ltr">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href="v1/enable_v2_behavior"><code translate="no" dir="ltr">enable_v2_behavior(...)</code></a>: Enables TensorFlow 2.x behaviors.</p> <p><a href="../ensure_shape"><code translate="no" dir="ltr">ensure_shape(...)</code></a>: Updates the shape of a tensor and checks at runtime that the shape holds.</p> <p><a href="../math/equal"><code translate="no" dir="ltr">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href="../executing_eagerly"><code translate="no" dir="ltr">executing_eagerly(...)</code></a>: Checks whether the current thread has eager execution enabled.</p> <p><a href="../math/exp"><code translate="no" dir="ltr">exp(...)</code></a>: Computes exponential of x element-wise. \(y = e^x\).</p> <p><a href="../expand_dims"><code translate="no" dir="ltr">expand_dims(...)</code></a>: Returns a tensor with an additional dimension inserted at index <code translate="no" dir="ltr">axis</code>.</p> <p><a href="../extract_volume_patches"><code translate="no" dir="ltr">extract_volume_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">input</code> and put them in the "depth" output dimension. 3D extension of <code translate="no" dir="ltr">extract_image_patches</code>.</p> <p><a href="../eye"><code translate="no" dir="ltr">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href="../fill"><code translate="no" dir="ltr">fill(...)</code></a>: Creates a tensor filled with a scalar value.</p> <p><a href="../fingerprint"><code translate="no" dir="ltr">fingerprint(...)</code></a>: Generates fingerprint values.</p> <p><a href="../math/floor"><code translate="no" dir="ltr">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href="../foldl"><code translate="no" dir="ltr">foldl(...)</code></a>: foldl on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../foldr"><code translate="no" dir="ltr">foldr(...)</code></a>: foldr on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../function"><code translate="no" dir="ltr">function(...)</code></a>: Compiles a function into a callable TensorFlow graph.</p> <p><a href="../gather"><code translate="no" dir="ltr">gather(...)</code></a>: Gather slices from params axis <code translate="no" dir="ltr">axis</code> according to indices.</p> <p><a href="../gather_nd"><code translate="no" dir="ltr">gather_nd(...)</code></a>: Gather slices from <code translate="no" dir="ltr">params</code> into a Tensor with shape specified by <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../get_logger"><code translate="no" dir="ltr">get_logger(...)</code></a>: Return TF logger instance.</p> <p><a href="../get_static_value"><code translate="no" dir="ltr">get_static_value(...)</code></a>: Returns the constant value of the given tensor, if efficiently calculable.</p> <p><a href="../grad_pass_through"><code translate="no" dir="ltr">grad_pass_through(...)</code></a>: Creates a grad-pass-through op with the forward behavior provided in f.</p> <p><a href="../gradients"><code translate="no" dir="ltr">gradients(...)</code></a>: Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="../math/greater"><code translate="no" dir="ltr">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href="../math/greater_equal"><code translate="no" dir="ltr">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href="../group"><code translate="no" dir="ltr">group(...)</code></a>: Create an op that groups multiple operations.</p> <p><a href="../guarantee_const"><code translate="no" dir="ltr">guarantee_const(...)</code></a>: Gives a guarantee to the TF runtime that the input tensor is a constant.</p> <p><a href="../hessians"><code translate="no" dir="ltr">hessians(...)</code></a>: Constructs the Hessian of sum of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">x</code> in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="../histogram_fixed_width"><code translate="no" dir="ltr">histogram_fixed_width(...)</code></a>: Return histogram of values.</p> <p><a href="../histogram_fixed_width_bins"><code translate="no" dir="ltr">histogram_fixed_width_bins(...)</code></a>: Bins the given values for use in a histogram.</p> <p><a href="../identity"><code translate="no" dir="ltr">identity(...)</code></a>: Return a tensor with the same shape and contents as input.</p> <p><a href="../identity_n"><code translate="no" dir="ltr">identity_n(...)</code></a>: Returns a list of tensors with the same shapes and contents as the input</p> <p><a href="../graph_util/import_graph_def"><code translate="no" dir="ltr">import_graph_def(...)</code></a>: Imports the graph from <code translate="no" dir="ltr">graph_def</code> into the current default <code translate="no" dir="ltr">Graph</code>. (deprecated arguments)</p> <p><a href="../init_scope"><code translate="no" dir="ltr">init_scope(...)</code></a>: A context manager that lifts ops out of control-flow scopes and function-building graphs.</p> <p><a href="../is_tensor"><code translate="no" dir="ltr">is_tensor(...)</code></a>: Checks whether <code translate="no" dir="ltr">x</code> is a tensor or "tensor-like".</p> <p><a href="../math/less"><code translate="no" dir="ltr">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href="../math/less_equal"><code translate="no" dir="ltr">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href="../linspace"><code translate="no" dir="ltr">linspace(...)</code></a>: Generates values in an interval.</p> <p><a href="../load_library"><code translate="no" dir="ltr">load_library(...)</code></a>: Loads a TensorFlow plugin.</p> <p><a href="../load_op_library"><code translate="no" dir="ltr">load_op_library(...)</code></a>: Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p><a href="../math/logical_and"><code translate="no" dir="ltr">logical_and(...)</code></a>: Returns the truth value of x AND y element-wise.</p> <p><a href="../math/logical_not"><code translate="no" dir="ltr">logical_not(...)</code></a>: Returns the truth value of NOT x element-wise.</p> <p><a href="../math/logical_or"><code translate="no" dir="ltr">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href="../make_ndarray"><code translate="no" dir="ltr">make_ndarray(...)</code></a>: Create a numpy ndarray from a tensor.</p> <p><a href="../make_tensor_proto"><code translate="no" dir="ltr">make_tensor_proto(...)</code></a>: Create a TensorProto.</p> <p><a href="../map_fn"><code translate="no" dir="ltr">map_fn(...)</code></a>: map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../linalg/matmul"><code translate="no" dir="ltr">matmul(...)</code></a>: Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p><a href="../linalg/sqrtm"><code translate="no" dir="ltr">matrix_square_root(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href="../math/maximum"><code translate="no" dir="ltr">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href="../meshgrid"><code translate="no" dir="ltr">meshgrid(...)</code></a>: Broadcasts parameters for evaluation on an N-D grid.</p> <p><a href="../math/minimum"><code translate="no" dir="ltr">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href="../math/multiply"><code translate="no" dir="ltr">multiply(...)</code></a>: Returns x * y element-wise.</p> <p><a href="../math/negative"><code translate="no" dir="ltr">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href="../no_gradient"><code translate="no" dir="ltr">no_gradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="../no_op"><code translate="no" dir="ltr">no_op(...)</code></a>: Does nothing. Only useful as a placeholder for control edges.</p> <p><a href="../nondifferentiable_batch_function"><code translate="no" dir="ltr">nondifferentiable_batch_function(...)</code></a>: Batches the computation done by the decorated function.</p> <p><a href="../norm"><code translate="no" dir="ltr">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors.</p> <p><a href="../math/not_equal"><code translate="no" dir="ltr">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href="../numpy_function"><code translate="no" dir="ltr">numpy_function(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="../one_hot"><code translate="no" dir="ltr">one_hot(...)</code></a>: Returns a one-hot tensor.</p> <p><a href="../ones"><code translate="no" dir="ltr">ones(...)</code></a>: Creates a tensor with all elements set to one (1).</p> <p><a href="../ones_like"><code translate="no" dir="ltr">ones_like(...)</code></a>: Creates a tensor with all elements set to one.</p> <p><a href="../pad"><code translate="no" dir="ltr">pad(...)</code></a>: Pads a tensor.</p> <p><a href="../parallel_stack"><code translate="no" dir="ltr">parallel_stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor in parallel.</p> <p><a href="../math/pow"><code translate="no" dir="ltr">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href="../print"><code translate="no" dir="ltr">print(...)</code></a>: Print the specified inputs.</p> <p><a href="../py_function"><code translate="no" dir="ltr">py_function(...)</code></a>: Wraps a python function into a TensorFlow op that executes it eagerly.</p> <p><a href="../range"><code translate="no" dir="ltr">range(...)</code></a>: Creates a sequence of numbers.</p> <p><a href="../rank"><code translate="no" dir="ltr">rank(...)</code></a>: Returns the rank of a tensor.</p> <p><a href="../realdiv"><code translate="no" dir="ltr">realdiv(...)</code></a>: Returns x / y element-wise for real types.</p> <p><a href="../recompute_grad"><code translate="no" dir="ltr">recompute_grad(...)</code></a>: An eager-compatible version of recompute_grad.</p> <p><a href="../reduce_all"><code translate="no" dir="ltr">reduce_all(...)</code></a>: Computes the "logical and" of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_any"><code translate="no" dir="ltr">reduce_any(...)</code></a>: Computes the "logical or" of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_logsumexp"><code translate="no" dir="ltr">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))).</p> <p><a href="../math/reduce_max"><code translate="no" dir="ltr">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_mean"><code translate="no" dir="ltr">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_min"><code translate="no" dir="ltr">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_prod"><code translate="no" dir="ltr">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor.</p> <p><a href="../math/reduce_sum"><code translate="no" dir="ltr">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor.</p> <p><a href="../register_tensor_conversion_function"><code translate="no" dir="ltr">register_tensor_conversion_function(...)</code></a>: Registers a function for converting objects of <code translate="no" dir="ltr">base_type</code> to <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="../repeat"><code translate="no" dir="ltr">repeat(...)</code></a>: Repeat elements of <code translate="no" dir="ltr">input</code>.</p> <p><a href="../required_space_to_batch_paddings"><code translate="no" dir="ltr">required_space_to_batch_paddings(...)</code></a>: Calculate padding required to make block_shape divide input_shape.</p> <p><a href="../reshape"><code translate="no" dir="ltr">reshape(...)</code></a>: Reshapes a tensor.</p> <p><a href="../reverse"><code translate="no" dir="ltr">reverse(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="../reverse_sequence"><code translate="no" dir="ltr">reverse_sequence(...)</code></a>: Reverses variable length slices. (deprecated arguments) (deprecated arguments)</p> <p><a href="../roll"><code translate="no" dir="ltr">roll(...)</code></a>: Rolls the elements of a tensor along an axis.</p> <p><a href="../math/round"><code translate="no" dir="ltr">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href="../dtypes/saturate_cast"><code translate="no" dir="ltr">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate="no" dir="ltr">value</code> to <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="../math/scalar_mul"><code translate="no" dir="ltr">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code> object.</p> <p><a href="../scan"><code translate="no" dir="ltr">scan(...)</code></a>: scan on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../scatter_nd"><code translate="no" dir="ltr">scatter_nd(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into a new tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../searchsorted"><code translate="no" dir="ltr">searchsorted(...)</code></a>: Searches input tensor for values on the innermost dimension.</p> <p><a href="../sequence_mask"><code translate="no" dir="ltr">sequence_mask(...)</code></a>: Returns a mask tensor representing the first N positions of each cell.</p> <p><a href="../shape"><code translate="no" dir="ltr">shape(...)</code></a>: Returns the shape of a tensor.</p> <p><a href="../shape_n"><code translate="no" dir="ltr">shape_n(...)</code></a>: Returns shape of tensors.</p> <p><a href="../math/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Computes sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../math/sign"><code translate="no" dir="ltr">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href="../math/sin"><code translate="no" dir="ltr">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href="../math/sinh"><code translate="no" dir="ltr">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href="../size"><code translate="no" dir="ltr">size(...)</code></a></p> <p><a href="../slice"><code translate="no" dir="ltr">slice(...)</code></a>: Extracts a slice from a tensor.</p> <p><a href="../sort"><code translate="no" dir="ltr">sort(...)</code></a>: Sorts a tensor.</p> <p><a href="../space_to_batch"><code translate="no" dir="ltr">space_to_batch(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="../space_to_batch_nd"><code translate="no" dir="ltr">space_to_batch_nd(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="../split"><code translate="no" dir="ltr">split(...)</code></a>: Splits a tensor into sub tensors.</p> <p><a href="../math/sqrt"><code translate="no" dir="ltr">sqrt(...)</code></a>: Computes square root of x element-wise.</p> <p><a href="../math/square"><code translate="no" dir="ltr">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href="../squeeze"><code translate="no" dir="ltr">squeeze(...)</code></a>: Removes dimensions of size 1 from the shape of a tensor.</p> <p><a href="../stack"><code translate="no" dir="ltr">stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor.</p> <p><a href="../stop_gradient"><code translate="no" dir="ltr">stop_gradient(...)</code></a>: Stops gradient computation.</p> <p><a href="../strided_slice"><code translate="no" dir="ltr">strided_slice(...)</code></a>: Extracts a strided slice of a tensor (generalized python array indexing).</p> <p><a href="../math/subtract"><code translate="no" dir="ltr">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href="../switch_case"><code translate="no" dir="ltr">switch_case(...)</code></a>: Create a switch/case operation, i.e. an integer-indexed conditional.</p> <p><a href="../math/tan"><code translate="no" dir="ltr">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href="../math/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="../tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_nd_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_nd_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_nd_update(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="../tensordot"><code translate="no" dir="ltr">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href="../tile"><code translate="no" dir="ltr">tile(...)</code></a>: Constructs a tensor by tiling a given tensor.</p> <p><a href="../timestamp"><code translate="no" dir="ltr">timestamp(...)</code></a>: Provides the time since epoch in seconds.</p> <p><a href="../transpose"><code translate="no" dir="ltr">transpose(...)</code></a>: Transposes <code translate="no" dir="ltr">a</code>.</p> <p><a href="../math/truediv"><code translate="no" dir="ltr">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href="../truncatediv"><code translate="no" dir="ltr">truncatediv(...)</code></a>: Returns x / y element-wise for integer types.</p> <p><a href="../truncatemod"><code translate="no" dir="ltr">truncatemod(...)</code></a>: Returns element-wise remainder of division. This emulates C semantics in that</p> <p><a href="../tuple"><code translate="no" dir="ltr">tuple(...)</code></a>: Group tensors together.</p> <p><a href="../unique"><code translate="no" dir="ltr">unique(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="../unique_with_counts"><code translate="no" dir="ltr">unique_with_counts(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="../unravel_index"><code translate="no" dir="ltr">unravel_index(...)</code></a>: Converts an array of flat indices into a tuple of coordinate arrays.</p> <p><a href="../unstack"><code translate="no" dir="ltr">unstack(...)</code></a>: Unpacks the given dimension of a rank-<code translate="no" dir="ltr">R</code> tensor into rank-<code translate="no" dir="ltr">(R-1)</code> tensors.</p> <p><a href="../variable_creator_scope"><code translate="no" dir="ltr">variable_creator_scope(...)</code></a>: Scope which defines a variable creation function to be used by variable().</p> <p><a href="../vectorized_map"><code translate="no" dir="ltr">vectorized_map(...)</code></a>: Parallel map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="../where"><code translate="no" dir="ltr">where(...)</code></a>: Return the elements, either from <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code>, depending on the <code translate="no" dir="ltr">condition</code>.</p> <p><a href="../while_loop"><code translate="no" dir="ltr">while_loop(...)</code></a>: Repeat <code translate="no" dir="ltr">body</code> while the condition <code translate="no" dir="ltr">cond</code> is true.</p> <p><a href="../zeros"><code translate="no" dir="ltr">zeros(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="../zeros_like"><code translate="no" dir="ltr">zeros_like(...)</code></a>: Creates a tensor with all elements set to zero.</p> <h2 id="other_members">Other Members</h2> <ul> <li>
<code translate="no" dir="ltr">__version__ = '2.1.0'</code> 
</li> <li>
<code translate="no" dir="ltr">bfloat16</code> 
</li> <li>
<code translate="no" dir="ltr">bool</code> 
</li> <li>
<code translate="no" dir="ltr">complex128</code> 
</li> <li>
<code translate="no" dir="ltr">complex64</code> 
</li> <li>
<code translate="no" dir="ltr">double</code> 
</li> <li>
<code translate="no" dir="ltr">float16</code> 
</li> <li>
<code translate="no" dir="ltr">float32</code> 
</li> <li>
<code translate="no" dir="ltr">float64</code> 
</li> <li>
<code translate="no" dir="ltr">half</code> 
</li> <li>
<code translate="no" dir="ltr">int16</code> 
</li> <li>
<code translate="no" dir="ltr">int32</code> 
</li> <li>
<code translate="no" dir="ltr">int64</code> 
</li> <li>
<code translate="no" dir="ltr">int8</code> 
</li> <li>
<code translate="no" dir="ltr">qint16</code> 
</li> <li>
<code translate="no" dir="ltr">qint32</code> 
</li> <li>
<code translate="no" dir="ltr">qint8</code> 
</li> <li>
<code translate="no" dir="ltr">quint16</code> 
</li> <li>
<code translate="no" dir="ltr">quint8</code> 
</li> <li>
<code translate="no" dir="ltr">resource</code> 
</li> <li>
<code translate="no" dir="ltr">string</code> 
</li> <li>
<code translate="no" dir="ltr">uint16</code> 
</li> <li>
<code translate="no" dir="ltr">uint32</code> 
</li> <li>
<code translate="no" dir="ltr">uint64</code> 
</li> <li>
<code translate="no" dir="ltr">uint8</code> 
</li> <li>
<code translate="no" dir="ltr">variant</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v2" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v2</a>
  </p>
</div>
