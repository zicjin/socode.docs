<h1 class="devsite-page-title">tf.compat.v1.ConditionalAccumulator</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.ConditionalAccumulator"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="accumulator_ref"> <meta itemprop="property" content="dtype"> <meta itemprop="property" content="name"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_grad"> <meta itemprop="property" content="num_accumulated"> <meta itemprop="property" content="set_global_step"> <meta itemprop="property" content="take_grad"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1242-L1329">  View source on GitHub </a> </td>
</table>  <h2 id="class_conditionalaccumulator">Class <code translate="no" dir="ltr">ConditionalAccumulator</code>
</h2> <p>A conditional accumulator for aggregating gradients.</p> <p>Inherits From: <a href="conditionalaccumulatorbase"><code translate="no" dir="ltr">ConditionalAccumulatorBase</code></a></p>  <p>Up-to-date gradients (i.e., time step at which gradient was computed is equal to the accumulator's time step) are added to the accumulator.</p> <p>Extraction of the average gradient is blocked until the required number of gradients has been accumulated.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1252-L1278">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    dtype,
    shape=None,
    shared_name=None,
    name='conditional_accumulator',
    reduction_type='MEAN'
)
</pre> <p>Creates a new ConditionalAccumulator.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">dtype</code></b>: Datatype of the accumulated gradients.</li> <li>
<b><code translate="no" dir="ltr">shape</code></b>: Shape of the accumulated gradients.</li> <li>
<b><code translate="no" dir="ltr">shared_name</code></b>: Optional. If non-empty, this accumulator will be shared under the given name across multiple sessions.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the accumulator.</li> <li>
<b><code translate="no" dir="ltr">reduction_type</code></b>: Reduction type to use when taking the gradient.</li> </ul> <h2 id="properties">Properties</h2> <h3 id="accumulator_ref"><code translate="no" dir="ltr">accumulator_ref</code></h3> <p>The underlying accumulator reference.</p> <h3 id="dtype"><code translate="no" dir="ltr">dtype</code></h3> <p>The datatype of the gradients accumulated by this accumulator.</p> <h3 id="name"><code translate="no" dir="ltr">name</code></h3> <p>The name of the underlying accumulator.</p> <h2 id="methods">Methods</h2> <h3 id="apply_grad"><code translate="no" dir="ltr">apply_grad</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1280-L1302">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply_grad(
    grad,
    local_step=0,
    name=None
)
</pre> <p>Attempts to apply a gradient to the accumulator.</p> <p>The attempt is silently dropped if the gradient is stale, i.e., local_step is less than the accumulator's global time step.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grad</code></b>: The gradient tensor to be applied.</li> <li>
<b><code translate="no" dir="ltr">local_step</code></b>: Time step at which the gradient was computed.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the operation.</li> </ul> <h4 id="returns">Returns:</h4> <p>The operation that (conditionally) applies a gradient to the accumulator.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If grad is of the wrong shape</li> </ul> <h3 id="num_accumulated"><code translate="no" dir="ltr">num_accumulated</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1207-L1220">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">num_accumulated(name=None)
</pre> <p>Number of gradients that have currently been aggregated in accumulator.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the operation.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>Number of accumulated gradients currently in accumulator.</p> <h3 id="set_global_step"><code translate="no" dir="ltr">set_global_step</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1222-L1238">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">set_global_step(
    new_global_step,
    name=None
)
</pre> <p>Sets the global time step of the accumulator.</p> <p>The operation logs a warning if we attempt to set to a time step that is lower than the accumulator's own time step.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">new_global_step</code></b>: Value of new time step. Can be a variable or a constant</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the operation.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>Operation that sets the accumulator's time step.</p> <h3 id="take_grad"><code translate="no" dir="ltr">take_grad</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/data_flow_ops.py#L1304-L1329">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">take_grad(
    num_required,
    name=None
)
</pre> <p>Attempts to extract the average gradient from the accumulator.</p> <p>The operation blocks until sufficient number of gradients have been successfully applied to the accumulator.</p> <p>Once successful, the following actions are also triggered:</p> <ul> <li>Counter of accumulated gradients is reset to 0.</li> <li>Aggregated gradient is reset to 0 tensor.</li> <li>Accumulator's internal time step is incremented by 1.</li> </ul> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">num_required</code></b>: Number of gradients that needs to have been aggregated</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the operation</li> </ul> <h4 id="returns_4">Returns:</h4> <p>A tensor holding the value of the average gradient.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">InvalidArgumentError</code></b>: If num_required &lt; 1</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConditionalAccumulator" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConditionalAccumulator</a>
  </p>
</div>
