<h1 class="devsite-page-title">tf.compat.v1.test.compute_gradient</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.test.compute_gradient"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/gradient_checker.py#L271-L335">  View source on GitHub </a> </td>
</table>  <p>Computes and returns the theoretical and numerical Jacobian. (deprecated)</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.compat.v1.test.compute_gradient(
    x,
    x_shape,
    y,
    y_shape,
    x_init_value=None,
    delta=0.001,
    init_targets=None,
    extra_feed_dict=None
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use tf.test.compute_gradient in 2.0, which has better support for functions. Note that the two versions have different usage, so code change is needed.</span></aside> <p>If <code translate="no" dir="ltr">x</code> or <code translate="no" dir="ltr">y</code> is complex, the Jacobian will still be real but the corresponding Jacobian dimension(s) will be twice as large. This is required even if both input and output is complex since TensorFlow graphs are not necessarily holomorphic, and may have gradients not expressible as complex numbers. For example, if <code translate="no" dir="ltr">x</code> is complex with shape <code translate="no" dir="ltr">[m]</code> and <code translate="no" dir="ltr">y</code> is complex with shape <code translate="no" dir="ltr">[n]</code>, each Jacobian <code translate="no" dir="ltr">J</code> will have shape <code translate="no" dir="ltr">[m * 2, n * 2]</code> with</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">J[:m, :n] = d(Re y)/d(Re x)
J[:m, n:] = d(Im y)/d(Re x)
J[m:, :n] = d(Re y)/d(Im x)
J[m:, n:] = d(Im y)/d(Im x)
</pre> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: a tensor or list of tensors</li> <li>
<b><code translate="no" dir="ltr">x_shape</code></b>: the dimensions of x as a tuple or an array of ints. If x is a list, then this is the list of shapes.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: a tensor</li> <li>
<b><code translate="no" dir="ltr">y_shape</code></b>: the dimensions of y as a tuple or an array of ints.</li> <li>
<b><code translate="no" dir="ltr">x_init_value</code></b>: (optional) a numpy array of the same shape as "x" representing the initial value of x. If x is a list, this should be a list of numpy arrays. If this is none, the function will pick a random tensor as the initial value.</li> <li>
<b><code translate="no" dir="ltr">delta</code></b>: (optional) the amount of perturbation.</li> <li>
<b><code translate="no" dir="ltr">init_targets</code></b>: list of targets to run to initialize model params.</li> <li>
<b><code translate="no" dir="ltr">extra_feed_dict</code></b>: dict that allows fixing specified tensor values during the Jacobian calculation.</li> </ul> <h4 id="returns">Returns:</h4> <p>Two 2-d numpy arrays representing the theoretical and numerical Jacobian for dy/dx. Each has "x_size" rows and "y_size" columns where "x_size" is the number of elements in x and "y_size" is the number of elements in y. If x is a list, returns a list of two numpy arrays.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/test/compute_gradient" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/test/compute_gradient</a>
  </p>
</div>
