<h1 class="devsite-page-title">tf.compat.v1.nn.ctc_loss</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.nn.ctc_loss"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/ctc_ops.py#L47-L178">  View source on GitHub </a> </td>
</table>  <p>Computes the CTC (Connectionist Temporal Classification) Loss.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.compat.v1.nn.ctc_loss(
    labels,
    inputs=None,
    sequence_length=None,
    preprocess_collapse_repeated=False,
    ctc_merge_repeated=True,
    ignore_longer_outputs_than_inputs=False,
    time_major=True,
    logits=None
)
</pre>  <p>This op implements the CTC loss as presented in the article:</p> <p><a href="http://www.cs.toronto.edu/%7Egraves/icml_2006.pdf">A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber. Connectionist Temporal Classification: Labeling Unsegmented Sequence Data with Recurrent Neural Networks. ICML 2006, Pittsburgh, USA, pp. 369-376.</a></p> <h4 id="input_requirements">Input requirements:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">sequence_length(b) &lt;= time for all b

max(labels.indices(labels.indices[:, 1] == b, 2))
  &lt;= sequence_length(b) for all b.
</pre> <h4 id="notes">Notes:</h4> <p>This class performs the softmax operation for you, so inputs should be e.g. linear projections of outputs by an LSTM.</p> <p>The <code translate="no" dir="ltr">inputs</code> Tensor's innermost dimension size, <code translate="no" dir="ltr">num_classes</code>, represents <code translate="no" dir="ltr">num_labels + 1</code> classes, where num_labels is the number of true labels, and the largest value <code translate="no" dir="ltr">(num_classes - 1)</code> is reserved for the blank label.</p> <p>For example, for a vocabulary containing 3 labels <code translate="no" dir="ltr">[a, b, c]</code>, <code translate="no" dir="ltr">num_classes = 4</code> and the labels indexing is <code translate="no" dir="ltr">{a: 0, b: 1, c: 2, blank: 3}</code>.</p> <p>Regarding the arguments <code translate="no" dir="ltr">preprocess_collapse_repeated</code> and <code translate="no" dir="ltr">ctc_merge_repeated</code>:</p> <p>If <code translate="no" dir="ltr">preprocess_collapse_repeated</code> is True, then a preprocessing step runs before loss calculation, wherein repeated labels passed to the loss are merged into single labels. This is useful if the training labels come from, e.g., forced alignments and therefore have unnecessary repetitions.</p> <p>If <code translate="no" dir="ltr">ctc_merge_repeated</code> is set False, then deep within the CTC calculation, repeated non-blank labels will not be merged and are interpreted as individual labels. This is a simplified (non-standard) version of CTC.</p> <p>Here is a table of the (roughly) expected first order behavior:</p> <ul> <li>
<p><code translate="no" dir="ltr">preprocess_collapse_repeated=False</code>, <code translate="no" dir="ltr">ctc_merge_repeated=True</code></p> <p>Classical CTC behavior: Outputs true repeated classes with blanks in between, and can also output repeated classes with no blanks in between that need to be collapsed by the decoder.</p>
</li> <li>
<p><code translate="no" dir="ltr">preprocess_collapse_repeated=True</code>, <code translate="no" dir="ltr">ctc_merge_repeated=False</code></p> <p>Never learns to output repeated classes, as they are collapsed in the input labels before training.</p>
</li> <li>
<p><code translate="no" dir="ltr">preprocess_collapse_repeated=False</code>, <code translate="no" dir="ltr">ctc_merge_repeated=False</code></p> <p>Outputs repeated classes with blanks in between, but generally does not require the decoder to collapse/merge repeated classes.</p>
</li> <li>
<p><code translate="no" dir="ltr">preprocess_collapse_repeated=True</code>, <code translate="no" dir="ltr">ctc_merge_repeated=True</code></p> <p>Untested. Very likely will not learn to output repeated classes.</p>
</li> </ul> <p>The <code translate="no" dir="ltr">ignore_longer_outputs_than_inputs</code> option allows to specify the behavior of the CTCLoss when dealing with sequences that have longer outputs than inputs. If true, the CTCLoss will simply return zero gradient for those items, otherwise an InvalidArgument error is returned, stopping training.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">labels</code></b>: An <code translate="no" dir="ltr">int32</code> <code translate="no" dir="ltr">SparseTensor</code>. <code translate="no" dir="ltr">labels.indices[i, :] == [b, t]</code> means <code translate="no" dir="ltr">labels.values[i]</code> stores the id for (batch b, time t). <code translate="no" dir="ltr">labels.values[i]</code> must take on values in <code translate="no" dir="ltr">[0, num_labels)</code>. See <code translate="no" dir="ltr">core/ops/ctc_ops.cc</code> for more details.</li> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: 3-D <code translate="no" dir="ltr">float</code> <code translate="no" dir="ltr">Tensor</code>. If time_major == False, this will be a <code translate="no" dir="ltr">Tensor</code> shaped: <code translate="no" dir="ltr">[batch_size, max_time, num_classes]</code>. If time_major == True (default), this will be a <code translate="no" dir="ltr">Tensor</code> shaped: <code translate="no" dir="ltr">[max_time, batch_size, num_classes]</code>. The logits.</li> <li>
<b><code translate="no" dir="ltr">sequence_length</code></b>: 1-D <code translate="no" dir="ltr">int32</code> vector, size <code translate="no" dir="ltr">[batch_size]</code>. The sequence lengths.</li> <li>
<b><code translate="no" dir="ltr">preprocess_collapse_repeated</code></b>: Boolean. Default: False. If True, repeated labels are collapsed prior to the CTC calculation.</li> <li>
<b><code translate="no" dir="ltr">ctc_merge_repeated</code></b>: Boolean. Default: True.</li> <li>
<b><code translate="no" dir="ltr">ignore_longer_outputs_than_inputs</code></b>: Boolean. Default: False. If True, sequences with longer outputs than inputs will be ignored.</li> <li>
<b><code translate="no" dir="ltr">time_major</code></b>: The shape format of the <code translate="no" dir="ltr">inputs</code> Tensors. If True, these <code translate="no" dir="ltr">Tensors</code> must be shaped <code translate="no" dir="ltr">[max_time, batch_size, num_classes]</code>. If False, these <code translate="no" dir="ltr">Tensors</code> must be shaped <code translate="no" dir="ltr">[batch_size, max_time, num_classes]</code>. Using <code translate="no" dir="ltr">time_major = True</code> (default) is a bit more efficient because it avoids transposes at the beginning of the ctc_loss calculation. However, most TensorFlow data is batch-major, so by this function also accepts inputs in batch-major form.</li> <li>
<b><code translate="no" dir="ltr">logits</code></b>: Alias for inputs.</li> </ul> <h4 id="returns">Returns:</h4> <p>A 1-D <code translate="no" dir="ltr">float</code> <code translate="no" dir="ltr">Tensor</code>, size <code translate="no" dir="ltr">[batch]</code>, containing the negative log probabilities.</p> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: if labels is not a <code translate="no" dir="ltr">SparseTensor</code>.</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/ctc_loss" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/ctc_loss</a>
  </p>
</div>
