<h1 class="devsite-page-title">tf.compat.v1.nn.embedding_lookup_sparse</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.nn.embedding_lookup_sparse"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/embedding_ops.py#L373-L544">  View source on GitHub </a> </td>
</table>  <p>Computes embeddings for the given ids and weights.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.compat.v1.nn.embedding_lookup_sparse(
    params,
    sp_ids,
    sp_weights,
    partition_strategy='mod',
    name=None,
    combiner=None,
    max_norm=None
)
</pre>  <p>This op assumes that there is at least one id for each row in the dense tensor represented by sp_ids (i.e. there are no rows with empty features), and that all the indices of sp_ids are in canonical row-major order.</p> <p>It also assumes that all id values lie in the range [0, p0), where p0 is the sum of the size of params along dimension 0.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">params</code></b>: A single tensor representing the complete embedding tensor, or a list of P tensors all of same shape except for the first dimension, representing sharded embedding tensors. Alternatively, a <code translate="no" dir="ltr">PartitionedVariable</code>, created by partitioning along dimension 0. Each element must be appropriately sized for the given <code translate="no" dir="ltr">partition_strategy</code>.</li> <li>
<b><code translate="no" dir="ltr">sp_ids</code></b>: N x M <code translate="no" dir="ltr">SparseTensor</code> of int64 ids where N is typically batch size and M is arbitrary.</li> <li>
<b><code translate="no" dir="ltr">sp_weights</code></b>: either a <code translate="no" dir="ltr">SparseTensor</code> of float / double weights, or <code translate="no" dir="ltr">None</code> to indicate all weights should be taken to be 1. If specified, <code translate="no" dir="ltr">sp_weights</code> must have exactly the same shape and indices as <code translate="no" dir="ltr">sp_ids</code>.</li> <li>
<b><code translate="no" dir="ltr">partition_strategy</code></b>: A string specifying the partitioning strategy, relevant if <code translate="no" dir="ltr">len(params) &gt; 1</code>. Currently <code translate="no" dir="ltr">"div"</code> and <code translate="no" dir="ltr">"mod"</code> are supported. Default is <code translate="no" dir="ltr">"mod"</code>. See <code translate="no" dir="ltr">tf.nn.embedding_lookup</code> for more details.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the op.</li> <li>
<b><code translate="no" dir="ltr">combiner</code></b>: A string specifying the reduction op. Currently "mean", "sqrtn" and "sum" are supported. "sum" computes the weighted sum of the embedding results for each row. "mean" is the weighted sum divided by the total weight. "sqrtn" is the weighted sum divided by the square root of the sum of the squares of the weights.</li> <li>
<b><code translate="no" dir="ltr">max_norm</code></b>: If not <code translate="no" dir="ltr">None</code>, each embedding is clipped if its l2-norm is larger than this value, before combining.</li> </ul> <h4 id="returns">Returns:</h4> <p>A dense tensor representing the combined embeddings for the sparse ids. For each row in the dense tensor represented by <code translate="no" dir="ltr">sp_ids</code>, the op looks up the embeddings for all ids in that row, multiplies them by the corresponding weight, and combines these embeddings as specified.</p> <p>In other words, if</p> <p><code translate="no" dir="ltr">shape(combined params) = [p0, p1, ..., pm]</code></p> <p>and</p> <p><code translate="no" dir="ltr">shape(sp_ids) = shape(sp_weights) = [d0, d1, ..., dn]</code></p> <p>then</p> <p><code translate="no" dir="ltr">shape(output) = [d0, d1, ..., dn-1, p1, ..., pm]</code>.</p> <p>For instance, if params is a 10x20 matrix, and sp_ids / sp_weights are</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">[0, 0]: id 1, weight 2.0
[0, 1]: id 3, weight 0.5
[1, 0]: id 0, weight 1.0
[2, 3]: id 1, weight 3.0
</pre> <p>with <code translate="no" dir="ltr">combiner</code>="mean", then the output will be a 3x20 matrix where</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">output[0, :] = (params[1, :] * 2.0 + params[3, :] * 0.5) / (2.0 + 0.5)
output[1, :] = (params[0, :] * 1.0) / 1.0
output[2, :] = (params[1, :] * 3.0) / 3.0
</pre> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If <code translate="no" dir="ltr">sp_ids</code> is not a <code translate="no" dir="ltr">SparseTensor</code>, or if <code translate="no" dir="ltr">sp_weights</code> is neither <code translate="no" dir="ltr">None</code> nor <code translate="no" dir="ltr">SparseTensor</code>.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">combiner</code> is not one of {"mean", "sqrtn", "sum"}.</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/embedding_lookup_sparse" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/embedding_lookup_sparse</a>
  </p>
</div>
