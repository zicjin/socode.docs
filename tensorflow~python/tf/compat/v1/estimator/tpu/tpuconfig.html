<h1 class="devsite-page-title">tf.compat.v1.estimator.tpu.TPUConfig</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.estimator.tpu.TPUConfig"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="iterations_per_loop"> <meta itemprop="property" content="num_shards"> <meta itemprop="property" content="num_cores_per_replica"> <meta itemprop="property" content="per_host_input_for_training"> <meta itemprop="property" content="tpu_job_name"> <meta itemprop="property" content="initial_infeed_sleep_secs"> <meta itemprop="property" content="input_partition_dims"> <meta itemprop="property" content="eval_training_input_configuration"> <meta itemprop="property" content="experimental_host_call_every_n_steps"> <meta itemprop="property" content="__new__"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/tpu/tpu_config.py">  View source on GitHub </a> </td>
</table>  <h2 id="class_tpuconfig">Class <code translate="no" dir="ltr">TPUConfig</code>
</h2> <p>TPU related configuration required by <code translate="no" dir="ltr">TPUEstimator</code>.</p>  <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">iterations_per_loop</code></b>: This is the number of train steps running in TPU system before returning to CPU host for each <code translate="no" dir="ltr">Session.run</code>. This means global step is increased <code translate="no" dir="ltr">iterations_per_loop</code> times in one <code translate="no" dir="ltr">Session.run</code>. It is recommended to be set as number of global steps for next checkpoint. Note that in evaluation don't use this value, instead we run total eval <code translate="no" dir="ltr">steps</code> on TPU for a single <code translate="no" dir="ltr">Session.run</code>. [Experimental]: <code translate="no" dir="ltr">iterations_per_loop</code> can be specified as a time interval. To specify N seconds in one <code translate="no" dir="ltr">Session.run</code>, one can specify it as <code translate="no" dir="ltr">Ns</code> and substitute the N with the N with the number of desired seconds. Alternatively, the unit of time can also be specified in minutes or hours, e.g. <code translate="no" dir="ltr">3600s</code> or <code translate="no" dir="ltr">60m</code> or <code translate="no" dir="ltr">1h</code>.</li> <li>
<b><code translate="no" dir="ltr">num_shards</code></b>: (Deprecated, ignored by TPUEstimator). The number of model replicas in the system. For non-model-parallelism case, this number equals the total number of TPU cores. For model-parallelism, the total number of TPU cores equals num_cores_per_replica * num_shards.</li> <li>
<b><code translate="no" dir="ltr">num_cores_per_replica</code></b>: Defaults to <code translate="no" dir="ltr">None</code>, which disables model parallelism. An integer which describes the number of TPU cores per model replica. This is required by model-parallelism which enables partitioning the model to multiple cores. Currently num_cores_per_replica must be 1, 2, 4, or 8.</li> <li>
<b><code translate="no" dir="ltr">per_host_input_for_training</code></b>: If <code translate="no" dir="ltr">True</code>, for <code translate="no" dir="ltr">PER_HOST_V1</code>, the <code translate="no" dir="ltr">input_fn</code> is invoked once on each host, and the number of hosts must be smaller or equal to the number of replicas. For PER_HOST_V2, the <code translate="no" dir="ltr">input_fn</code> is invoked once for each host (if the number of hosts is less than the number of replicas) or replica (if the number of replicas is less than the number of hosts. With the per-core input pipeline configuration, it is invoked once for each core. With a global batch size <code translate="no" dir="ltr">train_batch_size</code> in <code translate="no" dir="ltr">TPUEstimator</code> constructor, the batch size for each shard is <code translate="no" dir="ltr">train_batch_size</code> // #hosts in the <code translate="no" dir="ltr">True</code> or <code translate="no" dir="ltr">PER_HOST_V1</code> mode. In <code translate="no" dir="ltr">PER_HOST_V2</code> mode, it is <code translate="no" dir="ltr">train_batch_size</code> // #cores. In <code translate="no" dir="ltr">BROADCAST</code> mode, <code translate="no" dir="ltr">input_fn</code> is only invoked once on host 0 and the tensors are broadcasted to all other replicas. The batch size equals to <code translate="no" dir="ltr">train_batch_size</code>. With the per-core input pipeline configuration, the shard batch size is also <code translate="no" dir="ltr">train_batch_size</code> // #cores. Note: per_host_input_for_training==PER_SHARD_V1 only supports mode.TRAIN.</li> <li>
<b><code translate="no" dir="ltr">tpu_job_name</code></b>: The name of the TPU job. Typically, this name is auto-inferred within TPUEstimator, however when using ClusterSpec propagation in more esoteric cluster configurations, you may need to specify the job name as a string.</li> <li>
<b><code translate="no" dir="ltr">initial_infeed_sleep_secs</code></b>: The number of seconds the infeed thread should wait before enqueueing the first batch. This helps avoid timeouts for models that require a long compilation time.</li> <li>
<b><code translate="no" dir="ltr">input_partition_dims</code></b>: A nested list to describe the partition dims for all the tensors from input_fn(). The structure of input_partition_dims must match the structure of <code translate="no" dir="ltr">features</code> and <code translate="no" dir="ltr">labels</code> from input_fn(). The total number of partitions must match <code translate="no" dir="ltr">num_cores_per_replica</code>. For example, if input_fn() returns two tensors: images with shape [N, H, W, C] and labels [N]. input_partition_dims = [[1, 2, 2, 1], None] will split the images to 4 pieces and feed into 4 TPU cores. labels tensor are directly broadcasted to all the TPU cores since the partition dims is <code translate="no" dir="ltr">None</code>. Current limitations: This feature is only supported with the PER_HOST_V2 input mode.</li> <li>
<b><code translate="no" dir="ltr">eval_training_input_configuration</code></b>: If <code translate="no" dir="ltr">SLICED</code>, <code translate="no" dir="ltr">input_fn</code> is only invoked once on host 0 and the tensors are broadcasted to all other replicas. Unlike per_host_input_for_training=BROADCAST, each replica will only get a slice of the data instead of a whole copy. If <code translate="no" dir="ltr">PER_HOST_V1</code>, the behaviour is determined by per_host_input_for_training.</li> <li>
<b><code translate="no" dir="ltr">experimental_host_call_every_n_steps</code></b>: Within a training loop, this argument sets how often host calls are performed during training. Host calls will be evaluated every n steps within a training loop where n is the value of this argument.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If <code translate="no" dir="ltr">num_cores_per_replica</code> is not 1, 2, 4, 8, ..., 128.</li> </ul> <h2 id="__new__"><code translate="no" dir="ltr">__new__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/tpu/tpu_config.py">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@staticmethod
__new__(
    cls,
    iterations_per_loop=2,
    num_shards=None,
    num_cores_per_replica=None,
    per_host_input_for_training=True,
    tpu_job_name=None,
    initial_infeed_sleep_secs=None,
    input_partition_dims=None,
    eval_training_input_configuration=InputPipelineConfig.PER_HOST_V1,
    experimental_host_call_every_n_steps=1
)
</pre> <p>Create new instance of TPUConfig(iterations_per_loop, num_shards, num_cores_per_replica, per_host_input_for_training, tpu_job_name, initial_infeed_sleep_secs, input_partition_dims, eval_training_input_configuration, experimental_host_call_every_n_steps)</p> <h2 id="properties">Properties</h2> <h3 id="iterations_per_loop"><code translate="no" dir="ltr">iterations_per_loop</code></h3> <h3 id="num_shards"><code translate="no" dir="ltr">num_shards</code></h3> <h3 id="num_cores_per_replica"><code translate="no" dir="ltr">num_cores_per_replica</code></h3> <h3 id="per_host_input_for_training"><code translate="no" dir="ltr">per_host_input_for_training</code></h3> <h3 id="tpu_job_name"><code translate="no" dir="ltr">tpu_job_name</code></h3> <h3 id="initial_infeed_sleep_secs"><code translate="no" dir="ltr">initial_infeed_sleep_secs</code></h3> <h3 id="input_partition_dims"><code translate="no" dir="ltr">input_partition_dims</code></h3> <h3 id="eval_training_input_configuration"><code translate="no" dir="ltr">eval_training_input_configuration</code></h3> <h3 id="experimental_host_call_every_n_steps"><code translate="no" dir="ltr">experimental_host_call_every_n_steps</code></h3>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/tpu/TPUConfig" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/estimator/tpu/TPUConfig</a>
  </p>
</div>
