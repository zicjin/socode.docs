<h1 class="devsite-page-title">tf.compat.v1.tpu.CrossShardOptimizer</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.tpu.CrossShardOptimizer"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="compute_gradients"> <meta itemprop="property" content="get_name"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="variables"> <meta itemprop="property" content="GATE_GRAPH"> <meta itemprop="property" content="GATE_NONE"> <meta itemprop="property" content="GATE_OP"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L34-L224">  View source on GitHub </a> </td>
</table>  <h2 id="class_crossshardoptimizer">Class <code translate="no" dir="ltr">CrossShardOptimizer</code>
</h2> <p>An optimizer that averages gradients across TPU shards.</p> <p>Inherits From: <a href="../train/optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p>  <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L37-L70">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    opt,
    reduction=losses.Reduction.MEAN,
    name='CrossShardOptimizer',
    group_assignment=None
)
</pre> <p>Construct a new cross-shard optimizer.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">opt</code></b>: An existing <code translate="no" dir="ltr">Optimizer</code> to encapsulate.</li> <li>
<b><code translate="no" dir="ltr">reduction</code></b>: The reduction to apply to the shard losses.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name prefix for the operations created when applying gradients. Defaults to "CrossShardOptimizer".</li> <li>
<b><code translate="no" dir="ltr">group_assignment</code></b>: Optional 2d int32 lists with shape [num_groups, num_replicas_per_group] which describles how to apply optimizer to subgroups.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If reduction is not a valid cross-shard reduction.</li> </ul> <h2 id="methods">Methods</h2> <h3 id="apply_gradients"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L163-L192">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply_gradients(
    grads_and_vars,
    global_step=None,
    name=None
)
</pre> <p>Apply gradients to variables.</p> <p>Calls tpu_ops.cross_replica_sum() to sum gradient contributions across replicas, and then applies the real optimizer.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads_and_vars</code></b>: List of (gradient, variable) pairs as returned by compute_gradients().</li> <li>
<b><code translate="no" dir="ltr">global_step</code></b>: Optional Variable to increment by one after the variables have been updated.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation. Default to the name passed to the Optimizer constructor.</li> </ul> <h4 id="returns">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that applies the gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If the grads_and_vars is malformed.</li> </ul> <h3 id="compute_gradients"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L111-L161">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">compute_gradients(
    loss,
    var_list=None,
    **kwargs
)
</pre> <p>Compute gradients of "loss" for the variables in "var_list".</p> <p>This simply wraps <code translate="no" dir="ltr">compute_gradients()</code> from the real optimizer. The gradients will be aggregated in <code translate="no" dir="ltr">apply_gradients()</code> so that user can modify the gradients like clipping with per replica global norm if needed. The global norm with aggregated gradients can be bad as one replica's huge gradients can hurt the gradients from other replicas.</p> <p>When the CrossShardOptimizer is constructed with <code translate="no" dir="ltr">reduction == losses.Reduction.MEAN</code> (default), this function scales the loss by <code translate="no" dir="ltr">1.0 / num_shards</code> before computing the gradients. Assuming the optimizer uses the default implementation of <code translate="no" dir="ltr">compute_gradients()</code>, the gradients of the scaled loss are scaled by <code translate="no" dir="ltr">1.0 / num_shards</code> compared to the gradients of the original loss. This scaling factor is important because <code translate="no" dir="ltr">apply_gradients()</code> sums gradients across shards, rather than averaging them. However, the scaling factor must be taken into account when clipping the norm of the gradients or performing other postprocessing.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A Tensor containing the value to minimize.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: Optional list or tuple of <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKey.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code translate="no" dir="ltr">**kwargs</code></b>: Keyword arguments for compute_gradients().</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A list of (gradient, variable) pairs.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If not within a tpu_shard_context or group_assignment is invalid.</li> </ul> <h3 id="get_name"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L352-L353">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_name()
</pre> <h3 id="get_slot"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L194-L206">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot(
    *args,
    **kwargs
)
</pre> <p>Return a slot named "name" created for "var" by the Optimizer.</p> <p>This simply wraps the get_slot() from the actual optimizer.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">*args</code></b>: Arguments for get_slot().</li> <li>
<b><code translate="no" dir="ltr">**kwargs</code></b>: Keyword arguments for get_slot().</li> </ul> <h4 id="returns_3">Returns:</h4> <p>The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise.</p> <h3 id="get_slot_names"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L208-L220">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot_names(
    *args,
    **kwargs
)
</pre> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>This simply wraps the get_slot_names() from the actual optimizer.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">*args</code></b>: Arguments for get_slot().</li> <li>
<b><code translate="no" dir="ltr">**kwargs</code></b>: Keyword arguments for get_slot().</li> </ul> <h4 id="returns_4">Returns:</h4> <p>A list of strings.</p> <h3 id="minimize"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L355-L413">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">minimize(
    loss,
    global_step=None,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    name=None,
    grad_loss=None
)
</pre> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize.</li> <li>
<b><code translate="no" dir="ltr">global_step</code></b>: Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code translate="no" dir="ltr">gate_gradients</code></b>: How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>.</li> <li>
<b><code translate="no" dir="ltr">aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>.</li> <li>
<b><code translate="no" dir="ltr">colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation.</li> <li>
<b><code translate="no" dir="ltr">grad_loss</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</li> </ul> <h4 id="returns_5">Returns:</h4> <p>An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>.</p> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects.</li> </ul> <h4 id="eager_compatibility">Eager Compatibility</h4> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p> <h3 id="variables"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/tpu/tpu_optimizer.py#L222-L224">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables()
</pre> <p>Forwarding the variables from the underlying optimizer.</p> <h2 id="class_members">Class Members</h2> <ul> <li>
<code translate="no" dir="ltr">GATE_GRAPH = 2</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_NONE = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_OP = 1</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/tpu/CrossShardOptimizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/tpu/CrossShardOptimizer</a>
  </p>
</div>
