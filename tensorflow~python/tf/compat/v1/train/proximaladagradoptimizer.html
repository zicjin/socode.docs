<h1 class="devsite-page-title">tf.compat.v1.train.ProximalAdagradOptimizer</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.train.ProximalAdagradOptimizer"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="compute_gradients"> <meta itemprop="property" content="get_name"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="variables"> <meta itemprop="property" content="GATE_GRAPH"> <meta itemprop="property" content="GATE_NONE"> <meta itemprop="property" content="GATE_OP"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/proximal_adagrad.py#L30-L121">  View source on GitHub </a> </td>
</table>  <h2 id="class_proximaladagradoptimizer">Class <code translate="no" dir="ltr">ProximalAdagradOptimizer</code>
</h2> <p>Optimizer that implements the Proximal Adagrad algorithm.</p> <p>Inherits From: <a href="optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p>  <p>See this <a href="http://papers.nips.cc/paper/3793-efficient-learning-using-forward-backward-splitting.pdf">paper</a>.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/proximal_adagrad.py#L37-L68">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    learning_rate,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='ProximalAdagrad'
)
</pre> <p>Construct a new ProximalAdagrad optimizer.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">learning_rate</code></b>: A <code translate="no" dir="ltr">Tensor</code> or a floating point value. The learning rate.</li> <li>
<b><code translate="no" dir="ltr">initial_accumulator_value</code></b>: A floating point value. Starting value for the accumulators, must be positive.</li> <li>
<b><code translate="no" dir="ltr">l1_regularization_strength</code></b>: A float value, must be greater than or equal to zero.</li> <li>
<b><code translate="no" dir="ltr">l2_regularization_strength</code></b>: A float value, must be greater than or equal to zero.</li> <li>
<b><code translate="no" dir="ltr">use_locking</code></b>: If <code translate="no" dir="ltr">True</code> use locks for update operations.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name prefix for the operations created when applying gradients. Defaults to "Adagrad".</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If the <code translate="no" dir="ltr">initial_accumulator_value</code> is invalid.</li> </ul> <h2 id="methods">Methods</h2> <h3 id="apply_gradients"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L531-L638">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply_gradients(
    grads_and_vars,
    global_step=None,
    name=None
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads_and_vars</code></b>: List of (gradient, variable) pairs as returned by <code translate="no" dir="ltr">compute_gradients()</code>.</li> <li>
<b><code translate="no" dir="ltr">global_step</code></b>: Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor.</li> </ul> <h4 id="returns">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. If <code translate="no" dir="ltr">global_step</code> was not None, that operation also increments <code translate="no" dir="ltr">global_step</code>.</p> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If <code translate="no" dir="ltr">grads_and_vars</code> is malformed.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If none of the variables have gradients.</li> <li>
<b><code translate="no" dir="ltr">RuntimeError</code></b>: If you should use <code translate="no" dir="ltr">_distributed_apply()</code> instead.</li> </ul> <h3 id="compute_gradients"><code translate="no" dir="ltr">compute_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L415-L519">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
</pre> <p>Compute gradients of <code translate="no" dir="ltr">loss</code> for the variables in <code translate="no" dir="ltr">var_list</code>.</p> <p>This is the first part of <code translate="no" dir="ltr">minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code translate="no" dir="ltr">Tensor</code>, an <code translate="no" dir="ltr">IndexedSlices</code>, or <code translate="no" dir="ltr">None</code> if there is no gradient for the given variable.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: Optional list or tuple of <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code translate="no" dir="ltr">gate_gradients</code></b>: How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>.</li> <li>
<b><code translate="no" dir="ltr">aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>.</li> <li>
<b><code translate="no" dir="ltr">colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code translate="no" dir="ltr">grad_loss</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code translate="no" dir="ltr">None</code>.</p> <h4 id="raises_3">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If <code translate="no" dir="ltr">var_list</code> contains anything else than <code translate="no" dir="ltr">Variable</code> objects.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If some arguments are invalid.</li> <li>
<b><code translate="no" dir="ltr">RuntimeError</code></b>: If called with eager execution enabled and <code translate="no" dir="ltr">loss</code> is not callable.</li> </ul> <h4 id="eager_compatibility">Eager Compatibility</h4> <p>When eager execution is enabled, <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, and <code translate="no" dir="ltr">colocate_gradients_with_ops</code> are ignored.</p> <h3 id="get_name"><code translate="no" dir="ltr">get_name</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L352-L353">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_name()
</pre> <h3 id="get_slot"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L735-L771">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot(
    var,
    name
)
</pre> <p>Return a slot named <code translate="no" dir="ltr">name</code> created for <code translate="no" dir="ltr">var</code> by the Optimizer.</p> <p>Some <code translate="no" dir="ltr">Optimizer</code> subclasses use additional variables. For example <code translate="no" dir="ltr">Momentum</code> and <code translate="no" dir="ltr">Adagrad</code> use variables to accumulate updates. This method gives access to these <code translate="no" dir="ltr">Variable</code> objects if for some reason you need them.</p> <p>Use <code translate="no" dir="ltr">get_slot_names()</code> to get the list of slot names created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">var</code></b>: A variable passed to <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">apply_gradients()</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: A string.</li> </ul> <h4 id="returns_3">Returns:</h4> <p>The <code translate="no" dir="ltr">Variable</code> for the slot if it was created, <code translate="no" dir="ltr">None</code> otherwise.</p> <h3 id="get_slot_names"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L773-L781">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot_names()
</pre> <p>Return a list of the names of slots created by the <code translate="no" dir="ltr">Optimizer</code>.</p> <p>See <code translate="no" dir="ltr">get_slot()</code>.</p> <h4 id="returns_4">Returns:</h4> <p>A list of strings.</p> <h3 id="minimize"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L355-L413">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">minimize(
    loss,
    global_step=None,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    name=None,
    grad_loss=None
)
</pre> <p>Add operations to minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply combines calls <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying them call <code translate="no" dir="ltr">compute_gradients()</code> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A <code translate="no" dir="ltr">Tensor</code> containing the value to minimize.</li> <li>
<b><code translate="no" dir="ltr">global_step</code></b>: Optional <code translate="no" dir="ltr">Variable</code> to increment by one after the variables have been updated.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: Optional list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>. Defaults to the list of variables collected in the graph under the key <code translate="no" dir="ltr">GraphKeys.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code translate="no" dir="ltr">gate_gradients</code></b>: How to gate the computation of gradients. Can be <code translate="no" dir="ltr">GATE_NONE</code>, <code translate="no" dir="ltr">GATE_OP</code>, or <code translate="no" dir="ltr">GATE_GRAPH</code>.</li> <li>
<b><code translate="no" dir="ltr">aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code translate="no" dir="ltr">AggregationMethod</code>.</li> <li>
<b><code translate="no" dir="ltr">colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation.</li> <li>
<b><code translate="no" dir="ltr">grad_loss</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</li> </ul> <h4 id="returns_5">Returns:</h4> <p>An Operation that updates the variables in <code translate="no" dir="ltr">var_list</code>. If <code translate="no" dir="ltr">global_step</code> was not <code translate="no" dir="ltr">None</code>, that operation also increments <code translate="no" dir="ltr">global_step</code>.</p> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects.</li> </ul> <h4 id="eager_compatibility_2">Eager Compatibility</h4> <p>When eager execution is enabled, <code translate="no" dir="ltr">loss</code> should be a Python function that takes no arguments and computes the value to be minimized. Minimization (and gradient computation) is done with respect to the elements of <code translate="no" dir="ltr">var_list</code> if not None, else with respect to any trainable variables created during the execution of the <code translate="no" dir="ltr">loss</code> function. <code translate="no" dir="ltr">gate_gradients</code>, <code translate="no" dir="ltr">aggregation_method</code>, <code translate="no" dir="ltr">colocate_gradients_with_ops</code> and <code translate="no" dir="ltr">grad_loss</code> are ignored when eager execution is enabled.</p> <h3 id="variables"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/optimizer.py#L783-L809">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables()
</pre> <p>A list of variables which encode the current state of <code translate="no" dir="ltr">Optimizer</code>.</p> <p>Includes slot variables and additional global variables created by the optimizer in the current default graph.</p> <h4 id="returns_6">Returns:</h4> <p>A list of variables.</p> <h2 id="class_members">Class Members</h2> <ul> <li>
<code translate="no" dir="ltr">GATE_GRAPH = 2</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_NONE = 0</code> 
</li> <li>
<code translate="no" dir="ltr">GATE_OP = 1</code> 
</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/ProximalAdagradOptimizer</a>
  </p>
</div>
