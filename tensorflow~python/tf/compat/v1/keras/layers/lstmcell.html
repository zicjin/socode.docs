<h1 class="devsite-page-title">tf.compat.v1.keras.layers.LSTMCell</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.compat.v1.keras.layers.LSTMCell"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/recurrent.py#L2179-L2457">  View source on GitHub </a> </td>
</table>  <h2 id="class_lstmcell">Class <code translate="no" dir="ltr">LSTMCell</code>
</h2> <p>Cell class for the LSTM layer.</p> <p>Inherits From: <a href="../../../../keras/layers/layer"><code translate="no" dir="ltr">Layer</code></a></p>  <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">units</code></b>: Positive integer, dimensionality of the output space.</li> <li>
<b><code translate="no" dir="ltr">activation</code></b>: Activation function to use. Default: hyperbolic tangent (<code translate="no" dir="ltr">tanh</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">recurrent_activation</code></b>: Activation function to use for the recurrent step. Default: hard sigmoid (<code translate="no" dir="ltr">hard_sigmoid</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">kernel</code> weights matrix, used for the linear transformation of the inputs.</li> <li>
<b><code translate="no" dir="ltr">recurrent_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix, used for the linear transformation of the recurrent state.</li> <li>
<b><code translate="no" dir="ltr">bias_initializer</code></b>: Initializer for the bias vector.</li> <li>
<b><code translate="no" dir="ltr">unit_forget_bias</code></b>: Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force <code translate="no" dir="ltr">bias_initializer="zeros"</code>. This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a>
</li> <li>
<b><code translate="no" dir="ltr">kernel_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">recurrent_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">recurrent_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_constraint</code></b>: Constraint function applied to the bias vector.</li> <li>
<b><code translate="no" dir="ltr">dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.</li> <li>
<b><code translate="no" dir="ltr">recurrent_dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.</li> <li>
<b><code translate="no" dir="ltr">implementation</code></b>: Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications.</li> </ul> <h4 id="call_arguments">Call arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: A 2D tensor.</li> <li>
<b><code translate="no" dir="ltr">states</code></b>: List of state tensors corresponding to the previous timestep.</li> <li>
<b><code translate="no" dir="ltr">training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. Only relevant when <code translate="no" dir="ltr">dropout</code> or <code translate="no" dir="ltr">recurrent_dropout</code> is used.</li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/LSTMCell" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/compat/v1/keras/layers/LSTMCell</a>
  </p>
</div>
