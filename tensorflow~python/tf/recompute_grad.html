<h1 class="devsite-page-title">tf.recompute_grad</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.recompute_grad"> <meta itemprop="path" content="Stable"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/recompute_grad">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/ops/custom_gradient.py#L454-L499">  View source on GitHub </a> </td>
</table>  <p>An eager-compatible version of recompute_grad.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.recompute_grad(f)
</pre>  <p>For f(*args, **kwargs), this supports gradients with respect to args, or to gradients with respect to any variables residing in the kwarg 'variables'. Note that for keras layer and model objects, this is handled automatically.</p> <aside class="warning"><strong>Warning:</strong><span> If <code translate="no" dir="ltr">f</code> was originally a tf.keras Model or Layer object, <code translate="no" dir="ltr">g</code> will not be able to access the member variables of that object, because <code translate="no" dir="ltr">g</code> returns through the wrapper function <code translate="no" dir="ltr">inner</code>. When recomputing gradients through objects that inherit from keras, we suggest keeping a reference to the underlying object around for the purpose of accessing these variables.</span></aside> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">f</code></b>: function <code translate="no" dir="ltr">f(*x)</code> that returns a <code translate="no" dir="ltr">Tensor</code> or sequence of <code translate="no" dir="ltr">Tensor</code> outputs.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>A function <code translate="no" dir="ltr">g</code> that wraps <code translate="no" dir="ltr">f</code>, but which recomputes <code translate="no" dir="ltr">f</code> on the backwards pass of a gradient call.</p> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="recompute_grad"><code translate="no" dir="ltr">tf.compat.v1.recompute_grad</code></a></li> <li><a href="recompute_grad"><code translate="no" dir="ltr">tf.compat.v2.recompute_grad</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/recompute_grad" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/recompute_grad</a>
  </p>
</div>
