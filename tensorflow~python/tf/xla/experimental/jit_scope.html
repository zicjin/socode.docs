<h1 class="devsite-page-title">tf.xla.experimental.jit_scope</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.xla.experimental.jit_scope"> <meta itemprop="path" content="Stable"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/xla/experimental/jit_scope">  TensorFlow 1 version</a> </td> </table>  <p>Enable or disable JIT compilation of operators within the scope.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.xla.experimental.jit_scope(
    *args,
    **kwds
)
</pre>  <p>NOTE: This is an experimental feature.</p> <p>The compilation is a hint and only supported on a best-effort basis.</p> <h4 id="example_usage_2">Example usage:</h4> <p>with tf.xla.experimental.jit_scope(): c = tf.matmul(a, b) # compiled with tf.xla.experimental.jit_scope(compile_ops=False): d = tf.matmul(a, c) # not compiled with tf.xla.experimental.jit_scope( compile_ops=lambda node_def: 'matmul' in node_def.op.lower()): e = tf.matmul(a, b) + d # matmul is compiled, the addition is not.</p> <p>Example of separate_compiled_gradients: # In the example below, the computations for f, g and h will all be compiled # in separate scopes. with tf.xla.experimental.jit_scope( separate_compiled_gradients=True): f = tf.matmul(a, b) g = tf.gradients([f], [a, b], name='mygrads1') h = tf.gradients([f], [a, b], name='mygrads2')</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">compile_ops</code></b>: Whether to enable or disable compilation in the scope. Either a Python bool, or a callable that accepts the parameter <code translate="no" dir="ltr">node_def</code> and returns a python bool.</li> <li>
<b><code translate="no" dir="ltr">separate_compiled_gradients</code></b>: If true put each gradient subgraph into a separate compilation scope. This gives fine-grained control over which portions of the graph will be compiled as a single unit. Compiling gradients separately may yield better performance for some graphs. The scope is named based on the scope of the forward computation as well as the name of the gradients. As a result, the gradients will be compiled in a scope that is separate from both the forward computation, and from other gradients.</li> </ul> <h4 id="raises_2">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">RuntimeError</code></b>: if called when eager execution is enabled.</li> </ul> <h4 id="yields_2">Yields:</h4> <p>The current scope, enabling or disabling compilation.</p> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="jit_scope"><code translate="no" dir="ltr">tf.compat.v1.xla.experimental.jit_scope</code></a></li> <li><a href="jit_scope"><code translate="no" dir="ltr">tf.compat.v2.xla.experimental.jit_scope</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/xla/experimental/jit_scope</a>
  </p>
</div>
