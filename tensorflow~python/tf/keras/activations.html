<h1 class="devsite-page-title">Module: tf.keras.activations</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.activations"> <meta itemprop="path" content="Stable"> </div> <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/activations">  TensorFlow 1 version</a> </td> </table> <p>Built-in activation functions.</p> <h2 id="functions">Functions</h2> <p><a href="activations/deserialize"><code translate="no" dir="ltr">deserialize(...)</code></a>: Returns activation function denoted by input string.</p> <p><a href="activations/elu"><code translate="no" dir="ltr">elu(...)</code></a>: Exponential linear unit.</p> <p><a href="activations/exponential"><code translate="no" dir="ltr">exponential(...)</code></a>: Exponential activation function.</p> <p><a href="activations/get"><code translate="no" dir="ltr">get(...)</code></a>: Returns function.</p> <p><a href="activations/hard_sigmoid"><code translate="no" dir="ltr">hard_sigmoid(...)</code></a>: Hard sigmoid activation function.</p> <p><a href="activations/linear"><code translate="no" dir="ltr">linear(...)</code></a>: Linear activation function.</p> <p><a href="activations/relu"><code translate="no" dir="ltr">relu(...)</code></a>: Applies the rectified linear unit activation function.</p> <p><a href="activations/selu"><code translate="no" dir="ltr">selu(...)</code></a>: Scaled Exponential Linear Unit (SELU).</p> <p><a href="activations/serialize"><code translate="no" dir="ltr">serialize(...)</code></a>: Returns name attribute (<code translate="no" dir="ltr">__name__</code>) of function.</p> <p><a href="activations/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Sigmoid activation function.</p> <p><a href="activations/softmax"><code translate="no" dir="ltr">softmax(...)</code></a>: Softmax converts a real vector to a vector of categorical probabilities.</p> <p><a href="activations/softplus"><code translate="no" dir="ltr">softplus(...)</code></a>: Softplus activation function.</p> <p><a href="activations/softsign"><code translate="no" dir="ltr">softsign(...)</code></a>: Softsign activation function.</p> <p><a href="activations/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Hyperbolic tangent activation function.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/activations</a>
  </p>
</div>
