<h1 class="devsite-page-title">tf.keras.mixed_precision.experimental.LossScaleOptimizer</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.mixed_precision.experimental.LossScaleOptimizer"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="iterations"> <meta itemprop="property" content="learning_rate"> <meta itemprop="property" content="loss_scale"> <meta itemprop="property" content="lr"> <meta itemprop="property" content="weights"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="add_slot"> <meta itemprop="property" content="add_weight"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="get_gradients"> <meta itemprop="property" content="get_scaled_loss"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="get_unscaled_gradients"> <meta itemprop="property" content="get_updates"> <meta itemprop="property" content="get_weights"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="set_weights"> <meta itemprop="property" content="variables"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L49-L333">  View source on GitHub </a> </td>
</table>  <h2 id="class_lossscaleoptimizer_2">Class <code translate="no" dir="ltr">LossScaleOptimizer</code>
</h2> <p>An optimizer that applies loss scaling.</p> <p>Inherits From: <a href="../../optimizers/optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <h3 id="used_in_the_guide_2">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/keras/mixed_precision">Mixed precision</a></li> </ul> <p>Loss scaling is a process that multiplies the loss by a multiplier called the loss scale, and divides each gradient by the same multiplier. The pseudocode for this process is:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">loss = ...
loss *= loss_scale
grads = gradients(loss, vars)
grads /= loss_scale
</pre> <p>Mathematically, loss scaling has no effect, but can help avoid numerical underflow in intermediate gradients when float16 tensors are used. By multiplying the loss, each intermediate gradient will have the same multiplier applied.</p> <p>The loss scale can either be a fixed constant, chosen by the user, or be dynamically determined. Dynamically determining the loss scale is convenient as a loss scale does not have to be explicitly chosen. However it reduces performance.</p> <p>This optimizer wraps another optimizer and applies loss scaling to it via a <code translate="no" dir="ltr">LossScale</code>. Loss scaling is applied whenever gradients are computed, either through <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">get_gradients()</code>. The loss scale is updated via <a href="../../../mixed_precision/experimental/lossscale#update"><code translate="no" dir="ltr">LossScale.update()</code></a> whenever gradients are applied, either through <code translate="no" dir="ltr">minimize()</code> or <code translate="no" dir="ltr">apply_gradients()</code>. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">opt = tf.keras.optimizers.SGD(0.1)
opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, "dynamic")
# 'minimize' applies loss scaling to the loss and updates the loss sale.
opt.minimize(loss_fn)
</pre> <p>If a <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> is used to compute gradients instead of <a href="../../optimizers/optimizer#minimize"><code translate="no" dir="ltr">LossScaleOptimizer.minimize</code></a> or <a href="lossscaleoptimizer#get_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_gradients</code></a>, the loss and gradients must be scaled manually. This can be done by calling <a href="lossscaleoptimizer#get_scaled_loss"><code translate="no" dir="ltr">LossScaleOptimizer.get_scaled_loss</code></a> before passing the loss to <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>, and <a href="lossscaleoptimizer#get_unscaled_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_unscaled_gradients</code></a> after computing the gradients with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. For example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(...)
vars = ...
with tf.GradientTape() as tape:
  loss = ...
  scaled_loss = opt.get_scaled_loss(loss)
scaled_grads = tape.gradient(scaled_loss, vars)
grads = opt.get_unscaled_gradients(scaled_grads)
opt.apply_gradients(zip(grads, vars))  # Loss scale will be updated here
</pre> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L105-L143">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    optimizer,
    loss_scale
)
</pre> <p>Initializes this loss scale optimizer.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">optimizer</code></b>: The Optimizer instance to wrap.</li> <li>
<b><code translate="no" dir="ltr">loss_scale</code></b>: The loss scale to scale the loss and gradients. This can either be an int/float to use a fixed loss scale, the string "dynamic" to use dynamic loss scaling, or an instance of a LossScale. The string "dynamic" equivalent to passing <code translate="no" dir="ltr">DynamicLossScale()</code>, and passing an int/float is equivalent to passing a FixedLossScale with the given loss scale.</li> </ul> <h2 id="properties_2">Properties</h2> <h3 id="iterations"><code translate="no" dir="ltr">iterations</code></h3> <p>Variable. The number of training steps this Optimizer has run.</p> <h3 id="learning_rate"><code translate="no" dir="ltr">learning_rate</code></h3> <h3 id="loss_scale"><code translate="no" dir="ltr">loss_scale</code></h3> <p>The <code translate="no" dir="ltr">LossScale</code> instance associated with this optimizer.</p> <h3 id="lr"><code translate="no" dir="ltr">lr</code></h3> <h3 id="weights"><code translate="no" dir="ltr">weights</code></h3> <p>Returns variables of this Optimizer based on the order created.</p> <h2 id="methods_2">Methods</h2> <h3 id="add_slot"><code translate="no" dir="ltr">add_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L329-L333">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">add_slot(
    var,
    slot_name,
    initializer='zeros'
)
</pre> <p>Add a new slot variable for <code translate="no" dir="ltr">var</code>.</p> <h3 id="add_weight"><code translate="no" dir="ltr">add_weight</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L780-L820">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">add_weight(
    name,
    shape,
    dtype=None,
    initializer='zeros',
    trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</pre> <h3 id="apply_gradients"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L219-L224">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply_gradients(
    grads_and_vars,
    name=None
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <h4 id="args_7">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads_and_vars</code></b>: List of (gradient, variable) pairs.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor.</li> </ul> <h4 id="returns_9">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1.</p> <h4 id="raises_4">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If <code translate="no" dir="ltr">grads_and_vars</code> is malformed.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If none of the variables have gradients.</li> </ul> <h3 id="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L261-L268">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">@classmethod
from_config(
    cls,
    config,
    custom_objects=None
)
</pre> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p> <h4 id="arguments_4">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">config</code></b>: A Python dictionary, typically the output of get_config.</li> <li>
<b><code translate="no" dir="ltr">custom_objects</code></b>: A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter.</li> </ul> <h4 id="returns_10">Returns:</h4> <p>An optimizer instance.</p> <h3 id="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L253-L259">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_config()
</pre> <p>Returns the config of the optimimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p> <h4 id="returns_11">Returns:</h4> <p>Python dictionary.</p> <h3 id="get_gradients"><code translate="no" dir="ltr">get_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L214-L217">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_gradients(
    loss,
    params
)
</pre> <p>Returns gradients of <code translate="no" dir="ltr">loss</code> with respect to <code translate="no" dir="ltr">params</code>.</p> <h4 id="arguments_5">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: Loss tensor.</li> <li>
<b><code translate="no" dir="ltr">params</code></b>: List of variables.</li> </ul> <h4 id="returns_12">Returns:</h4> <p>List of gradient tensors.</p> <h4 id="raises_5">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient function not implemented).</li> </ul> <h3 id="get_scaled_loss"><code translate="no" dir="ltr">get_scaled_loss</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L150-L177">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_scaled_loss(loss)
</pre> <p>Scales the loss by the loss scale.</p> <p>This method is only needed if you compute gradients manually, e.g. with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. In that case, call this method to scale the loss before passing the loss to <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. If you use <a href="../../optimizers/optimizer#minimize"><code translate="no" dir="ltr">LossScaleOptimizer.minimize</code></a> or <a href="lossscaleoptimizer#get_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_gradients</code></a>, loss scaling is automatically applied and this method is unneeded.</p> <p>If this method is called, <code translate="no" dir="ltr">get_unscaled_gradients</code> should also be called. See the <a href="lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.experimental.LossScaleOptimizer</code></a> doc for an example.</p> <h4 id="args_8">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: The loss, which will be multiplied by the loss scale. Can either be a tensor or a callable returning a tensor.</li> </ul> <h4 id="returns_13">Returns:</h4> <p><code translate="no" dir="ltr">loss</code> multiplied by <a href="lossscaleoptimizer#loss_scale"><code translate="no" dir="ltr">LossScaleOptimizer.loss_scale()</code></a>.</p> <h3 id="get_slot"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L318-L327">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot(
    var,
    slot_name
)
</pre> <h3 id="get_slot_names"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L281-L282">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot_names()
</pre> <p>A list of names for this optimizer's slots.</p> <h3 id="get_unscaled_gradients"><code translate="no" dir="ltr">get_unscaled_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L179-L203">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_unscaled_gradients(grads)
</pre> <p>Unscales the gradients by the loss scale.</p> <p>This method is only needed if you compute gradients manually, e.g. with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. In that case, call this method to unscale the gradients after computing them with <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a>. If you use <a href="../../optimizers/optimizer#minimize"><code translate="no" dir="ltr">LossScaleOptimizer.minimize</code></a> or <a href="lossscaleoptimizer#get_gradients"><code translate="no" dir="ltr">LossScaleOptimizer.get_gradients</code></a>, loss scaling is automatically applied and this method is unneeded.</p> <p>If this method is called, <code translate="no" dir="ltr">get_scaled_loss</code> should also be called. See the <a href="lossscaleoptimizer"><code translate="no" dir="ltr">tf.keras.mixed_precision.experimental.LossScaleOptimizer</code></a> doc for an example.</p> <h4 id="args_9">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads</code></b>: A list of tensors, each which will be divided by the loss scale. Can have None values, which are ignored.</li> </ul> <h4 id="returns_14">Returns:</h4> <p>A new list the same size as <code translate="no" dir="ltr">grads</code>, where every non-None value in <code translate="no" dir="ltr">grads</code> is divided by <a href="lossscaleoptimizer#loss_scale"><code translate="no" dir="ltr">LossScaleOptimizer.loss_scale()</code></a>.</p> <h3 id="get_updates"><code translate="no" dir="ltr">get_updates</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L502-L509">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_updates(
    loss,
    params
)
</pre> <h3 id="get_weights"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L291-L292">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_weights()
</pre> <h3 id="minimize"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">minimize(
    loss,
    var_list,
    grad_loss=None,
    name=None
)
</pre> <p>Minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply computes gradient using <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and calls <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying then call <a href="../../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p> <h4 id="args_10">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A callable taking no arguments which returns the value to minimize.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>, or a callable returning the list or tuple of <code translate="no" dir="ltr">Variable</code> objects. Use callable when the variable list would otherwise be incomplete before <code translate="no" dir="ltr">minimize</code> since the variables are created at the first time <code translate="no" dir="ltr">loss</code> is called.</li> <li>
<b><code translate="no" dir="ltr">grad_loss</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation.</li> </ul> <h4 id="returns_15">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that updates the variables in <code translate="no" dir="ltr">var_list</code>. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1.</p> <h4 id="raises_6">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects.</li> </ul> <h3 id="set_weights"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L294-L295">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">set_weights(weights)
</pre> <h3 id="variables"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/mixed_precision/experimental/loss_scale_optimizer.py#L284-L285">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables()
</pre> <p>Returns variables of this Optimizer based on the order created.</p> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="lossscaleoptimizer"><code translate="no" dir="ltr">tf.compat.v1.keras.mixed_precision.experimental.LossScaleOptimizer</code></a></li> <li><a href="lossscaleoptimizer"><code translate="no" dir="ltr">tf.compat.v2.keras.mixed_precision.experimental.LossScaleOptimizer</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/experimental/LossScaleOptimizer</a>
  </p>
</div>
