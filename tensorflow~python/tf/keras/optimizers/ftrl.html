<h1 class="devsite-page-title">tf.keras.optimizers.Ftrl</h1>    <devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>  <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.optimizers.Ftrl"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="iterations"> <meta itemprop="property" content="weights"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="add_slot"> <meta itemprop="property" content="add_weight"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="get_gradients"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="get_updates"> <meta itemprop="property" content="get_weights"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="set_weights"> <meta itemprop="property" content="variables"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/optimizers/Ftrl">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/ftrl.py#L29-L244">  View source on GitHub </a> </td>
</table>  <h2 id="class_ftrl_2">Class <code translate="no" dir="ltr">Ftrl</code>
</h2> <p>Optimizer that implements the FTRL algorithm.</p> <p>Inherits From: <a href="optimizer"><code translate="no" dir="ltr">Optimizer</code></a></p> <p><strong>Aliases</strong>: <a href="ftrl"><code translate="no" dir="ltr">tf.optimizers.Ftrl</code></a></p>  <p>See Algorithm 1 of this <a href="https://www.eecs.tufts.edu/%7Edsculley/papers/ad-click-prediction.pdf">paper</a>. This version has support for both online L2 (the L2 penalty given in the paper above) and shrinkage-type L2 (which is the addition of an L2 penalty to the loss function).</p> <h4 id="initialization_2">Initialization:</h4> <div> $$t = 0$$ </div> <div> $$n_{0} = 0$$ </div> <div> $$\sigma_{0} = 0$$ </div> <div> $$z_{0} = 0$$ </div> <p>Update (</p>
<div> \(i\) </div> is variable index): <div> \(t = t + 1\) </div> <div> \(n_{t,i} = n_{t-1,i} + g_{t,i}^{2}\) </div> <div> \(\sigma_{t,i} = (\sqrt{n_{t,i}} - \sqrt{n_{t-1,i}}) / \alpha\) </div> <div> \(z_{t,i} = z_{t-1,i} + g_{t,i} - \sigma_{t,i} * w_{t,i}\) </div> <div> \(w_{t,i} = - ((\beta+\sqrt{n+{t}}) / \alpha + \lambda_{2})^{-1} * (z_{i} - sgn(z_{i}) * \lambda_{1}) if \abs{z_{i}} &gt; \lambda_{i} else 0\) </div> <p>Check the documentation for the l2_shrinkage_regularization_strength parameter for more details when shrinkage is enabled, where gradient is replaced with gradient_with_shrinkage.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/ftrl.py#L57-L136">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    name='Ftrl',
    l2_shrinkage_regularization_strength=0.0,
    **kwargs
)
</pre> <p>Construct a new FTRL optimizer.</p> <h4 id="args_4">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">learning_rate</code></b>: A float value or a constant float <code translate="no" dir="ltr">Tensor</code>.</li> <li>
<b><code translate="no" dir="ltr">learning_rate_power</code></b>: A float value, must be less or equal to zero. Controls how the learning rate decreases during training. Use zero for a fixed learning rate.</li> <li>
<b><code translate="no" dir="ltr">initial_accumulator_value</code></b>: The starting value for accumulators. Only zero or positive values are allowed.</li> <li>
<b><code translate="no" dir="ltr">l1_regularization_strength</code></b>: A float value, must be greater than or equal to zero.</li> <li>
<b><code translate="no" dir="ltr">l2_regularization_strength</code></b>: A float value, must be greater than or equal to zero.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name prefix for the operations created when applying gradients. Defaults to "Ftrl".</li> <li>
<b><code translate="no" dir="ltr">l2_shrinkage_regularization_strength</code></b>: A float value, must be greater than or equal to zero. This differs from L2 above in that the L2 above is a stabilization penalty, whereas this L2 shrinkage is a magnitude penalty. The FTRL formulation can be written as: w_{t+1} = argmin<em>w(\hat{g}</em>{1:t}w + L1<em>||w||_1 + L2</em>||w||_2^2), where \hat{g} = g + (2<em>L2_shrinkage</em>w), and g is the gradient of the loss function w.r.t. the weights w. Specifically, in the absence of L1 regularization, it is equivalent to the following update rule: w_{t+1} = w_t - lr_t / (1 + 2<em>L2</em>lr_t) * g_t - 2<em>L2_shrinkage</em>lr_t / (1 + 2<em>L2</em>lr_t) * w_t where lr_t is the learning rate at t. When input is sparse shrinkage will only happen on the active weights.\</li> <li>
<b><code translate="no" dir="ltr">**kwargs</code></b>: keyword arguments. Allowed to be {<code translate="no" dir="ltr">clipnorm</code>, <code translate="no" dir="ltr">clipvalue</code>, <code translate="no" dir="ltr">lr</code>, <code translate="no" dir="ltr">decay</code>}. <code translate="no" dir="ltr">clipnorm</code> is clip gradients by norm; <code translate="no" dir="ltr">clipvalue</code> is clip gradients by value, <code translate="no" dir="ltr">decay</code> is included for backward compatibility to allow time inverse decay of learning rate. <code translate="no" dir="ltr">lr</code> is included for backward compatibility, recommended to use <code translate="no" dir="ltr">learning_rate</code> instead.</li> </ul> <h4 id="raises_5">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If one of the arguments is invalid.</li> </ul> <p>References See <a href="https://www.eecs.tufts.edu/%7Edsculley/papers/ad-click-prediction.pdf">paper</a></p> <h2 id="properties_2">Properties</h2> <h3 id="iterations"><code translate="no" dir="ltr">iterations</code></h3> <p>Variable. The number of training steps this Optimizer has run.</p> <h3 id="weights"><code translate="no" dir="ltr">weights</code></h3> <p>Returns variables of this Optimizer based on the order created.</p> <h2 id="methods_2">Methods</h2> <h3 id="add_slot"><code translate="no" dir="ltr">add_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L570-L606">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">add_slot(
    var,
    slot_name,
    initializer='zeros'
)
</pre> <p>Add a new slot variable for <code translate="no" dir="ltr">var</code>.</p> <h3 id="add_weight"><code translate="no" dir="ltr">add_weight</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L780-L820">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">add_weight(
    name,
    shape,
    dtype=None,
    initializer='zeros',
    trainable=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
</pre> <h3 id="apply_gradients"><code translate="no" dir="ltr">apply_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L407-L444">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">apply_gradients(
    grads_and_vars,
    name=None
)
</pre> <p>Apply gradients to variables.</p> <p>This is the second part of <code translate="no" dir="ltr">minimize()</code>. It returns an <code translate="no" dir="ltr">Operation</code> that applies gradients.</p> <h4 id="args_5">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads_and_vars</code></b>: List of (gradient, variable) pairs.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation. Default to the name passed to the <code translate="no" dir="ltr">Optimizer</code> constructor.</li> </ul> <h4 id="returns_7">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that applies the specified gradients. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1.</p> <h4 id="raises_6">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">TypeError</code></b>: If <code translate="no" dir="ltr">grads_and_vars</code> is malformed.</li> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If none of the variables have gradients.</li> </ul> <h3 id="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L710-L733">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">from_config(
    cls,
    config,
    custom_objects=None
)
</pre> <p>Creates an optimizer from its config.</p> <p>This method is the reverse of <code translate="no" dir="ltr">get_config</code>, capable of instantiating the same optimizer from the config dictionary.</p> <h4 id="arguments_4">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">config</code></b>: A Python dictionary, typically the output of get_config.</li> <li>
<b><code translate="no" dir="ltr">custom_objects</code></b>: A Python dictionary mapping names to additional Python objects used to create this optimizer, such as a function used for a hyperparameter.</li> </ul> <h4 id="returns_8">Returns:</h4> <p>An optimizer instance.</p> <h3 id="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/ftrl.py#L226-L244">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_config()
</pre> <p>Returns the config of the optimimizer.</p> <p>An optimizer config is a Python dictionary (serializable) containing the configuration of an optimizer. The same optimizer can be reinstantiated later (without any saved state) from this configuration.</p> <h4 id="returns_9">Returns:</h4> <p>Python dictionary.</p> <h3 id="get_gradients"><code translate="no" dir="ltr">get_gradients</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L373-L405">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_gradients(
    loss,
    params
)
</pre> <p>Returns gradients of <code translate="no" dir="ltr">loss</code> with respect to <code translate="no" dir="ltr">params</code>.</p> <h4 id="arguments_5">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: Loss tensor.</li> <li>
<b><code translate="no" dir="ltr">params</code></b>: List of variables.</li> </ul> <h4 id="returns_10">Returns:</h4> <p>List of gradient tensors.</p> <h4 id="raises_7">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: In case any gradient cannot be computed (e.g. if gradient function not implemented).</li> </ul> <h3 id="get_slot"><code translate="no" dir="ltr">get_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L608-L611">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot(
    var,
    slot_name
)
</pre> <h3 id="get_slot_names"><code translate="no" dir="ltr">get_slot_names</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L566-L568">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_slot_names()
</pre> <p>A list of names for this optimizer's slots.</p> <h3 id="get_updates"><code translate="no" dir="ltr">get_updates</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L502-L509">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_updates(
    loss,
    params
)
</pre> <h3 id="get_weights"><code translate="no" dir="ltr">get_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L755-L757">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_weights()
</pre> <h3 id="minimize"><code translate="no" dir="ltr">minimize</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L289-L318">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">minimize(
    loss,
    var_list,
    grad_loss=None,
    name=None
)
</pre> <p>Minimize <code translate="no" dir="ltr">loss</code> by updating <code translate="no" dir="ltr">var_list</code>.</p> <p>This method simply computes gradient using <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and calls <code translate="no" dir="ltr">apply_gradients()</code>. If you want to process the gradient before applying then call <a href="../../gradienttape"><code translate="no" dir="ltr">tf.GradientTape</code></a> and <code translate="no" dir="ltr">apply_gradients()</code> explicitly instead of using this function.</p> <h4 id="args_6">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss</code></b>: A callable taking no arguments which returns the value to minimize.</li> <li>
<b><code translate="no" dir="ltr">var_list</code></b>: list or tuple of <code translate="no" dir="ltr">Variable</code> objects to update to minimize <code translate="no" dir="ltr">loss</code>, or a callable returning the list or tuple of <code translate="no" dir="ltr">Variable</code> objects. Use callable when the variable list would otherwise be incomplete before <code translate="no" dir="ltr">minimize</code> since the variables are created at the first time <code translate="no" dir="ltr">loss</code> is called.</li> <li>
<b><code translate="no" dir="ltr">grad_loss</code></b>: Optional. A <code translate="no" dir="ltr">Tensor</code> holding the gradient computed for <code translate="no" dir="ltr">loss</code>.</li> <li>
<b><code translate="no" dir="ltr">name</code></b>: Optional name for the returned operation.</li> </ul> <h4 id="returns_11">Returns:</h4> <p>An <code translate="no" dir="ltr">Operation</code> that updates the variables in <code translate="no" dir="ltr">var_list</code>. The <code translate="no" dir="ltr">iterations</code> will be automatically increased by 1.</p> <h4 id="raises_8">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If some of the variables are not <code translate="no" dir="ltr">Variable</code> objects.</li> </ul> <h3 id="set_weights"><code translate="no" dir="ltr">set_weights</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L760-L778">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">set_weights(weights)
</pre> <h3 id="variables"><code translate="no" dir="ltr">variables</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L746-L748">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">variables()
</pre> <p>Returns variables of this Optimizer based on the order created.</p> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="ftrl"><code translate="no" dir="ltr">tf.compat.v1.keras.optimizers.Ftrl</code></a></li> <li><a href="ftrl"><code translate="no" dir="ltr">tf.compat.v2.keras.optimizers.Ftrl</code></a></li> <li><a href="ftrl"><code translate="no" dir="ltr">tf.compat.v2.optimizers.Ftrl</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl</a>
  </p>
</div>
