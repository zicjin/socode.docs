<h1 class="devsite-page-title">tf.keras.preprocessing.text.Tokenizer</h1>    <devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax>  <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.preprocessing.text.Tokenizer"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="fit_on_sequences"> <meta itemprop="property" content="fit_on_texts"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="sequences_to_matrix"> <meta itemprop="property" content="sequences_to_texts"> <meta itemprop="property" content="sequences_to_texts_generator"> <meta itemprop="property" content="texts_to_matrix"> <meta itemprop="property" content="texts_to_sequences"> <meta itemprop="property" content="texts_to_sequences_generator"> <meta itemprop="property" content="to_json"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/preprocessing/text/Tokenizer">  TensorFlow 1 version</a> </td> </table>  <h2 id="class_tokenizer">Class <code translate="no" dir="ltr">Tokenizer</code>
</h2> <p>Text tokenization utility class.</p> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/text/image_captioning">Image captioning with visual attention</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">Neural machine translation with attention</a></li> </ul> <p>This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf...</p> <h1 id="arguments" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">num_words: the maximum number of words to keep, based
    on word frequency. Only the most common `num_words-1` words will
    be kept.
filters: a string where each element is a character that will be
    filtered from the texts. The default is all punctuation, plus
    tabs and line breaks, minus the `'` character.
lower: boolean. Whether to convert the texts to lowercase.
split: str. Separator for word splitting.
char_level: if True, every character will be treated as a token.
oov_token: if given, it will be added to word_index and used to
    replace out-of-vocabulary words during text_to_sequence calls
</pre> <p>By default, all punctuation is removed, turning the texts into space-separated sequences of words (words maybe include the <code translate="no" dir="ltr">'</code> character). These sequences are then split into lists of tokens. They will then be indexed or vectorized.</p> <p><code translate="no" dir="ltr">0</code> is a reserved index that won't be assigned to any word.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    num_words=None,
    filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    char_level=False,
    oov_token=None,
    document_count=0,
    **kwargs
)
</pre> <p>Initialize self. See help(type(self)) for accurate signature.</p> <h2 id="methods">Methods</h2> <h3 id="fit_on_sequences"><code translate="no" dir="ltr">fit_on_sequences</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">fit_on_sequences(sequences)
</pre> <p>Updates internal vocabulary based on a list of sequences.</p> <p>Required before using <code translate="no" dir="ltr">sequences_to_matrix</code> (if <code translate="no" dir="ltr">fit_on_texts</code> was never called).</p> <h1 id="arguments_2" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">sequences: A list of sequence.
    A "sequence" is a list of integer word indices.
</pre> <h3 id="fit_on_texts"><code translate="no" dir="ltr">fit_on_texts</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">fit_on_texts(texts)
</pre> <p>Updates internal vocabulary based on a list of texts.</p> <p>In the case where texts contains lists, we assume each entry of the lists to be a token.</p> <p>Required before using <code translate="no" dir="ltr">texts_to_sequences</code> or <code translate="no" dir="ltr">texts_to_matrix</code>.</p> <h1 id="arguments_3" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">texts: can be a list of strings,
    a generator of strings (for memory-efficiency),
    or a list of list of strings.
</pre> <h3 id="get_config"><code translate="no" dir="ltr">get_config</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_config()
</pre> <p>Returns the tokenizer configuration as Python dictionary. The word count dictionaries used by the tokenizer get serialized into plain JSON, so that the configuration can be read by other projects.</p> <h1 id="returns" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A Python dictionary with the tokenizer configuration.
</pre> <h3 id="sequences_to_matrix"><code translate="no" dir="ltr">sequences_to_matrix</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">sequences_to_matrix(
    sequences,
    mode='binary'
)
</pre> <p>Converts a list of sequences into a Numpy matrix.</p> <h1 id="arguments_4" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">sequences: list of sequences
    (a sequence is a list of integer word indices).
mode: one of "binary", "count", "tfidf", "freq"
</pre> <h1 id="returns_2" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A Numpy matrix.
</pre> <h1 id="raises" class="page-title">Raises</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">ValueError: In case of invalid `mode` argument,
    or if the Tokenizer requires to be fit to sample data.
</pre> <h3 id="sequences_to_texts"><code translate="no" dir="ltr">sequences_to_texts</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">sequences_to_texts(sequences)
</pre> <p>Transforms each sequence into a list of text.</p> <p>Only top <code translate="no" dir="ltr">num_words-1</code> most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.</p> <h1 id="arguments_5" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">sequences: A list of sequences (list of integers).
</pre> <h1 id="returns_3" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A list of texts (strings)
</pre> <h3 id="sequences_to_texts_generator"><code translate="no" dir="ltr">sequences_to_texts_generator</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">sequences_to_texts_generator(sequences)
</pre> <p>Transforms each sequence in <code translate="no" dir="ltr">sequences</code> to a list of texts(strings).</p> <p>Each sequence has to a list of integers. In other words, sequences should be a list of sequences</p> <p>Only top <code translate="no" dir="ltr">num_words-1</code> most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.</p> <h1 id="arguments_6" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">sequences: A list of sequences.
</pre> <h1 id="yields" class="page-title">Yields</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">Yields individual texts.
</pre> <h3 id="texts_to_matrix"><code translate="no" dir="ltr">texts_to_matrix</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">texts_to_matrix(
    texts,
    mode='binary'
)
</pre> <p>Convert a list of texts to a Numpy matrix.</p> <h1 id="arguments_7" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">texts: list of strings.
mode: one of "binary", "count", "tfidf", "freq".
</pre> <h1 id="returns_4" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A Numpy matrix.
</pre> <h3 id="texts_to_sequences"><code translate="no" dir="ltr">texts_to_sequences</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">texts_to_sequences(texts)
</pre> <p>Transforms each text in texts to a sequence of integers.</p> <p>Only top <code translate="no" dir="ltr">num_words-1</code> most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.</p> <h1 id="arguments_8" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">texts: A list of texts (strings).
</pre> <h1 id="returns_5" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A list of sequences.
</pre> <h3 id="texts_to_sequences_generator"><code translate="no" dir="ltr">texts_to_sequences_generator</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">texts_to_sequences_generator(texts)
</pre> <p>Transforms each text in <code translate="no" dir="ltr">texts</code> to a sequence of integers.</p> <p>Each item in texts can also be a list, in which case we assume each item of that list to be a token.</p> <p>Only top <code translate="no" dir="ltr">num_words-1</code> most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.</p> <h1 id="arguments_9" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">texts: A list of texts (strings).
</pre> <h1 id="yields_2" class="page-title">Yields</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">Yields individual sequences.
</pre> <h3 id="to_json"><code translate="no" dir="ltr">to_json</code></h3> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">to_json(**kwargs)
</pre> <p>Returns a JSON string containing the tokenizer configuration. To load a tokenizer from a JSON string, use <a href="tokenizer_from_json"><code translate="no" dir="ltr">keras.preprocessing.text.tokenizer_from_json(json_string)</code></a>.</p> <h1 id="arguments_10" class="page-title">Arguments</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">**kwargs: Additional keyword arguments
    to be passed to `json.dumps()`.
</pre> <h1 id="returns_6" class="page-title">Returns</h1> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">A JSON string containing the tokenizer configuration.
</pre> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="tokenizer"><code translate="no" dir="ltr">tf.compat.v1.keras.preprocessing.text.Tokenizer</code></a></li> <li><a href="tokenizer"><code translate="no" dir="ltr">tf.compat.v2.keras.preprocessing.text.Tokenizer</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer</a>
  </p>
</div>
