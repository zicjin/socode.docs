<h1 class="devsite-page-title">tf.keras.layers.LayerNormalization</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.LayerNormalization"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/LayerNormalization">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/normalization.py#L910-L1158">  View source on GitHub </a> </td>
</table>  <h2 id="class_layernormalization">Class <code translate="no" dir="ltr">LayerNormalization</code>
</h2> <p>Layer normalization layer (Ba et al., 2016).</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding</a></li> </ul> <p>Normalize the activations of the previous layer for each given example in a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that maintains the mean activation within each example close to 0 and the activation standard deviation close to 1.</p> <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">axis</code></b>: Integer or List/Tuple. The axis that should be normalized (typically the features axis).</li> <li>
<b><code translate="no" dir="ltr">epsilon</code></b>: Small float added to variance to avoid dividing by zero.</li> <li>
<b><code translate="no" dir="ltr">center</code></b>: If True, add offset of <code translate="no" dir="ltr">beta</code> to normalized tensor. If False, <code translate="no" dir="ltr">beta</code> is ignored.</li> <li>
<b><code translate="no" dir="ltr">scale</code></b>: If True, multiply by <code translate="no" dir="ltr">gamma</code>. If False, <code translate="no" dir="ltr">gamma</code> is not used. When the next layer is linear (also e.g. <a href="../../nn/relu"><code translate="no" dir="ltr">nn.relu</code></a>), this can be disabled since the scaling will be done by the next layer.</li> <li>
<b><code translate="no" dir="ltr">beta_initializer</code></b>: Initializer for the beta weight.</li> <li>
<b><code translate="no" dir="ltr">gamma_initializer</code></b>: Initializer for the gamma weight.</li> <li>
<b><code translate="no" dir="ltr">beta_regularizer</code></b>: Optional regularizer for the beta weight.</li> <li>
<b><code translate="no" dir="ltr">gamma_regularizer</code></b>: Optional regularizer for the gamma weight.</li> <li>
<b><code translate="no" dir="ltr">beta_constraint</code></b>: Optional constraint for the beta weight.</li> <li>
<b><code translate="no" dir="ltr">gamma_constraint</code></b>: Optional constraint for the gamma weight.</li> <li>
<b><code translate="no" dir="ltr">trainable</code></b>: Boolean, if <code translate="no" dir="ltr">True</code> the variables will be marked as trainable.</li> </ul> <h4 id="input_shape">Input shape:</h4> <p>Arbitrary. Use the keyword argument <code translate="no" dir="ltr">input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p> <h4 id="output_shape">Output shape:</h4> <p>Same shape as input.</p> <h4 id="references">References:</h4> <ul> <li><a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></li> </ul> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/normalization.py#L949-L987">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    axis=-1,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
</pre> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="layernormalization"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.LayerNormalization</code></a></li> <li><a href="layernormalization"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.LayerNormalization</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization</a>
  </p>
</div>
