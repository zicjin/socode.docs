<h1 class="devsite-page-title">tf.keras.layers.Embedding</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.Embedding"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/Embedding">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/embeddings.py#L35-L203">  View source on GitHub </a> </td>
</table>  <h2 id="class_embedding">Class <code translate="no" dir="ltr">Embedding</code>
</h2> <p>Turns positive integers (indexes) into dense vectors of fixed size.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p> <h3 id="used_in_the_guide">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/keras/masking_and_padding">Masking and padding with Keras</a></li> <li><a href="https://www.tensorflow.org/guide/keras/rnn">Recurrent Neural Networks (RNN) with Keras</a></li> <li><a href="https://www.tensorflow.org/guide/keras/functional">The Keras functional API in TensorFlow</a></li> </ul> <h3 id="used_in_the_tutorials">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/text/nmt_with_attention">Neural machine translation with attention</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn">Text classification with an RNN</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/word_embeddings">Word embeddings</a></li> <li><a href="https://www.tensorflow.org/tutorials/keras/text_classification">Text classification with preprocessed text: Movie reviews</a></li> </ul> <p>e.g. <code translate="no" dir="ltr">[[4], [20]] -&gt; [[0.25, 0.1], [0.6, -0.2]]</code></p> <p>This layer can only be used as the first layer in a model.</p> <h4 id="example">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = Sequential()
model.add(Embedding(1000, 64, input_length=10))
# the model will take as input an integer matrix of size (batch,
# input_length).
# the largest integer (i.e. word index) in the input should be no larger
# than 999 (vocabulary size).
# now model.output_shape == (None, 10, 64), where None is the batch
# dimension.

input_array = np.random.randint(1000, size=(32, 10))

model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
assert output_array.shape == (32, 10, 64)
</pre> <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">input_dim</code></b>: int &gt; 0. Size of the vocabulary, i.e. maximum integer index + 1.</li> <li>
<b><code translate="no" dir="ltr">output_dim</code></b>: int &gt;= 0. Dimension of the dense embedding.</li> <li>
<b><code translate="no" dir="ltr">embeddings_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">embeddings</code> matrix.</li> <li>
<b><code translate="no" dir="ltr">embeddings_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">embeddings</code> matrix.</li> <li>
<b><code translate="no" dir="ltr">embeddings_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">embeddings</code> matrix.</li> <li>
<b><code translate="no" dir="ltr">mask_zero</code></b>: Whether or not the input value 0 is a special "padding" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is <code translate="no" dir="ltr">True</code> then all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).</li> <li>
<b><code translate="no" dir="ltr">input_length</code></b>: Length of input sequences, when it is constant. This argument is required if you are going to connect <code translate="no" dir="ltr">Flatten</code> then <code translate="no" dir="ltr">Dense</code> layers upstream (without it, the shape of the dense outputs cannot be computed).</li> </ul> <h4 id="input_shape">Input shape:</h4> <p>2D tensor with shape: <code translate="no" dir="ltr">(batch_size, input_length)</code>.</p> <h4 id="output_shape">Output shape:</h4> <p>3D tensor with shape: <code translate="no" dir="ltr">(batch_size, input_length, output_dim)</code>.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/embeddings.py#L91-L123">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    input_dim,
    output_dim,
    embeddings_initializer='uniform',
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)
</pre> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="embedding"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.Embedding</code></a></li> <li><a href="embedding"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.Embedding</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a>
  </p>
</div>
