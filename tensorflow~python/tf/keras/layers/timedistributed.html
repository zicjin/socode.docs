<h1 class="devsite-page-title">tf.keras.layers.TimeDistributed</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.TimeDistributed"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/TimeDistributed">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/wrappers.py#L91-L349">  View source on GitHub </a> </td>
</table>  <h2 id="class_timedistributed">Class <code translate="no" dir="ltr">TimeDistributed</code>
</h2> <p>This wrapper allows to apply a layer to every temporal slice of an input.</p> <p>Inherits From: <a href="wrapper"><code translate="no" dir="ltr">Wrapper</code></a></p>  <p>The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.</p> <p>Consider a batch of 32 samples, where each sample is a sequence of 10 vectors of 16 dimensions. The batch input shape of the layer is then <code translate="no" dir="ltr">(32, 10, 16)</code>, and the <code translate="no" dir="ltr">input_shape</code>, not including the samples dimension, is <code translate="no" dir="ltr">(10, 16)</code>.</p> <p>You can then use <code translate="no" dir="ltr">TimeDistributed</code> to apply a <code translate="no" dir="ltr">Dense</code> layer to each of the 10 timesteps, independently:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># as the first layer in a model
model = Sequential()
model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))
# now model.output_shape == (None, 10, 8)
</pre> <p>The output will then have shape <code translate="no" dir="ltr">(32, 10, 8)</code>.</p> <p>In subsequent layers, there is no need for the <code translate="no" dir="ltr">input_shape</code>:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model.add(TimeDistributed(Dense(32)))
# now model.output_shape == (None, 10, 32)
</pre> <p>The output will then have shape <code translate="no" dir="ltr">(32, 10, 32)</code>.</p> <p><code translate="no" dir="ltr">TimeDistributed</code> can be used with arbitrary layers, not just <code translate="no" dir="ltr">Dense</code>, for instance with a <code translate="no" dir="ltr">Conv2D</code> layer:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = Sequential()
model.add(TimeDistributed(Conv2D(64, (3, 3)),
                          input_shape=(10, 299, 299, 3)))
</pre> <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">layer</code></b>: a layer instance.</li> </ul> <h4 id="call_arguments">Call arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: Input tensor.</li> <li>
<b><code translate="no" dir="ltr">training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the wrapped layer (only if the layer supports this argument).</li> <li>
<b><code translate="no" dir="ltr">mask</code></b>: Binary tensor of shape <code translate="no" dir="ltr">(samples, timesteps)</code> indicating whether a given timestep should be masked. This argument is passed to the wrapped layer (only if the layer supports this argument).</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If not initialized with a <code translate="no" dir="ltr">Layer</code> instance.</li> </ul> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/wrappers.py#L148-L161">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    layer,
    **kwargs
)
</pre> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="timedistributed"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.TimeDistributed</code></a></li> <li><a href="timedistributed"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.TimeDistributed</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/TimeDistributed</a>
  </p>
</div>
