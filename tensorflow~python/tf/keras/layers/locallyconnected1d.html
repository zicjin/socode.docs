<h1 class="devsite-page-title">tf.keras.layers.LocallyConnected1D</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.LocallyConnected1D"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/LocallyConnected1D">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/local.py#L36-L331">  View source on GitHub </a> </td>
</table>  <h2 id="class_locallyconnected1d_2">Class <code translate="no" dir="ltr">LocallyConnected1D</code>
</h2> <p>Locally-connected layer for 1D inputs.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p>  <p>The <code translate="no" dir="ltr">LocallyConnected1D</code> layer works similarly to the <code translate="no" dir="ltr">Conv1D</code> layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</p> <h4 id="example_2">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># apply a unshared weight convolution 1d of length 3 to a sequence with
# 10 timesteps, with 64 output filters
model = Sequential()
model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
# now model.output_shape == (None, 8, 64)
# add a new conv1d on top
model.add(LocallyConnected1D(32, 3))
# now model.output_shape == (None, 6, 32)
</pre> <h4 id="arguments_2">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">filters</code></b>: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</li> <li>
<b><code translate="no" dir="ltr">kernel_size</code></b>: An integer or tuple/list of a single integer, specifying the length of the 1D convolution window.</li> <li>
<b><code translate="no" dir="ltr">strides</code></b>: An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any <code translate="no" dir="ltr">dilation_rate</code> value != 1.</li> <li>
<b><code translate="no" dir="ltr">padding</code></b>: Currently only supports <code translate="no" dir="ltr">"valid"</code> (case-insensitive). <code translate="no" dir="ltr">"same"</code> may be supported in the future.</li> <li>
<b><code translate="no" dir="ltr">data_format</code></b>: A string, one of <code translate="no" dir="ltr">channels_last</code> (default) or <code translate="no" dir="ltr">channels_first</code>. The ordering of the dimensions in the inputs. <code translate="no" dir="ltr">channels_last</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, length, channels)</code> while <code translate="no" dir="ltr">channels_first</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, channels, length)</code>. It defaults to the <code translate="no" dir="ltr">image_data_format</code> value found in your Keras config file at <code translate="no" dir="ltr">~/.keras/keras.json</code>. If you never set it, then it will be "channels_last".</li> <li>
<b><code translate="no" dir="ltr">activation</code></b>: Activation function to use. If you don't specify anything, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_initializer</code></b>: Initializer for the bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li> <li>
<b><code translate="no" dir="ltr">activity_regularizer</code></b>: Regularizer function applied to the output of the layer (its "activation")..</li> <li>
<b><code translate="no" dir="ltr">kernel_constraint</code></b>: Constraint function applied to the kernel matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_constraint</code></b>: Constraint function applied to the bias vector.</li> <li>
<p><b><code translate="no" dir="ltr">implementation</code></b>: implementation mode, either <code translate="no" dir="ltr">1</code>, <code translate="no" dir="ltr">2</code>, or <code translate="no" dir="ltr">3</code>. <code translate="no" dir="ltr">1</code> loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops.</p> <p><code translate="no" dir="ltr">2</code> stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops.</p> <p><code translate="no" dir="ltr">3</code> stores layer weights in a sparse tensor and implements the forward pass as a single sparse matrix-multiply.</p> <p>How to choose:</p> <p><code translate="no" dir="ltr">1</code>: large, dense models, <code translate="no" dir="ltr">2</code>: small models, <code translate="no" dir="ltr">3</code>: large, sparse models,</p> <p>where "large" stands for large input/output activations (i.e. many <code translate="no" dir="ltr">filters</code>, <code translate="no" dir="ltr">input_filters</code>, large <code translate="no" dir="ltr">input_size</code>, <code translate="no" dir="ltr">output_size</code>), and "sparse" stands for few connections between inputs and outputs, i.e. small ratio <code translate="no" dir="ltr">filters * input_filters * kernel_size / (input_size * strides)</code>, where inputs to and outputs of the layer are assumed to have shapes <code translate="no" dir="ltr">(input_size, input_filters)</code>, <code translate="no" dir="ltr">(output_size, filters)</code> respectively.</p> <p>It is recommended to benchmark each in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Correct choice of implementation can lead to dramatic speed improvements (e.g. 50X), potentially at the expense of RAM.</p> <p>Also, only <code translate="no" dir="ltr">padding="valid"</code> is supported by <code translate="no" dir="ltr">implementation=1</code>.</p>
</li> </ul> <h4 id="input_shape_2">Input shape:</h4> <p>3D tensor with shape: <code translate="no" dir="ltr">(batch_size, steps, input_dim)</code></p> <h4 id="output_shape_2">Output shape:</h4> <p>3D tensor with shape: <code translate="no" dir="ltr">(batch_size, new_steps, filters)</code> <code translate="no" dir="ltr">steps</code> value might have changed due to padding or strides.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/local.py#L131-L168">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
</pre> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="locallyconnected1d"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.LocallyConnected1D</code></a></li> <li><a href="locallyconnected1d"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.LocallyConnected1D</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected1D" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected1D</a>
  </p>
</div>
