<h1 class="devsite-page-title">tf.keras.layers.PReLU</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.PReLU"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/PReLU">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/advanced_activations.py#L72-L158">  View source on GitHub </a> </td>
</table>  <h2 id="class_prelu">Class <code translate="no" dir="ltr">PReLU</code>
</h2> <p>Parametric Rectified Linear Unit.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p>  <h4 id="it_follows">It follows:</h4> <p><code translate="no" dir="ltr">f(x) = alpha * x for x &lt; 0</code>, <code translate="no" dir="ltr">f(x) = x for x &gt;= 0</code>, where <code translate="no" dir="ltr">alpha</code> is a learned array with the same shape as x.</p> <h4 id="input_shape">Input shape:</h4> <p>Arbitrary. Use the keyword argument <code translate="no" dir="ltr">input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p> <h4 id="output_shape">Output shape:</h4> <p>Same shape as the input.</p> <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">alpha_initializer</code></b>: Initializer function for the weights.</li> <li>
<b><code translate="no" dir="ltr">alpha_regularizer</code></b>: Regularizer for the weights.</li> <li>
<b><code translate="no" dir="ltr">alpha_constraint</code></b>: Constraint for the weights.</li> <li>
<b><code translate="no" dir="ltr">shared_axes</code></b>: The axes along which to share learnable parameters for the activation function. For example, if the incoming feature maps are from a 2D convolution with output shape <code translate="no" dir="ltr">(batch, height, width, channels)</code>, and you wish to share parameters across space so that each filter only has one set of parameters, set <code translate="no" dir="ltr">shared_axes=[1, 2]</code>.</li> </ul> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/advanced_activations.py#L102-L118">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    alpha_initializer='zeros',
    alpha_regularizer=None,
    alpha_constraint=None,
    shared_axes=None,
    **kwargs
)
</pre> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="prelu"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.PReLU</code></a></li> <li><a href="prelu"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.PReLU</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLU</a>
  </p>
</div>
