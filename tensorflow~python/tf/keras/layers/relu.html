<h1 class="devsite-page-title">tf.keras.layers.ReLU</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.ReLU"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/ReLU">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/advanced_activations.py#L273-L332">  View source on GitHub </a> </td>
</table>  <h2 id="class_relu_2">Class <code translate="no" dir="ltr">ReLU</code>
</h2> <p>Rectified Linear Unit activation function.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p> <h3 id="used_in_the_tutorials_2">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/generative/pix2pix">Pix2Pix</a></li> </ul> <p>With default values, it returns element-wise <code translate="no" dir="ltr">max(x, 0)</code>.</p> <p>Otherwise, it follows: <code translate="no" dir="ltr">f(x) = max_value</code> for <code translate="no" dir="ltr">x &gt;= max_value</code>, <code translate="no" dir="ltr">f(x) = x</code> for <code translate="no" dir="ltr">threshold &lt;= x &lt; max_value</code>, <code translate="no" dir="ltr">f(x) = negative_slope * (x - threshold)</code> otherwise.</p> <h4 id="input_shape_2">Input shape:</h4> <p>Arbitrary. Use the keyword argument <code translate="no" dir="ltr">input_shape</code> (tuple of integers, does not include the samples axis) when using this layer as the first layer in a model.</p> <h4 id="output_shape_2">Output shape:</h4> <p>Same shape as the input.</p> <h4 id="arguments_2">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">max_value</code></b>: Float &gt;= 0. Maximum activation value.</li> <li>
<b><code translate="no" dir="ltr">negative_slope</code></b>: Float &gt;= 0. Negative slope coefficient.</li> <li>
<b><code translate="no" dir="ltr">threshold</code></b>: Float. Threshold value for thresholded activation.</li> </ul> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/advanced_activations.py#L297-L311">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    max_value=None,
    negative_slope=0,
    threshold=0,
    **kwargs
)
</pre> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="relu"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.ReLU</code></a></li> <li><a href="relu"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.ReLU</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU</a>
  </p>
</div>
