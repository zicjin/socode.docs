<h1 class="devsite-page-title">tf.keras.layers.LSTM</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.LSTM"> <meta itemprop="path" content="Stable"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/LSTM">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/recurrent_v2.py#L890-L1170">  View source on GitHub </a> </td>
</table>  <h2 id="class_lstm_2">Class <code translate="no" dir="ltr">LSTM</code>
</h2> <p>Long Short-Term Memory layer - Hochreiter 1997.</p> <p>Inherits From: <a href="../../compat/v1/keras/layers/lstm"><code translate="no" dir="ltr">LSTM</code></a></p> <h3 id="used_in_the_guide_2">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/keras/rnn">Recurrent Neural Networks (RNN) with Keras</a></li> <li><a href="https://www.tensorflow.org/guide/keras/functional">The Keras functional API in TensorFlow</a></li> <li><a href="https://www.tensorflow.org/guide/keras/masking_and_padding">Masking and padding with Keras</a></li> </ul> <h3 id="used_in_the_tutorials_2">Used in the tutorials:</h3> <ul> <li><a href="https://www.tensorflow.org/tutorials/structured_data/time_series">Time series forecasting</a></li> <li><a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn">Text classification with an RNN</a></li> <li><a href="https://www.tensorflow.org/tutorials/load_data/text">Load text</a></li> </ul> <p>See <a href="https://www.tensorflow.org/guide/keras/rnn">the Keras RNN API guide</a> for details about the usage of RNN API.</p> <p>Based on available runtime hardware and constraints, this layer will choose different implementations (cuDNN-based or pure-TensorFlow) to maximize the performance. If a GPU is available and all the arguments to the layer meet the requirement of the CuDNN kernel (see below for details), the layer will use a fast cuDNN implementation.</p> <p>The requirements to use the cuDNN implementation are:</p> <ol> <li>
<code translate="no" dir="ltr">activation</code> == <code translate="no" dir="ltr">tanh</code>
</li> <li>
<code translate="no" dir="ltr">recurrent_activation</code> == <code translate="no" dir="ltr">sigmoid</code>
</li> <li>
<code translate="no" dir="ltr">recurrent_dropout</code> == 0</li> <li>
<code translate="no" dir="ltr">unroll</code> is <code translate="no" dir="ltr">False</code>
</li> <li>
<code translate="no" dir="ltr">use_bias</code> is <code translate="no" dir="ltr">True</code>
</li> <li>Inputs are not masked or strictly right padded.</li> </ol> <h4 id="arguments_2">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">units</code></b>: Positive integer, dimensionality of the output space.</li> <li>
<b><code translate="no" dir="ltr">activation</code></b>: Activation function to use. Default: hyperbolic tangent (<code translate="no" dir="ltr">tanh</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">recurrent_activation</code></b>: Activation function to use for the recurrent step. Default: sigmoid (<code translate="no" dir="ltr">sigmoid</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">use_bias</code></b>: Boolean (default <code translate="no" dir="ltr">True</code>), whether the layer uses a bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">kernel</code> weights matrix, used for the linear transformation of the inputs. Default: <code translate="no" dir="ltr">glorot_uniform</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix, used for the linear transformation of the recurrent state. Default: <code translate="no" dir="ltr">orthogonal</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_initializer</code></b>: Initializer for the bias vector. Default: <code translate="no" dir="ltr">zeros</code>.</li> <li>
<b><code translate="no" dir="ltr">unit_forget_bias</code></b>: Boolean (default <code translate="no" dir="ltr">True</code>). If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force <code translate="no" dir="ltr">bias_initializer="zeros"</code>. This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a>.</li> <li>
<b><code translate="no" dir="ltr">kernel_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_regularizer</code></b>: Regularizer function applied to the bias vector. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">activity_regularizer</code></b>: Regularizer function applied to the output of the layer (its "activation"). Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">kernel_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_constraint</code></b>: Constraint function applied to the bias vector. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.</li> <li>
<b><code translate="no" dir="ltr">recurrent_dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.</li> <li>
<b><code translate="no" dir="ltr">implementation</code></b>: Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. Default: 2.</li> <li>
<b><code translate="no" dir="ltr">return_sequences</code></b>: Boolean. Whether to return the last output. in the output sequence, or the full sequence. Default: <code translate="no" dir="ltr">False</code>.</li> <li>
<b><code translate="no" dir="ltr">return_state</code></b>: Boolean. Whether to return the last state in addition to the output. Default: <code translate="no" dir="ltr">False</code>.</li> <li>
<b><code translate="no" dir="ltr">go_backwards</code></b>: Boolean (default <code translate="no" dir="ltr">False</code>). If True, process the input sequence backwards and return the reversed sequence.</li> <li>
<b><code translate="no" dir="ltr">stateful</code></b>: Boolean (default <code translate="no" dir="ltr">False</code>). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.</li> <li>
<b><code translate="no" dir="ltr">time_major</code></b>: The shape format of the <code translate="no" dir="ltr">inputs</code> and <code translate="no" dir="ltr">outputs</code> tensors. If True, the inputs and outputs will be in shape <code translate="no" dir="ltr">[timesteps, batch, feature]</code>, whereas in the False case, it will be <code translate="no" dir="ltr">[batch, timesteps, feature]</code>. Using <code translate="no" dir="ltr">time_major = True</code> is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</li> <li>
<b><code translate="no" dir="ltr">unroll</code></b>: Boolean (default <code translate="no" dir="ltr">False</code>). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.</li> </ul> <h4 id="call_arguments_2">Call arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: A 3D tensor with shape <code translate="no" dir="ltr">[batch, timesteps, feature]</code>.</li> <li>
<b><code translate="no" dir="ltr">mask</code></b>: Binary tensor of shape <code translate="no" dir="ltr">[batch, timesteps]</code> indicating whether a given timestep should be masked (optional, defaults to <code translate="no" dir="ltr">None</code>).</li> <li>
<b><code translate="no" dir="ltr">training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. This argument is passed to the cell when calling it. This is only relevant if <code translate="no" dir="ltr">dropout</code> or <code translate="no" dir="ltr">recurrent_dropout</code> is used (optional, defaults to <code translate="no" dir="ltr">None</code>).</li> <li>
<b><code translate="no" dir="ltr">initial_state</code></b>: List of initial state tensors to be passed to the first call of the cell (optional, defaults to <code translate="no" dir="ltr">None</code> which causes creation of zero-filled initial state tensors).</li> </ul> <h4 id="examples_2">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = np.random.random([32, 10, 8]).astype(np.float32)
lstm = tf.keras.layers.LSTM(4)

output = lstm(inputs)  # The output has shape `[32, 4]`.

lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_memory_state and final_carry_state both have shape `[32, 4]`.
whole_sequence_output, final_memory_state, final_carry_state = lstm(inputs)
</pre> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="lstm"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.LSTM</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM</a>
  </p>
</div>
