<h1 class="devsite-page-title">tf.keras.layers.LSTMCell</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.LSTMCell"> <meta itemprop="path" content="Stable"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/LSTMCell">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/recurrent_v2.py#L769-L886">  View source on GitHub </a> </td>
</table>  <h2 id="class_lstmcell_2">Class <code translate="no" dir="ltr">LSTMCell</code>
</h2> <p>Cell class for the LSTM layer.</p> <p>Inherits From: <a href="../../compat/v1/keras/layers/lstmcell"><code translate="no" dir="ltr">LSTMCell</code></a></p> <h3 id="used_in_the_guide_2">Used in the guide:</h3> <ul> <li><a href="https://www.tensorflow.org/guide/function">Better performance with tf.function and AutoGraph</a></li> <li><a href="https://www.tensorflow.org/guide/keras/rnn">Recurrent Neural Networks (RNN) with Keras</a></li> </ul> <p>See <a href="https://www.tensorflow.org/guide/keras/rnn">the Keras RNN API guide</a> for details about the usage of RNN API.</p> <p>This class processes one step within the whole time sequence input, whereas <code translate="no" dir="ltr">tf.keras.layer.LSTM</code> processes the whole sequence.</p> <h4 id="arguments_2">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">units</code></b>: Positive integer, dimensionality of the output space.</li> <li>
<b><code translate="no" dir="ltr">activation</code></b>: Activation function to use. Default: hyperbolic tangent (<code translate="no" dir="ltr">tanh</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">recurrent_activation</code></b>: Activation function to use for the recurrent step. Default: sigmoid (<code translate="no" dir="ltr">sigmoid</code>). If you pass <code translate="no" dir="ltr">None</code>, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">use_bias</code></b>: Boolean, (default <code translate="no" dir="ltr">True</code>), whether the layer uses a bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">kernel</code> weights matrix, used for the linear transformation of the inputs. Default: <code translate="no" dir="ltr">glorot_uniform</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix, used for the linear transformation of the recurrent state. Default: <code translate="no" dir="ltr">orthogonal</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_initializer</code></b>: Initializer for the bias vector. Default: <code translate="no" dir="ltr">zeros</code>.</li> <li>
<b><code translate="no" dir="ltr">unit_forget_bias</code></b>: Boolean (default <code translate="no" dir="ltr">True</code>). If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force <code translate="no" dir="ltr">bias_initializer="zeros"</code>. This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al.</a>
</li> <li>
<b><code translate="no" dir="ltr">kernel_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_regularizer</code></b>: Regularizer function applied to the bias vector. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">kernel_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">recurrent_constraint</code></b>: Constraint function applied to the <code translate="no" dir="ltr">recurrent_kernel</code> weights matrix. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">bias_constraint</code></b>: Constraint function applied to the bias vector. Default: <code translate="no" dir="ltr">None</code>.</li> <li>
<b><code translate="no" dir="ltr">dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.</li> <li>
<b><code translate="no" dir="ltr">recurrent_dropout</code></b>: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.</li> <li>
<b><code translate="no" dir="ltr">implementation</code></b>: Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 (default) will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. Default: 2.</li> </ul> <h4 id="call_arguments_2">Call arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">inputs</code></b>: A 2D tensor, with shape of <code translate="no" dir="ltr">[batch, feature]</code>.</li> <li>
<b><code translate="no" dir="ltr">states</code></b>: List of 2 tensors that corresponding to the cell's units. Both of them have shape <code translate="no" dir="ltr">[batch, units]</code>, the first tensor is the memory state from previous time step, the second tesnor is the carry state from previous time step. For timestep 0, the initial state provided by user will be feed to cell.</li> <li>
<b><code translate="no" dir="ltr">training</code></b>: Python boolean indicating whether the layer should behave in training mode or in inference mode. Only relevant when <code translate="no" dir="ltr">dropout</code> or <code translate="no" dir="ltr">recurrent_dropout</code> is used.</li> </ul> <h4 id="examples_2">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">inputs = np.random.random([32, 10, 8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))

output = rnn(inputs)  # The output has shape `[32, 4]`.

rnn = tf.keras.layers.RNN(
    tf.keras.layers.LSTMCell(4),
    return_sequences=True,
    return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_memory_state and final_carry_state both have shape `[32, 4]`.
whole_sequence_output, final_memory_state, final_carry_state = rnn(inputs)
</pre> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="lstmcell"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.LSTMCell</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell</a>
  </p>
</div>
