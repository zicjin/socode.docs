<h1 class="devsite-page-title">tf.keras.layers.LocallyConnected2D</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.layers.LocallyConnected2D"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__init__"> </div>   <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/layers/LocallyConnected2D">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/local.py#L335-L655">  View source on GitHub </a> </td>
</table>  <h2 id="class_locallyconnected2d_2">Class <code translate="no" dir="ltr">LocallyConnected2D</code>
</h2> <p>Locally-connected layer for 2D inputs.</p> <p>Inherits From: <a href="layer"><code translate="no" dir="ltr">Layer</code></a></p>  <p>The <code translate="no" dir="ltr">LocallyConnected2D</code> layer works similarly to the <code translate="no" dir="ltr">Conv2D</code> layer, except that weights are unshared, that is, a different set of filters is applied at each different patch of the input.</p> <h4 id="examples_2">Examples:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># apply a 3x3 unshared weights convolution with 64 output filters on a
32x32 image
# with `data_format="channels_last"`:
model = Sequential()
model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
# now model.output_shape == (None, 30, 30, 64)
# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64
parameters

# add a 3x3 unshared weights convolution on top, with 32 output filters:
model.add(LocallyConnected2D(32, (3, 3)))
# now model.output_shape == (None, 28, 28, 32)
</pre> <h4 id="arguments_2">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">filters</code></b>: Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution).</li> <li>
<b><code translate="no" dir="ltr">kernel_size</code></b>: An integer or tuple/list of 2 integers, specifying the width and height of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.</li> <li>
<b><code translate="no" dir="ltr">strides</code></b>: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the width and height. Can be a single integer to specify the same value for all spatial dimensions.</li> <li>
<b><code translate="no" dir="ltr">padding</code></b>: Currently only support <code translate="no" dir="ltr">"valid"</code> (case-insensitive). <code translate="no" dir="ltr">"same"</code> will be supported in future.</li> <li>
<b><code translate="no" dir="ltr">data_format</code></b>: A string, one of <code translate="no" dir="ltr">channels_last</code> (default) or <code translate="no" dir="ltr">channels_first</code>. The ordering of the dimensions in the inputs. <code translate="no" dir="ltr">channels_last</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, height, width, channels)</code> while <code translate="no" dir="ltr">channels_first</code> corresponds to inputs with shape <code translate="no" dir="ltr">(batch, channels, height, width)</code>. It defaults to the <code translate="no" dir="ltr">image_data_format</code> value found in your Keras config file at <code translate="no" dir="ltr">~/.keras/keras.json</code>. If you never set it, then it will be "channels_last".</li> <li>
<b><code translate="no" dir="ltr">activation</code></b>: Activation function to use. If you don't specify anything, no activation is applied (ie. "linear" activation: <code translate="no" dir="ltr">a(x) = x</code>).</li> <li>
<b><code translate="no" dir="ltr">use_bias</code></b>: Boolean, whether the layer uses a bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_initializer</code></b>: Initializer for the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_initializer</code></b>: Initializer for the bias vector.</li> <li>
<b><code translate="no" dir="ltr">kernel_regularizer</code></b>: Regularizer function applied to the <code translate="no" dir="ltr">kernel</code> weights matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_regularizer</code></b>: Regularizer function applied to the bias vector.</li> <li>
<b><code translate="no" dir="ltr">activity_regularizer</code></b>: Regularizer function applied to the output of the layer (its "activation").</li> <li>
<b><code translate="no" dir="ltr">kernel_constraint</code></b>: Constraint function applied to the kernel matrix.</li> <li>
<b><code translate="no" dir="ltr">bias_constraint</code></b>: Constraint function applied to the bias vector.</li> <li>
<p><b><code translate="no" dir="ltr">implementation</code></b>: implementation mode, either <code translate="no" dir="ltr">1</code>, <code translate="no" dir="ltr">2</code>, or <code translate="no" dir="ltr">3</code>. <code translate="no" dir="ltr">1</code> loops over input spatial locations to perform the forward pass. It is memory-efficient but performs a lot of (small) ops.</p> <p><code translate="no" dir="ltr">2</code> stores layer weights in a dense but sparsely-populated 2D matrix and implements the forward pass as a single matrix-multiply. It uses a lot of RAM but performs few (large) ops.</p> <p><code translate="no" dir="ltr">3</code> stores layer weights in a sparse tensor and implements the forward pass as a single sparse matrix-multiply.</p> <p>How to choose:</p> <p><code translate="no" dir="ltr">1</code>: large, dense models, <code translate="no" dir="ltr">2</code>: small models, <code translate="no" dir="ltr">3</code>: large, sparse models,</p> <p>where "large" stands for large input/output activations (i.e. many <code translate="no" dir="ltr">filters</code>, <code translate="no" dir="ltr">input_filters</code>, large <code translate="no" dir="ltr">np.prod(input_size)</code>, <code translate="no" dir="ltr">np.prod(output_size)</code>), and "sparse" stands for few connections between inputs and outputs, i.e. small ratio `filters * input_filters * np.prod(kernel_size) / (np.prod(input_size)</p> <ul> <li>np.prod(strides))<code translate="no" dir="ltr">, where inputs to and outputs of the layer are assumed to have shapes</code>input_size + (input_filters,)<code translate="no" dir="ltr">,</code>output_size + (filters,)` respectively.</li> </ul> <p>It is recommended to benchmark each in the setting of interest to pick the most efficient one (in terms of speed and memory usage). Correct choice of implementation can lead to dramatic speed improvements (e.g. 50X), potentially at the expense of RAM.</p> <p>Also, only <code translate="no" dir="ltr">padding="valid"</code> is supported by <code translate="no" dir="ltr">implementation=1</code>.</p>
</li> </ul> <h4 id="input_shape_2">Input shape:</h4> <p>4D tensor with shape: <code translate="no" dir="ltr">(samples, channels, rows, cols)</code> if data_format='channels_first' or 4D tensor with shape: <code translate="no" dir="ltr">(samples, rows, cols, channels)</code> if data_format='channels_last'.</p> <h4 id="output_shape_2">Output shape:</h4> <p>4D tensor with shape: <code translate="no" dir="ltr">(samples, filters, new_rows, new_cols)</code> if data_format='channels_first' or 4D tensor with shape: <code translate="no" dir="ltr">(samples, new_rows, new_cols, filters)</code> if data_format='channels_last'. <code translate="no" dir="ltr">rows</code> and <code translate="no" dir="ltr">cols</code> values might have changed due to padding.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/layers/local.py#L442-L479">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
</pre> <h2 id="compat_aliases_2">Compat aliases</h2> <ul> <li><a href="locallyconnected2d"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.LocallyConnected2D</code></a></li> <li><a href="locallyconnected2d"><code translate="no" dir="ltr">tf.compat.v2.keras.layers.LocallyConnected2D</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LocallyConnected2D</a>
  </p>
</div>
