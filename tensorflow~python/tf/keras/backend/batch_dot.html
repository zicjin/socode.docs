<h1 class="devsite-page-title">tf.keras.backend.batch_dot</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.keras.backend.batch_dot"> <meta itemprop="path" content="Stable"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/keras/backend/batch_dot">  TensorFlow 1 version</a> </td> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/keras/backend.py#L1682-L1879">  View source on GitHub </a> </td>
</table>  <p>Batchwise dot product.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">tf.keras.backend.batch_dot(
    x,
    y,
    axes=None
)
</pre>  <p><code translate="no" dir="ltr">batch_dot</code> is used to compute dot product of <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> when <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code> are data in batch, i.e. in a shape of <code translate="no" dir="ltr">(batch_size, :)</code>. <code translate="no" dir="ltr">batch_dot</code> results in a tensor or variable with less dimensions than the input. If the number of dimensions is reduced to 1, we use <code translate="no" dir="ltr">expand_dims</code> to make sure that ndim is at least 2.</p> <h4 id="arguments">Arguments:</h4> <ul> <li>
<b><code translate="no" dir="ltr">x</code></b>: Keras tensor or variable with <code translate="no" dir="ltr">ndim &gt;= 2</code>.</li> <li>
<b><code translate="no" dir="ltr">y</code></b>: Keras tensor or variable with <code translate="no" dir="ltr">ndim &gt;= 2</code>.</li> <li>
<b><code translate="no" dir="ltr">axes</code></b>: Tuple or list of integers with target dimensions, or single integer. The sizes of <code translate="no" dir="ltr">x.shape[axes[0]]</code> and <code translate="no" dir="ltr">y.shape[axes[1]]</code> should be equal.</li> </ul> <h4 id="returns">Returns:</h4> <p>A tensor with shape equal to the concatenation of <code translate="no" dir="ltr">x</code>'s shape (less the dimension that was summed over) and <code translate="no" dir="ltr">y</code>'s shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to <code translate="no" dir="ltr">(batch_size, 1)</code>.</p> <h4 id="examples">Examples:</h4> <p>Assume <code translate="no" dir="ltr">x = [[1, 2], [3, 4]]</code> and <code translate="no" dir="ltr">y = [[5, 6], [7, 8]]</code> <code translate="no" dir="ltr">batch_dot(x, y, axes=1) = [[17], [53]]</code> which is the main diagonal of <code translate="no" dir="ltr">x.dot(y.T)</code>, although we never have to calculate the off-diagonal elements.</p> <ul> <li>
<b><code translate="no" dir="ltr">Pseudocode</code></b>: ``` inner_products = [] for xi, yi in zip(x, y): inner_products.append(xi.dot(yi)) result = stack(inner_products)</li> </ul> <pre class="prettyprint" translate="no" dir="ltr" data-language="python">
Shape inference:
Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.
If `axes` is (1, 2), to find the output shape of resultant tensor,
    loop through each dimension in `x`'s shape and `y`'s shape:

* `x.shape[0]` : 100 : append to output shape
* `x.shape[1]` : 20 : do not append to output shape,
    dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)
* `y.shape[0]` : 100 : do not append to output shape,
    always ignore first dimension of `y`
* `y.shape[1]` : 30 : append to output shape
* `y.shape[2]` : 20 : do not append to output shape,
    dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)
`output_shape` = `(100, 30)`

&lt;pre class="devsite-click-to-copy prettyprint lang-py"&gt;
&lt;code class="devsite-terminal" data-terminal-prefix="&amp;gt;&amp;gt;&amp;gt;"&gt;x_batch = tf.keras.backend.ones(shape=(32, 20, 1)) &lt;/code&gt;
&lt;code class="devsite-terminal" data-terminal-prefix="&amp;gt;&amp;gt;&amp;gt;"&gt;y_batch = tf.keras.backend.ones(shape=(32, 30, 20)) &lt;/code&gt;
&lt;code class="devsite-terminal" data-terminal-prefix="&amp;gt;&amp;gt;&amp;gt;"&gt;xy_batch_dot = tf.keras.backend.batch_dot(x_batch, y_batch, axes=(1, 2)) &lt;/code&gt;
&lt;code class="devsite-terminal" data-terminal-prefix="&amp;gt;&amp;gt;&amp;gt;"&gt;tf.keras.backend.int_shape(xy_batch_dot) &lt;/code&gt;
&lt;code class="no-select nocode"&gt;(32, 1, 30) &lt;/code&gt;
&lt;/pre&gt;

## Compat aliases

* &lt;a href="/api_docs/python/tf/keras/backend/batch_dot"&gt;&lt;code&gt;tf.compat.v1.keras.backend.batch_dot&lt;/code&gt;&lt;/a&gt;
* &lt;a href="/api_docs/python/tf/keras/backend/batch_dot"&gt;&lt;code&gt;tf.compat.v2.keras.backend.batch_dot&lt;/code&gt;&lt;/a&gt;
</pre>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_dot" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/keras/backend/batch_dot</a>
  </p>
</div>
