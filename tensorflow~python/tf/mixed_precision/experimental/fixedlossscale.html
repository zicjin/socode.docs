<h1 class="devsite-page-title">tf.mixed_precision.experimental.FixedLossScale</h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.mixed_precision.experimental.FixedLossScale"> <meta itemprop="path" content="Stable"> <meta itemprop="property" content="__call__"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="from_config"> <meta itemprop="property" content="get_config"> <meta itemprop="property" content="update"> </div>  <table class="tfo-notebook-buttons tfo-api" align="left"> <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L195-L239">  View source on GitHub </a> </td>
</table>  <h2 id="class_fixedlossscale">Class <code translate="no" dir="ltr">FixedLossScale</code>
</h2> <p>Loss scale with a fixed value.</p> <p>Inherits From: <a href="lossscale"><code translate="no" dir="ltr">LossScale</code></a></p> <p><strong>Aliases</strong>: <a href="fixedlossscale"><code translate="no" dir="ltr">tf.train.experimental.FixedLossScale</code></a></p>  <p>The loss scale is not updated for the lifetime of instances of this class. A given instance of this class always returns the same number when called.</p> <h2 id="__init__"><code translate="no" dir="ltr">__init__</code></h2> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L202-L226">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__init__(loss_scale_value)
</pre> <p>Creates the fixed loss scale.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">loss_scale_value</code></b>: A Python float. Its ideal value varies depending on models to run. Choosing a too small loss_scale might affect model quality; a too big loss_scale might cause inf or nan. There is no single right loss_scale to apply. There is no harm choosing a relatively big number as long as no nan or inf is encountered in training.</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code translate="no" dir="ltr">ValueError</code></b>: If loss_scale_value is less than 1.</li> </ul> <h2 id="methods">Methods</h2> <h3 id="__call__"><code translate="no" dir="ltr">__call__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L228-L229">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">__call__()
</pre> <p>Returns the current loss scale as a scalar <code translate="no" dir="ltr">float32</code> tensor.</p> <h3 id="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L182-L185">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">from_config(
    cls,
    config
)
</pre> <p>Creates the LossScale from its config.</p> <h3 id="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L238-L239">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">get_config()
</pre> <p>Returns the config of this loss scale.</p> <h3 id="update"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/r2.1/tensorflow/python/training/experimental/loss_scale.py#L231-L233">View source</a></p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">update(grads)
</pre> <p>Updates the value of the loss scale.</p> <p>The loss scale will be potentially updated, based on the value of <code translate="no" dir="ltr">grads</code>. The tensor returned by calling this class is only updated when this function is evaluated.</p> <p>In eager mode, this directly updates the loss scale, so that calling <code translate="no" dir="ltr">__call__</code> will return the newly updated loss scale. In graph mode, this returns an op that, when evaluated, updates the loss scale.</p> <p>This function also returns a <code translate="no" dir="ltr">should_apply_gradients</code> bool. If False, gradients should not be applied to the variables that step, as nonfinite gradients were found, and the loss scale has been be updated to reduce the chance of finding nonfinite gradients in the next step. Some loss scale classes will always return True, as they cannot adjust themselves in response to nonfinite gradients.</p> <p>When a DistributionStrategy is used, this function may only be called in a cross-replica context.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code translate="no" dir="ltr">grads</code></b>: A nested structure of unscaled gradients, each which is the gradient of the loss with respect to a weight. The gradients should have already been divided by the loss scale being before passed to this function. 'None' gradients are accepted, and are ignored.</li> </ul> <h4 id="returns">Returns:</h4> <ul> <li>
<b><code translate="no" dir="ltr">update_op</code></b>: In eager mode, None. In graph mode, an op to update the loss scale.</li> <li>
<b><code translate="no" dir="ltr">should_apply_gradients</code></b>: Either a bool or a scalar boolean tensor. If False, the caller should skip applying <code translate="no" dir="ltr">grads</code> to the variables this step.</li> </ul> <h2 id="compat_aliases">Compat aliases</h2> <ul> <li><a href="fixedlossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.experimental.FixedLossScale</code></a></li> <li><a href="fixedlossscale"><code translate="no" dir="ltr">tf.compat.v1.train.experimental.FixedLossScale</code></a></li> <li><a href="fixedlossscale"><code translate="no" dir="ltr">tf.compat.v2.mixed_precision.experimental.FixedLossScale</code></a></li> <li><a href="fixedlossscale"><code translate="no" dir="ltr">tf.compat.v2.train.experimental.FixedLossScale</code></a></li> </ul>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/FixedLossScale" class="_attribution-link">https://www.tensorflow.org/api_docs/python/tf/mixed_precision/experimental/FixedLossScale</a>
  </p>
</div>
